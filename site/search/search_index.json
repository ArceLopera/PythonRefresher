{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Python Refresher Everyone can forget about grammar and vocabulary. The most important think is to know where to look. Basic Topics Advanced Topics Basics Useful Modules Data Structures Machine Learning Functions Algorithms Classes Exercises","title":"Home"},{"location":"#welcome-to-python-refresher","text":"Everyone can forget about grammar and vocabulary. The most important think is to know where to look. Basic Topics Advanced Topics Basics Useful Modules Data Structures Machine Learning Functions Algorithms Classes Exercises","title":"Welcome to Python Refresher"},{"location":"pythonRefresh/","text":"Python is strongly typed (i.e. types are enforced), dynamically, implicitly typed (i.e. you don\u2019t have to declare variables), case sensitive (i.e. var and VAR are two different variables) and object-oriented (i.e. everything is an object, Like C# and Java). A unique aspect of the Python language: indentation. While a language like C uses curly braces to contain code statements within loops or conditionals, Python indicates these statements through indentation. This feature lends Python code readability. Notice the colon at the end of the expression in the for, while and if statement. Python Resources Free e-book Python Like You Mean It , by Ryan Soklaski, will provide a more comprehensive introduction to Python for scientific computing. The website http://www.python.org , Tutorial and doc","title":"General"},{"location":"pythonRefresh/#python-resources","text":"Free e-book Python Like You Mean It , by Ryan Soklaski, will provide a more comprehensive introduction to Python for scientific computing. The website http://www.python.org , Tutorial and doc","title":"Python Resources"},{"location":"Algo/pyArrayAlgo/","text":"Kadane's Algorithm Floyd's Cycle Detection Algorithm Quick Select Algorithm KMP Algorithm","title":"Array Algorithms"},{"location":"Algo/pyDynamic/","text":"Those who cannot remember the past are condemned to repeat it .* \u2014 Dynamic Programming Dynamic programming is an algorithmic paradigm that divides broader problems into smaller subproblems and stores the result for later use, eliminating the need for any re-computation. This problem-solving approach is quite similar to the divide and conquer approach. We solve problems in both these paradigms by integrating the answers to smaller subproblems. However, unlike divide and conquer, the subproblems in dynamic programming repeat themselves multiple times. This means dynamic programming has different properties than the divide and conquer approach. If the problem abides by properties given below, only then it can be solved using a dynamic programming paradigm: Optimal Substructure: A problem is said to have an optimal substructure if we can formulate a recurrence relation. Overlapping Subproblem: Dynamic programming is used where the solutions of the same sub-problems are required again and again. Dynamic programming is used because the solutions of sub-problems can be stored in a table so that it does not need to recompute again and again. Dynamic programming is not required when there are no common or overlapping sub-problems as there is no use of storing the results in a table again and again. For example, Binary search does not require dynamic programming as it has no common sub-problems. In contrast, a recursive program of Fibonacci series has many common sub-problems, so it requires a dynamic programming approach to solve the problem optimally. There are two ways that we can use to store the values: Memoization Tabulation Both tabulation and memoization are used to store the solutions of sub-problems. In memoization, table entry is filled on demand; it means that its not mandatory that all the entries are completely filled. In the tabulation technique, all the table entries are filled one by one starting from the very first entry. Top-Down Approach Bottom-Up Approach Uses memorization technique Uses tabulation technique Recursive nature Iterative nature Structured programming languages such as COBOL, Fortran, C Object-oriented programming languages Uses decomposition Uses composition A lookup table is maintained and checked before computation of any subproblem The solution is built from a bottom-most case using iteration Memoization : TOP-DOWN Approach Memoization is a technique that stores the result of the sub-problem. Memoization technique is almost similar to the recursive technique with a small difference that memorization looks into the lookup table before computing the sub-problem. In the memorization technique, a lookup table is used. Initially, a look up table is initialized with NIL values. Whenever we require a solution of the sub-problem then we first look into the look up table and if we do not find the solution into the look up table then we compute the sub-problem and stores the result into the look up table so that it can be reused later. The top-down approach follows the memorization technique. It consists of two distinct events: recursion and caching. \u2018Recursion\u2019 represents the process of computation by calling functions repeatedly, whereas \u2018caching\u2019 represents the storing of intermediate results. Advantages Easy to understand and implement Solves the subproblem only if the solution is not memorized. When not all the subproblems need to be solved, memoization means only the necessary ones are computed. Debugging is easier. Disadvantages Uses recursion, which takes up more memory space in the call stack, degrading the overall performance.Trades speed for memory. Possibility of a stack overflow error. Simple recursion: def fibonacci ( n ): print ( n ) if n == 0 : return 0 elif n == 1 : return 1 else : return fibonacci ( n - 1 ) + fibonacci ( n - 2 ) fibonacci ( 4 ) 4 3 2 1 0 1 2 1 0 3 Here\u2019s a stack graph that shows all stack frames created during this function call. Note that these frames are not all on the stack at the same time. known = { 0 : 0 , 1 : 1 } def fibonacci_memo ( n ): if n in known : return known [ n ] print ( n ) res = fibonacci_memo ( n - 1 ) + fibonacci_memo ( n - 2 ) known [ n ] = res return res fibonacci_memo ( 4 ) 4 3 2 3 known is a dictionary that keeps track of the Fibonacci numbers we already know. It starts with two items: 0 maps to 0 and 1 maps to 1. Whenever fibonacci is called, it checks known. If the result is already there, it can return immediately. Otherwise it has to compute the new value, add it to the dictionary, and return it. If you run this version of fibonacci and compare it with the original, you will find that it is much faster. Complexity Time Complexity: Time Complexity is defined as the number of times a particular instruction set is executed rather than the total time is taken. It is because the total time took also depends on some external factors like the compiler used, processor\u2019s speed, etc. Space Complexity: Space Complexity is the total memory space required by the program for its execution. Both are calculated as the function of input size(n). One important thing here is that in spite of these parameters the efficiency of an algorithm also depends upon the nature and size of the input. import time from functools import lru_cache def fib ( n ): \"\"\" Returns the n-th Fibonacci number. \"\"\" if n == 0 or n == 1 : return n return fib ( n - 1 ) + fib ( n - 2 ) @lru_cache ( maxsize = 32 ) def fib_lru ( n ): \"\"\" Returns the n-th Fibonacci number. \"\"\" if n == 0 or n == 1 : return n return fib_lru ( n - 1 ) + fib_lru ( n - 2 ) # Manual caching using a dictionary. def fib_cache ( n , cache = None ): if cache is None : cache = {} if n in cache : return cache [ n ] if n == 0 or n == 1 : return n result = fib_cache ( n - 1 , cache ) + fib_cache ( n - 2 , cache ) cache [ n ] = result return result n = 35 start = time . perf_counter () fib ( n ) end = time . perf_counter () print ( \"Plain recursive version. Seconds taken: {:.7f} \" . format ( end - start )) start = time . perf_counter () fib_lru ( n ) end = time . perf_counter () print ( \"lru cache version. Seconds taken: {:.7f} \" . format ( end - start )) start = time . perf_counter () fib_cache ( n ) end = time . perf_counter () print ( \"Manual cache version. Seconds taken: {:.7f} \" . format ( end - start )) Plain recursive version. Seconds taken: 4.4927939 lru cache version. Seconds taken: 0.0000846 Manual cache version. Seconds taken: 0.0000724 Recursion limit Use sys.setrecursionlimit() to set the maximum depth of the Python interpreter stack to the integer value required for the program. It will raise a Recursion Error exception if the new limit is too low at the current recursion depth. # Python3 program to explain the sys.setrecursionlimit() method import sys # Print the current recursion limit # using sys.getrecursionlimit() print ( \"Original recursion limit was: \" ) print ( sys . getrecursionlimit ()) # Set a new recursion limit sys . setrecursionlimit ( 10 ** 6 ) # Print the new recursion limit print ( \"New recursion limit is: \" ) print ( sys . getrecursionlimit ()) Original recursion limit was: 1000 New recursion limit is: 1000000 Tabulation: BOTTOM-UP Approach In the tabulation technique, we solve the sub-problems in a bottom-up fashion and use the solutions of sub-problems to reach at the bigger sub-problems. The tabulation technique can be used as generating the solutions of bigger sub-problems iteratively by using the solutions of the smaller sub-problems. It addresses the same problems as before, but without recursion. The recursion is replaced with iteration in this approach. Hence, there is no stack overflow error or overhead of recursive procedures. We maintain a table (e.g. 3D matrix) to solve the problem in this method. # Fibonacci using the bottom up approach def fib ( n ): a = 1 # f(i - 2) b = 1 # f(i - 1) for i in range ( 2 , n + 1 ): # end of range is exclusive # the old \"a\" is no longer accessible after this a , b = b , a + b return b import time start = time . perf_counter () fib ( 35 ) end = time . perf_counter () print ( \"Bottum-up version. Seconds taken: {:.7f} \" . format ( end - start )) Bottum-up version. Seconds taken: 0.0000956 Both memoization and bottom-up approach are linear in time for calculation. Though the bottom-up approach is faster in a constant factor. The big difference is in memory usage. Bottom-up approach uses a small constant space, while recursion needs a linear space. Bottom up approach represent recursive problems as a Directed acyclic graphs (DAGs). The Dag can be traversed in order of dependency, solving sobproblems before they are needed and using minimal time and space. from tabulate import tabulate def knapsack ( max_capacity , weights , values ): num_items = len ( values ) results_table = [[ 0 for _ in range ( max_capacity + 1 )] for _ in range ( num_items + 1 )] # Build results table in bottom-up manner for i in range ( num_items + 1 ): for j in range ( max_capacity + 1 ): # This initial empty rows and columns if i == 0 or j == 0 : results_table [ i ][ j ] = 0 # The rest of the cells elif weights [ i - 1 ] <= j : results_table [ i ][ j ] = max ( results_table [ i - 1 ][ j ], values [ i - 1 ] + results_table [ i - 1 ][ j - weights [ i - 1 ]]) else : results_table [ i ][ j ] = results_table [ i - 1 ][ j ] # Display results table print ( tabulate ( results_table , tablefmt = \"pretty\" )) # for row in results_table: # for el in row: # print(el, end=\",\") # print() return results_table [ num_items ][ max_capacity ] values = [ 10 , 5 , 20 , 35 ] weights = [ 1 , 2 , 3 , 5 ] max_capacity = 6 print ( knapsack ( max_capacity , weights , values )) +---+----+----+----+----+----+----+ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | | 0 | 10 | 10 | 10 | 10 | 10 | 10 | | 0 | 10 | 10 | 15 | 15 | 15 | 15 | | 0 | 10 | 10 | 20 | 30 | 30 | 35 | | 0 | 10 | 10 | 20 | 30 | 35 | 45 | +---+----+----+----+----+----+----+ 45 Dynamic programming is useful when you\u2019re trying to optimize something given a constraint. In the knapsack problem, you had to maximize the value of the goods you stole, constrained by the size of the knapsack. You can use dynamic programming when the problem can be broken into discrete subproblems, and they don\u2019t depend on each other Every dynamic-programming solution involves a grid. The values in the cells are usually what you\u2019re trying to optimize. Each cell is a subproblem, so think about how you can divide your problem into subproblems. That will help you figure out what the axes are References: Useful Youtube video Other Convert to an Iterative Approach: Any problem of recursion can be converted to an iterative approach, so try to use the iterative approach instead of recursion. Using Stack and while loop may save the execution time of the program. # Program to show the time taken in # iteration and recursion import time # Recursive function to find factorial # of the given number N def factorial ( N ): # Base Case if N == 0 : return 1 # Recursive Call return N * factorial ( N - 1 ) # Driver Code if __name__ == '__main__' : n = 20 # Find the time taken for the # recursive approach start = time . perf_counter () print ( \"Calculated using recursion: \" , factorial ( n )) end = time . perf_counter () print ( \"Time taken in recursion: \" , \" {0:.9f} \" . format ( end - start )) # Find time taken for iterative # approach start = time . perf_counter () result = 1 while n > 0 : result *= n n -= 1 print ( \"Calculated using the iterative method: \" , result ) end = time . perf_counter () print ( \"Time taken in iteration: \" , \" {0:.9f} \" . format ( end - start )) Calculated using recursion: 2432902008176640000 Time taken in recursion: 0.004770568 Calculated using the iterative method: 2432902008176640000 Time taken in iteration: 0.000037236","title":"Dynamic Programming"},{"location":"Algo/pyDynamic/#memoization-top-down-approach","text":"Memoization is a technique that stores the result of the sub-problem. Memoization technique is almost similar to the recursive technique with a small difference that memorization looks into the lookup table before computing the sub-problem. In the memorization technique, a lookup table is used. Initially, a look up table is initialized with NIL values. Whenever we require a solution of the sub-problem then we first look into the look up table and if we do not find the solution into the look up table then we compute the sub-problem and stores the result into the look up table so that it can be reused later. The top-down approach follows the memorization technique. It consists of two distinct events: recursion and caching. \u2018Recursion\u2019 represents the process of computation by calling functions repeatedly, whereas \u2018caching\u2019 represents the storing of intermediate results. Advantages Easy to understand and implement Solves the subproblem only if the solution is not memorized. When not all the subproblems need to be solved, memoization means only the necessary ones are computed. Debugging is easier. Disadvantages Uses recursion, which takes up more memory space in the call stack, degrading the overall performance.Trades speed for memory. Possibility of a stack overflow error. Simple recursion: def fibonacci ( n ): print ( n ) if n == 0 : return 0 elif n == 1 : return 1 else : return fibonacci ( n - 1 ) + fibonacci ( n - 2 ) fibonacci ( 4 ) 4 3 2 1 0 1 2 1 0 3 Here\u2019s a stack graph that shows all stack frames created during this function call. Note that these frames are not all on the stack at the same time. known = { 0 : 0 , 1 : 1 } def fibonacci_memo ( n ): if n in known : return known [ n ] print ( n ) res = fibonacci_memo ( n - 1 ) + fibonacci_memo ( n - 2 ) known [ n ] = res return res fibonacci_memo ( 4 ) 4 3 2 3 known is a dictionary that keeps track of the Fibonacci numbers we already know. It starts with two items: 0 maps to 0 and 1 maps to 1. Whenever fibonacci is called, it checks known. If the result is already there, it can return immediately. Otherwise it has to compute the new value, add it to the dictionary, and return it. If you run this version of fibonacci and compare it with the original, you will find that it is much faster.","title":"Memoization : TOP-DOWN Approach"},{"location":"Algo/pyDynamic/#complexity","text":"Time Complexity: Time Complexity is defined as the number of times a particular instruction set is executed rather than the total time is taken. It is because the total time took also depends on some external factors like the compiler used, processor\u2019s speed, etc. Space Complexity: Space Complexity is the total memory space required by the program for its execution. Both are calculated as the function of input size(n). One important thing here is that in spite of these parameters the efficiency of an algorithm also depends upon the nature and size of the input. import time from functools import lru_cache def fib ( n ): \"\"\" Returns the n-th Fibonacci number. \"\"\" if n == 0 or n == 1 : return n return fib ( n - 1 ) + fib ( n - 2 ) @lru_cache ( maxsize = 32 ) def fib_lru ( n ): \"\"\" Returns the n-th Fibonacci number. \"\"\" if n == 0 or n == 1 : return n return fib_lru ( n - 1 ) + fib_lru ( n - 2 ) # Manual caching using a dictionary. def fib_cache ( n , cache = None ): if cache is None : cache = {} if n in cache : return cache [ n ] if n == 0 or n == 1 : return n result = fib_cache ( n - 1 , cache ) + fib_cache ( n - 2 , cache ) cache [ n ] = result return result n = 35 start = time . perf_counter () fib ( n ) end = time . perf_counter () print ( \"Plain recursive version. Seconds taken: {:.7f} \" . format ( end - start )) start = time . perf_counter () fib_lru ( n ) end = time . perf_counter () print ( \"lru cache version. Seconds taken: {:.7f} \" . format ( end - start )) start = time . perf_counter () fib_cache ( n ) end = time . perf_counter () print ( \"Manual cache version. Seconds taken: {:.7f} \" . format ( end - start )) Plain recursive version. Seconds taken: 4.4927939 lru cache version. Seconds taken: 0.0000846 Manual cache version. Seconds taken: 0.0000724","title":"Complexity"},{"location":"Algo/pyDynamic/#recursion-limit","text":"Use sys.setrecursionlimit() to set the maximum depth of the Python interpreter stack to the integer value required for the program. It will raise a Recursion Error exception if the new limit is too low at the current recursion depth. # Python3 program to explain the sys.setrecursionlimit() method import sys # Print the current recursion limit # using sys.getrecursionlimit() print ( \"Original recursion limit was: \" ) print ( sys . getrecursionlimit ()) # Set a new recursion limit sys . setrecursionlimit ( 10 ** 6 ) # Print the new recursion limit print ( \"New recursion limit is: \" ) print ( sys . getrecursionlimit ()) Original recursion limit was: 1000 New recursion limit is: 1000000","title":"Recursion limit"},{"location":"Algo/pyDynamic/#tabulation-bottom-up-approach","text":"In the tabulation technique, we solve the sub-problems in a bottom-up fashion and use the solutions of sub-problems to reach at the bigger sub-problems. The tabulation technique can be used as generating the solutions of bigger sub-problems iteratively by using the solutions of the smaller sub-problems. It addresses the same problems as before, but without recursion. The recursion is replaced with iteration in this approach. Hence, there is no stack overflow error or overhead of recursive procedures. We maintain a table (e.g. 3D matrix) to solve the problem in this method. # Fibonacci using the bottom up approach def fib ( n ): a = 1 # f(i - 2) b = 1 # f(i - 1) for i in range ( 2 , n + 1 ): # end of range is exclusive # the old \"a\" is no longer accessible after this a , b = b , a + b return b import time start = time . perf_counter () fib ( 35 ) end = time . perf_counter () print ( \"Bottum-up version. Seconds taken: {:.7f} \" . format ( end - start )) Bottum-up version. Seconds taken: 0.0000956 Both memoization and bottom-up approach are linear in time for calculation. Though the bottom-up approach is faster in a constant factor. The big difference is in memory usage. Bottom-up approach uses a small constant space, while recursion needs a linear space. Bottom up approach represent recursive problems as a Directed acyclic graphs (DAGs). The Dag can be traversed in order of dependency, solving sobproblems before they are needed and using minimal time and space. from tabulate import tabulate def knapsack ( max_capacity , weights , values ): num_items = len ( values ) results_table = [[ 0 for _ in range ( max_capacity + 1 )] for _ in range ( num_items + 1 )] # Build results table in bottom-up manner for i in range ( num_items + 1 ): for j in range ( max_capacity + 1 ): # This initial empty rows and columns if i == 0 or j == 0 : results_table [ i ][ j ] = 0 # The rest of the cells elif weights [ i - 1 ] <= j : results_table [ i ][ j ] = max ( results_table [ i - 1 ][ j ], values [ i - 1 ] + results_table [ i - 1 ][ j - weights [ i - 1 ]]) else : results_table [ i ][ j ] = results_table [ i - 1 ][ j ] # Display results table print ( tabulate ( results_table , tablefmt = \"pretty\" )) # for row in results_table: # for el in row: # print(el, end=\",\") # print() return results_table [ num_items ][ max_capacity ] values = [ 10 , 5 , 20 , 35 ] weights = [ 1 , 2 , 3 , 5 ] max_capacity = 6 print ( knapsack ( max_capacity , weights , values )) +---+----+----+----+----+----+----+ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | | 0 | 10 | 10 | 10 | 10 | 10 | 10 | | 0 | 10 | 10 | 15 | 15 | 15 | 15 | | 0 | 10 | 10 | 20 | 30 | 30 | 35 | | 0 | 10 | 10 | 20 | 30 | 35 | 45 | +---+----+----+----+----+----+----+ 45 Dynamic programming is useful when you\u2019re trying to optimize something given a constraint. In the knapsack problem, you had to maximize the value of the goods you stole, constrained by the size of the knapsack. You can use dynamic programming when the problem can be broken into discrete subproblems, and they don\u2019t depend on each other Every dynamic-programming solution involves a grid. The values in the cells are usually what you\u2019re trying to optimize. Each cell is a subproblem, so think about how you can divide your problem into subproblems. That will help you figure out what the axes are References: Useful Youtube video Other","title":"Tabulation: BOTTOM-UP Approach"},{"location":"Algo/pyDynamic/#convert-to-an-iterative-approach","text":"Any problem of recursion can be converted to an iterative approach, so try to use the iterative approach instead of recursion. Using Stack and while loop may save the execution time of the program. # Program to show the time taken in # iteration and recursion import time # Recursive function to find factorial # of the given number N def factorial ( N ): # Base Case if N == 0 : return 1 # Recursive Call return N * factorial ( N - 1 ) # Driver Code if __name__ == '__main__' : n = 20 # Find the time taken for the # recursive approach start = time . perf_counter () print ( \"Calculated using recursion: \" , factorial ( n )) end = time . perf_counter () print ( \"Time taken in recursion: \" , \" {0:.9f} \" . format ( end - start )) # Find time taken for iterative # approach start = time . perf_counter () result = 1 while n > 0 : result *= n n -= 1 print ( \"Calculated using the iterative method: \" , result ) end = time . perf_counter () print ( \"Time taken in iteration: \" , \" {0:.9f} \" . format ( end - start )) Calculated using recursion: 2432902008176640000 Time taken in recursion: 0.004770568 Calculated using the iterative method: 2432902008176640000 Time taken in iteration: 0.000037236","title":"Convert to an Iterative Approach:"},{"location":"Algo/pyGen/","text":"Useful Algorithms Divide and Conquer Transform and Conquer Dynamic Programming Sorting Algorithms Searching Algorithms Array Algorithms Graph Algorithms Other Algorithms Greedy Algorithms Divide and Conquer A typical Divide and Conquer algorithm solves a problem using following three steps: Divide: This involves dividing the problem into smaller sub-problems. Conquer: Solve sub-problems by calling recursively until solved. Combine: Combine the sub-problems to get the final solution of the whole problem. Divide and Conquer deals with non-overlapping subproblems. When subproblems overlap use Dynamic Programming. Examples: Quicksort is a sorting algorithm. The algorithm picks a pivot element and rearranges the array elements so that all elements smaller than the picked pivot element move to the left side of the pivot, and all greater elements move to the right side. Finally, the algorithm recursively sorts the subarrays on the left and right of the pivot element. Merge Sort is also a sorting algorithm. The algorithm divides the array into two halves, recursively sorts them, and finally merges the two sorted halves. Closest Pair of Points The problem is to find the closest pair of points in a set of points in the x-y plane. The problem can be solved in O(n^2) time by calculating the distances of every pair of points and comparing the distances to find the minimum. The Divide and Conquer algorithm solves the problem in O(N log N) time. Strassen\u2019s Algorithm is an efficient algorithm to multiply two matrices. A simple method to multiply two matrices needs 3 nested loops and is O(n^3). Strassen\u2019s algorithm multiplies two matrices in O(n^2.8974) time. Cooley\u2013Tukey Fast Fourier Transform (FFT) algorithm is the most common algorithm for FFT. It is a divide and conquer algorithm which works in O(N log N) time. Karatsuba algorithm for fast multiplication does the multiplication of two n-digit numbers in at most 3n^{log_{2}^{3}}\\approx 3n^{1.585} single-digit multiplications in general (and exactly n^{\\log_23} when n is a power of 2). It is, therefore, faster than the classical algorithm, which requires n2 single-digit products. If n = 210 = 1024, in particular, the exact counts are 310 = 59, 049 and (210)2 = 1, 048, 576, respectively. Transform and Conquer Transform and Conquer is a technique whose main idea is to transfer the problem into some easier or similar versions using some procedure and then solve that easier or simpler versions and combine those versions to get the solution of the actual one. The design consists of two parts: The first stage involves the transformation/breakdown of the complex problem into other problems that is simpler than the original one. The second stage involves solving the simpler problems and after the problem is solved the solutions are combined and converted back to get the solution of the original problem. There are three ways to do that: Instance simplification: a technique of simplifying the problem to more convenient or simpler instances. e.g. Presorting Representation change: the data structure is transformed to represent the problem more efficiently. Problem reduction: the problem can be transformed to an easier problem to solve Presorting: Instance simplification \"Presorting\" is a common example of \"instance simplification.\" Presorting is sorting ahead of time, to make repetitive solutions faster. For example if you wish to find many kth statistics in an array then it might make sense to sort the array ahead of time for so that the cost for determining each statistics is constant time. Presorting is a form of preconditioning. Preconditioning is manipulating the data to make the algorithm faster. Another example is the problem to determine the uniqueness of array elements. The brute force algorithm would compare each array element with the rest of the array. The cost would be \u0398(n2). If we presort the array then the algorithm only needs to compare adjacent elements for uniqueness. The cost for determining uniqueness (without the sorting cost) is \u0398(n). The total cost is T(n) = Tsort(n) + Tscan(n) \u03b5 \u0398(n log n) + \u0398(n) = \u0398(n log n) Another example is computing the mode of an array. The mode is the most frequent array element. The brute force cost would be quadratic. If the array is sorted then we only need to count runs of value. Algorithm PresortMode ( A [ 0. .. n - 1 ] ) // assumes that A is sorted i \u2190 0 modefrequency \u2190 0 while i < n do runlength \u2190 1 runvalue \u2190 A [ i ] while i + runlength < n and A [ i + runlength ] = runvalue do runlength ++ if runlength > modefequency then modefrequency \u2190 runlength modevalue \u2190 runvalue return modevalue The algorithm runs in linear time, so the cost is sorting is presorting the array. Geometrical problems frequently sort the collection of points before solving the problem. Also diagraphs algorithms frequently do a topological sort before running. def mode_presort ( arr ): arr . sort () # Array must be sorted before we apply the algorithm. i = 0 mode_frequency = 0 while i < len ( arr ): run_length = 1 run_value = arr [ i ] while i + run_length < len ( arr ) and arr [ i + run_length ] == run_value : run_length += 1 if run_length > mode_frequency : mode_frequency = run_length mode_value = [ run_value ] # Make it a list elif run_length == mode_frequency : # Also deal with this case # Add alternative to existing list of modes mode_value . append ( run_value ) i += run_length return mode_value arr = [ 1 , 1 , 1 , 2 , 2 ] print ( mode_presort ( arr )) arr = [ 1 , 1 , 2 , 2 ] print ( mode_presort ( arr )) arr = [ 3 , 7 , 5 , 13 , 20 , 23 , 39 , 23 , 40 , 23 , 14 , 12 , 56 , 23 , 29 ] print ( mode_presort ( arr )) [1] [1, 2] [23] ## Number placement puzzle import random PUZZLE_SIZE = 10 # Create random puzzle # random.sample ensures no duplicates puzzle_nums = random . sample ( range ( 100 ), PUZZLE_SIZE ) puzzle_symbols = [] # Randomly assign inequalities for i in range ( PUZZLE_SIZE - 1 ): puzzle_symbols . append ( \">\" if random . random () < .5 else \"<\" ) # print(puzzle_nums) # print(puzzle_symbols) # Sort puzzle numbers first # Use largest remaining if greater than, smallest remaining if less than sorted_puzzle_nums = sorted ( puzzle_nums , reverse = True ) # Vars for the \"two pointer\" method high = 0 low = PUZZLE_SIZE - 1 # store solution values solution_values = [] # Integrate through the puzzle symbols and apply solution algorithm for symbol in puzzle_symbols : if symbol == \">\" : solution_values . append ( sorted_puzzle_nums [ high ]) high += 1 elif symbol == \"<\" : solution_values . append ( sorted_puzzle_nums [ low ]) low -= 1 solution_values . append ( sorted_puzzle_nums [ high ]) # Convert solution_values to list of strings solution_values = list ( map ( str , solution_values )) # Create a list to store the final solution representation final_solution_representation = [ None ] * ( len ( solution_values ) + len ( puzzle_symbols )) # Create solution representation using nifty slicing with step parameter final_solution_representation [:: 2 ] = solution_values final_solution_representation [ 1 :: 2 ] = puzzle_symbols # Display final solution # The join() method takes all items in an iterable and joins them into one string print ( \" \" . join ( final_solution_representation )) # Evaluate whether solution is correct. Some say eval is evil! print ( eval ( \" \" . join ( final_solution_representation ))) 18 < 21 < 99 > 94 > 44 < 93 > 53 < 92 > 90 > 74 True Heaps: Representation change A heap is a priority queue that is a complete tree. The textbook has the largest key at the root, so parent keys are larger than their children. The data structure finds the largest priority item in constant time. Removing the highest priority item and adding new item are lg(n) time, using bubble down and bubble up algorithm. Properties of Heaps The height of a heap is floor(lg n). The root contains the highest priority item. A node and all the descendants is a heap A heap can be implemented using an array and all operations are in-place. if index of the root = 1 then index of left child = 2i and right child = 2i+1 Level i of the heap has 2i elements heap order, the parent value is larger than the children The naive construction of the heap is by adding the items one at a time. The cost is \u0398(n lg n). There is a faster construction in linear time, bottom-up heap construction . Bottom-up heap construction starts at the last trivial sub heap, floor(n/2), and checks for heap ordering and correctly. Iteratively the algorithm moves up the heap checking the heap ordering and correcting. Algorithm HeapBottomUp ( H [ 1. .. n ] ) for i \u2190 floor ( n / 2 ) down to 1 do k \u2190 i // parent index v \u2190 H [ k ] // proposed parent value heap \u2190 false while not heap and 2 * k \u2264 n do j \u2190 2 * k // left child index if j < n then // there are children if H [ j ] < H [ j + 1 ] then j ++ // find the larger child value if v \u2265 H [ j ] then heap \u2190 true // check heap ordering else // correct the heap ordering H [ k ] \u2190 H [ j ] // bubble down k \u2190 j // move down the heap H [ k ] \u2190 v // insert the value The worst case cost is O(n), less than 2n comparisons.","title":"General"},{"location":"Algo/pyGen/#divide-and-conquer","text":"A typical Divide and Conquer algorithm solves a problem using following three steps: Divide: This involves dividing the problem into smaller sub-problems. Conquer: Solve sub-problems by calling recursively until solved. Combine: Combine the sub-problems to get the final solution of the whole problem. Divide and Conquer deals with non-overlapping subproblems. When subproblems overlap use Dynamic Programming. Examples: Quicksort is a sorting algorithm. The algorithm picks a pivot element and rearranges the array elements so that all elements smaller than the picked pivot element move to the left side of the pivot, and all greater elements move to the right side. Finally, the algorithm recursively sorts the subarrays on the left and right of the pivot element. Merge Sort is also a sorting algorithm. The algorithm divides the array into two halves, recursively sorts them, and finally merges the two sorted halves. Closest Pair of Points The problem is to find the closest pair of points in a set of points in the x-y plane. The problem can be solved in O(n^2) time by calculating the distances of every pair of points and comparing the distances to find the minimum. The Divide and Conquer algorithm solves the problem in O(N log N) time. Strassen\u2019s Algorithm is an efficient algorithm to multiply two matrices. A simple method to multiply two matrices needs 3 nested loops and is O(n^3). Strassen\u2019s algorithm multiplies two matrices in O(n^2.8974) time. Cooley\u2013Tukey Fast Fourier Transform (FFT) algorithm is the most common algorithm for FFT. It is a divide and conquer algorithm which works in O(N log N) time. Karatsuba algorithm for fast multiplication does the multiplication of two n-digit numbers in at most 3n^{log_{2}^{3}}\\approx 3n^{1.585} single-digit multiplications in general (and exactly n^{\\log_23} when n is a power of 2). It is, therefore, faster than the classical algorithm, which requires n2 single-digit products. If n = 210 = 1024, in particular, the exact counts are 310 = 59, 049 and (210)2 = 1, 048, 576, respectively.","title":"Divide and Conquer"},{"location":"Algo/pyGen/#transform-and-conquer","text":"Transform and Conquer is a technique whose main idea is to transfer the problem into some easier or similar versions using some procedure and then solve that easier or simpler versions and combine those versions to get the solution of the actual one. The design consists of two parts: The first stage involves the transformation/breakdown of the complex problem into other problems that is simpler than the original one. The second stage involves solving the simpler problems and after the problem is solved the solutions are combined and converted back to get the solution of the original problem. There are three ways to do that: Instance simplification: a technique of simplifying the problem to more convenient or simpler instances. e.g. Presorting Representation change: the data structure is transformed to represent the problem more efficiently. Problem reduction: the problem can be transformed to an easier problem to solve","title":"Transform and Conquer"},{"location":"Algo/pyGen/#presorting-instance-simplification","text":"\"Presorting\" is a common example of \"instance simplification.\" Presorting is sorting ahead of time, to make repetitive solutions faster. For example if you wish to find many kth statistics in an array then it might make sense to sort the array ahead of time for so that the cost for determining each statistics is constant time. Presorting is a form of preconditioning. Preconditioning is manipulating the data to make the algorithm faster. Another example is the problem to determine the uniqueness of array elements. The brute force algorithm would compare each array element with the rest of the array. The cost would be \u0398(n2). If we presort the array then the algorithm only needs to compare adjacent elements for uniqueness. The cost for determining uniqueness (without the sorting cost) is \u0398(n). The total cost is T(n) = Tsort(n) + Tscan(n) \u03b5 \u0398(n log n) + \u0398(n) = \u0398(n log n) Another example is computing the mode of an array. The mode is the most frequent array element. The brute force cost would be quadratic. If the array is sorted then we only need to count runs of value. Algorithm PresortMode ( A [ 0. .. n - 1 ] ) // assumes that A is sorted i \u2190 0 modefrequency \u2190 0 while i < n do runlength \u2190 1 runvalue \u2190 A [ i ] while i + runlength < n and A [ i + runlength ] = runvalue do runlength ++ if runlength > modefequency then modefrequency \u2190 runlength modevalue \u2190 runvalue return modevalue The algorithm runs in linear time, so the cost is sorting is presorting the array. Geometrical problems frequently sort the collection of points before solving the problem. Also diagraphs algorithms frequently do a topological sort before running. def mode_presort ( arr ): arr . sort () # Array must be sorted before we apply the algorithm. i = 0 mode_frequency = 0 while i < len ( arr ): run_length = 1 run_value = arr [ i ] while i + run_length < len ( arr ) and arr [ i + run_length ] == run_value : run_length += 1 if run_length > mode_frequency : mode_frequency = run_length mode_value = [ run_value ] # Make it a list elif run_length == mode_frequency : # Also deal with this case # Add alternative to existing list of modes mode_value . append ( run_value ) i += run_length return mode_value arr = [ 1 , 1 , 1 , 2 , 2 ] print ( mode_presort ( arr )) arr = [ 1 , 1 , 2 , 2 ] print ( mode_presort ( arr )) arr = [ 3 , 7 , 5 , 13 , 20 , 23 , 39 , 23 , 40 , 23 , 14 , 12 , 56 , 23 , 29 ] print ( mode_presort ( arr )) [1] [1, 2] [23] ## Number placement puzzle import random PUZZLE_SIZE = 10 # Create random puzzle # random.sample ensures no duplicates puzzle_nums = random . sample ( range ( 100 ), PUZZLE_SIZE ) puzzle_symbols = [] # Randomly assign inequalities for i in range ( PUZZLE_SIZE - 1 ): puzzle_symbols . append ( \">\" if random . random () < .5 else \"<\" ) # print(puzzle_nums) # print(puzzle_symbols) # Sort puzzle numbers first # Use largest remaining if greater than, smallest remaining if less than sorted_puzzle_nums = sorted ( puzzle_nums , reverse = True ) # Vars for the \"two pointer\" method high = 0 low = PUZZLE_SIZE - 1 # store solution values solution_values = [] # Integrate through the puzzle symbols and apply solution algorithm for symbol in puzzle_symbols : if symbol == \">\" : solution_values . append ( sorted_puzzle_nums [ high ]) high += 1 elif symbol == \"<\" : solution_values . append ( sorted_puzzle_nums [ low ]) low -= 1 solution_values . append ( sorted_puzzle_nums [ high ]) # Convert solution_values to list of strings solution_values = list ( map ( str , solution_values )) # Create a list to store the final solution representation final_solution_representation = [ None ] * ( len ( solution_values ) + len ( puzzle_symbols )) # Create solution representation using nifty slicing with step parameter final_solution_representation [:: 2 ] = solution_values final_solution_representation [ 1 :: 2 ] = puzzle_symbols # Display final solution # The join() method takes all items in an iterable and joins them into one string print ( \" \" . join ( final_solution_representation )) # Evaluate whether solution is correct. Some say eval is evil! print ( eval ( \" \" . join ( final_solution_representation ))) 18 < 21 < 99 > 94 > 44 < 93 > 53 < 92 > 90 > 74 True","title":"Presorting: Instance simplification"},{"location":"Algo/pyGen/#heaps-representation-change","text":"A heap is a priority queue that is a complete tree. The textbook has the largest key at the root, so parent keys are larger than their children. The data structure finds the largest priority item in constant time. Removing the highest priority item and adding new item are lg(n) time, using bubble down and bubble up algorithm. Properties of Heaps The height of a heap is floor(lg n). The root contains the highest priority item. A node and all the descendants is a heap A heap can be implemented using an array and all operations are in-place. if index of the root = 1 then index of left child = 2i and right child = 2i+1 Level i of the heap has 2i elements heap order, the parent value is larger than the children The naive construction of the heap is by adding the items one at a time. The cost is \u0398(n lg n). There is a faster construction in linear time, bottom-up heap construction . Bottom-up heap construction starts at the last trivial sub heap, floor(n/2), and checks for heap ordering and correctly. Iteratively the algorithm moves up the heap checking the heap ordering and correcting. Algorithm HeapBottomUp ( H [ 1. .. n ] ) for i \u2190 floor ( n / 2 ) down to 1 do k \u2190 i // parent index v \u2190 H [ k ] // proposed parent value heap \u2190 false while not heap and 2 * k \u2264 n do j \u2190 2 * k // left child index if j < n then // there are children if H [ j ] < H [ j + 1 ] then j ++ // find the larger child value if v \u2265 H [ j ] then heap \u2190 true // check heap ordering else // correct the heap ordering H [ k ] \u2190 H [ j ] // bubble down k \u2190 j // move down the heap H [ k ] \u2190 v // insert the value The worst case cost is O(n), less than 2n comparisons.","title":"Heaps:  Representation change"},{"location":"Algo/pyGraphAlgo/","text":"Kruskal Algorithm Dijkstra\u2019s Algorithm Find the cheapest node. This is the node you can get to in the least amount of time. Check whether there\u2019s a cheaper path to the neighbors of this node. If so, update their costs. Repeat until you\u2019ve done this for every node in the graph. Calculate the final path. #To code this example, you\u2019ll need three hash tables. graph, cost and parents graph = {} graph [ 'start' ] = {} graph [ 'start' ][ 'a' ] = 6 graph [ 'start' ][ 'b' ] = 2 graph [ 'a' ] = {} graph [ 'a' ][ 'fin' ] = 1 graph [ 'b' ] = {} graph [ 'b' ][ 'a' ] = 3 graph [ 'b' ][ 'fin' ] = 5 graph [ 'fin' ] = {} infinity = float ( 'inf' ) costs = {} costs [ 'a' ] = 6 costs [ 'b' ] = 2 costs [ 'fin' ] = infinity parents = {} parents [ 'a' ] = 'start' parents [ 'b' ] = 'start' parents [ 'fin' ] = None # an array to keep track of all the nodes you\u2019ve already # processed, because you don\u2019t need to process a node more than once processed = [] def find_lowest_cost_node ( costs ): lowest_cost = float ( 'inf' ) lowest_cost_node = None for node in costs : #Go through each node. cost = costs [ node ] if cost < lowest_cost and node not in processed : lowest_cost = cost #\u2026 set it as the new lowest-cost node. lowest_cost_node = node return lowest_cost_node node = find_lowest_cost_node ( costs ) while node is not None : cost = costs [ node ] neighbors = graph [ node ] for n in neighbors . keys (): new_cost = cost + neighbors [ n ] if costs [ n ] > new_cost : costs [ n ] = new_cost parents [ n ] = node processed . append ( node ) node = find_lowest_cost_node ( costs ) print ( costs ) {'a': 5, 'b': 2, 'fin': 6} Bellman Ford Algorithm Topological Sort Algorithm Floyd-Warshall Algorithm Flood Fill Algorithm Lee Algorithm","title":"Graph Algorithms"},{"location":"Algo/pyGraphAlgo/#kruskal-algorithm","text":"","title":"Kruskal Algorithm"},{"location":"Algo/pyGraphAlgo/#dijkstras-algorithm","text":"Find the cheapest node. This is the node you can get to in the least amount of time. Check whether there\u2019s a cheaper path to the neighbors of this node. If so, update their costs. Repeat until you\u2019ve done this for every node in the graph. Calculate the final path. #To code this example, you\u2019ll need three hash tables. graph, cost and parents graph = {} graph [ 'start' ] = {} graph [ 'start' ][ 'a' ] = 6 graph [ 'start' ][ 'b' ] = 2 graph [ 'a' ] = {} graph [ 'a' ][ 'fin' ] = 1 graph [ 'b' ] = {} graph [ 'b' ][ 'a' ] = 3 graph [ 'b' ][ 'fin' ] = 5 graph [ 'fin' ] = {} infinity = float ( 'inf' ) costs = {} costs [ 'a' ] = 6 costs [ 'b' ] = 2 costs [ 'fin' ] = infinity parents = {} parents [ 'a' ] = 'start' parents [ 'b' ] = 'start' parents [ 'fin' ] = None # an array to keep track of all the nodes you\u2019ve already # processed, because you don\u2019t need to process a node more than once processed = [] def find_lowest_cost_node ( costs ): lowest_cost = float ( 'inf' ) lowest_cost_node = None for node in costs : #Go through each node. cost = costs [ node ] if cost < lowest_cost and node not in processed : lowest_cost = cost #\u2026 set it as the new lowest-cost node. lowest_cost_node = node return lowest_cost_node node = find_lowest_cost_node ( costs ) while node is not None : cost = costs [ node ] neighbors = graph [ node ] for n in neighbors . keys (): new_cost = cost + neighbors [ n ] if costs [ n ] > new_cost : costs [ n ] = new_cost parents [ n ] = node processed . append ( node ) node = find_lowest_cost_node ( costs ) print ( costs ) {'a': 5, 'b': 2, 'fin': 6} Bellman Ford Algorithm Topological Sort Algorithm Floyd-Warshall Algorithm Flood Fill Algorithm Lee Algorithm","title":"Dijkstra\u2019s Algorithm"},{"location":"Algo/pyGreedy/","text":"Greedy algorithms optimize locally, hoping to end up with a global optimum. NP-complete problems have no known fast solution. If you have an NP-complete problem, your best bet is to use an approximation algorithm. Greedy algorithms are easy to write and fast to run, so they make good approximation algorithms.","title":"Greedy Algorithms"},{"location":"Algo/pyOtherAlgo/","text":"Huffman Coding Compression Algorithm Euclid's Algorithm Union Find Algorithm","title":"Other Useful Algorithms"},{"location":"Algo/pySearch/","text":"Breadth-first search is used to calculate the shortest path for an unweighted graph. Dijkstra\u2019s algorithm is used to calculate the shortest path for a weighted graph. Dijkstra\u2019s algorithm works when all the weights are positive. If you have negative weights, use the Bellman-Ford algorithm. Binary Search Binary search only works when your list is in sorted order. For example, the names in a phone book are sorted in alphabetical order, so you can use binary search to look for a name. Binary search is a lot faster than simple search. His time complexity is O(log n) is faster than O(n) -Simple search, but it gets a lot faster once the list of items you\u2019re searching through grows. Algorithm speed isn\u2019t measured in seconds. Algorithm times are measured in terms of growth of an algorithm. Algorithm times are written in Big O notation. def binary_search ( list , item ): low = 0 high = len ( list ) - 1 while low <= high : mid = ( low + high ) guess = list [ mid ] if guess == item : return mid if guess > item : high = mid - 1 else : low = mid + 1 return None my_list = [ 1 , 3 , 5 , 7 , 9 ] print ( binary_search ( my_list , 3 )) # => 1 print ( binary_search ( my_list , - 1 )) # => None 1 None Hash Tables Hashes are good for: Modeling relationships from one thing to another thing Filtering out duplicates Caching/memorizing data instead of making your server do work Python has hash tables; they\u2019re called dictionaries. You can make a new hash table using the dict function: book = dict () book [ 'apple' ] = 0.67 book [ 'milk' ] = 1.49 book [ 'avocado' ] = 1.49 print ( book ) print ( book [ 'avocado' ]) {'apple': 0.67, 'milk': 1.49, 'avocado': 1.49} 1.49 A hash table has keys and values. In the book hash, the names of produce are the keys, and their prices are the values. A hash table maps keys to values. Hash Table (Average) Hash Table (Worst) Arrays Linked List Search O(1) O(n) O(1) O(n) Insert O(1) O(n) O(n) O(1) Delete O(1) O(n) O(n) O(1) You can make a hash table by combining a hash function with an array. Collisions are bad. You need a hash function that minimizes collisions. Hash tables have really fast search, insert, and delete. Hash tables are good for modeling relationships from one item to another item. Once your load factor is greater than .07, it\u2019s time to resize your hash table. Hash tables are used for caching data (for example, with a web server). Hash tables are great for catching duplicates. # 2-Sum Interview Problem Solution def two_sum_hash_table ( arr , total ): hash_table = {} for idx , val in enumerate ( arr ): complement = total - val if complement in hash_table : return idx , hash_table [ complement ] else : # The hash table stores the values from the array as keys, and the indices of # these values as the values in the table. print ( f \"Adding to hash table: key: { val } , value: { idx } \" ) hash_table [ val ] = idx return None assert two_sum_hash_table ([ 1 , 2 , 3 ], 4 ) in [( 0 , 2 ), ( 2 , 0 )] assert two_sum_hash_table ([ 1234 , 5678 , 9012 ], 14690 ) in [( 1 , 2 ), ( 2 , 1 )] assert two_sum_hash_table ([ 2 , 2 , 3 ], 4 ) in [( 0 , 1 ), ( 1 , 0 )] assert two_sum_hash_table ([ 2 , 2 ], 4 ) in [( 0 , 1 ), ( 1 , 0 )] assert two_sum_hash_table ([ 8 , 7 , 2 , 5 , 3 , 1 ], 10 ) in [( 0 , 2 ), ( 2 , 0 ), ( 1 , 4 ), ( 4 , 1 )] Adding to hash table: key:1, value: 0 Adding to hash table: key:2, value: 1 Adding to hash table: key:1234, value: 0 Adding to hash table: key:5678, value: 1 Adding to hash table: key:2, value: 0 Adding to hash table: key:2, value: 0 Adding to hash table: key:8, value: 0 Adding to hash table: key:7, value: 1 Ransom note example: def ransom_note ( magazine , note ): mag_words = {} for word in magazine : if word in mag_words : mag_words [ word ] += 1 else : mag_words [ word ] = 1 for word in note : if mag_words . get ( word , 0 ) < 1 : return False else : mag_words [ word ] -= 1 return True # # This criminal has no regard for punctuation magazine = \"give me one grand today night\" . split () note = \"give one grand today\" . split () assert ransom_note ( magazine , note ) is True magazine = \"two times three is not four\" . split () note = \"two times two is four\" . split () assert ransom_note ( magazine , note ) is False from collections import Counter def ransom_note ( magazine , note ): mag_counter = Counter ( magazine ) note_counter = Counter ( note ) # intersection operator &. This returns the minimum of corresponding counts. return mag_counter & note_counter == note_counter # # This criminal has no regard for punctuation magazine = \"give me one grand today night\" . split () note = \"give one grand today\" . split () assert ransom_note ( magazine , note ) is True magazine = \"two times three is not four\" . split () note = \"two times two is four\" . split () assert ransom_note ( magazine , note ) is False Breadth-first search Make a queue to start. In Python, you use the double-ended queue (deque) function for this: graph = {} graph [ 'you' ] = [ 'alice' , 'bob' , 'claire' ] graph [ 'bob' ] = [ 'anuj' , 'peggy' ] graph [ 'alice' ] = [ 'peggy' ] graph [ 'claire' ] = [ 'thom' , 'jonny' ] graph [ 'anuj' ] = [] graph [ 'peggy' ] = [] graph [ 'thom' ] = [] graph [ 'jonny' ] = [] from collections import deque def person_is_seller ( name ): return name [ - 1 ] == 'm' def search ( name ): search_queue = deque () search_queue += graph [ name ] searched = [] while search_queue : person = search_queue . popleft () if not person in searched : if person_is_seller ( person ): print ( person + ' is a mango seller!' ) return True else : search_queue += graph [ person ] searched . append ( person ) return False search ( 'you' ) thom is a mango seller! True Breadth-first search tells you if there\u2019s a path from A to B. If there\u2019s a path, breadth-first search will find the shortest path. If you have a problem like \u201cfind the shortest X,\u201d try modeling your problem as a graph, and use breadth-first search to solve. A directed graph has arrows, and the relationship follows the direction of the arrow (rama -> adit means \u201crama owes adit money\u201d). Undirected graphs don\u2019t have arrows, and the relationship goes both ways (ross - rachel means \u201cross dated rachel and rachel dated ross\u201d). Queues are FIFO (First In, First Out). Stacks are LIFO (Last In, First Out). You need to check people in the order they were added to the search list, so the search list needs to be a queue. Otherwise, you won\u2019t get the shortest path. Once you check someone, make sure you don\u2019t check them again. Otherwise, you might end up in an infinite loop. Depth-first search BFS uses queues and DFS uses Stacks DFS starts at the root of the tree and selects the first child. If the child has children, it selects the first child again. When it gets to a node with no children, it backtracks, moving up the tree to the parent node, where it selects the next child if there is one; otherwise it backtracks again. When it has explored the last child of the root, it\u2019s done. There are two common ways to implement DFS, recursively and iteratively. The recursive implementation looks like this: html_doc = \"\"\" <html><head><title>The Dormouse's story</title></head> <body> <p class=\"title\"><b>The Dormouse's story</b></p> <p class=\"story\">Once upon a time there were three little sisters; and their names were <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>, <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and <a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>; and they lived at the bottom of a well.</p> <p class=\"story\">...</p> \"\"\" from bs4 import BeautifulSoup soup = BeautifulSoup ( html_doc ) type ( soup ) #bs4.BeautifulSoup from bs4 import Tag , NavigableString def print_element ( element ): if isinstance ( element , Tag ): print ( f ' { type ( element ) . __name__ } < { element . name } >' ) if isinstance ( element , NavigableString ): print ( type ( element ) . __name__ ) element = soup . contents [ 0 ] print_element ( element ) # Tag<html> def recursive_DFS ( element ): if isinstance ( element , NavigableString ): print ( element , end = '' ) return for child in element . children : recursive_DFS ( child ) recursive_DFS ( soup ) The Dormouse's story The Dormouse's story Once upon a time there were three little sisters; and their names were Elsie, Lacie and Tillie; and they lived at the bottom of a well. ... Here is an iterative version of DFS that uses a list to represent a stack of elements: def iterative_DFS ( root ): stack = [ root ] while ( stack ): element = stack . pop () if isinstance ( element , NavigableString ): print ( element , end = '' ) else : children = reversed ( element . contents ) stack . extend ( children ) The parameter, root, is the root of the tree we want to traverse, so we start by creating the stack and pushing the root onto it. The loop continues until the stack is empty. Each time through, it pops a PageElement off the stack. If it gets a NavigableString, it prints the contents. Then it pushes the children onto the stack. In order to process the children in the right order, we have to push them onto the stack in reverse order. iterative_DFS ( soup ) The Dormouse's story The Dormouse's story Once upon a time there were three little sisters; and their names were Elsie, Lacie and Tillie; and they lived at the bottom of a well. ... A* Search Algorithm A* Search algorithm is one of the best and popular technique used in path-finding and graph traversals. Consider a square grid having many obstacles and we are given a starting cell and a target cell. We want to reach the target cell (if possible) from the starting cell as quickly as possible. Here A* Search Algorithm comes to the rescue. What A* Search Algorithm does is that at each step it picks the node according to a value-\u2018f\u2019 which is a parameter equal to the sum of two other parameters \u2013 \u2018g\u2019 and \u2018h\u2019. At each step it picks the node/cell having the lowest \u2018f\u2019, and process that node/cell. f = g + h g = the movement cost to move from the starting point to a given square on the grid, following the path generated to get there. h = the estimated movement cost to move from that given square on the grid to the final destination. This is often referred to as the heuristic, which is nothing but a kind of smart guess. We really don\u2019t know the actual distance until we find the path, because all sorts of things can be in the way (walls, water, etc.). There can be many ways to calculate this \u2018h\u2019. Algorithm We create two lists \u2013 Open List and Closed List (just like Dijkstra Algorithm) // A* Search Algorithm 1. Initialize the open list 2. Initialize the closed list put the starting node on the open list (you can leave its f at zero) 3. while the open list is not empty a) find the node with the least f on the open list, call it \"q\" b) pop q off the open list c) generate q's 8 successors and set their parents to q d) for each successor i) if successor is the goal, stop search ii) else, compute both g and h for successor successor.g = q.g + distance between successor and q successor.h = distance from goal to successor (This can be done using many ways, we will discuss three heuristics- Manhattan, Diagonal and Euclidean Heuristics) successor.f = successor.g + successor.h iii) if a node with the same position as successor is in the OPEN list which has a lower f than successor, skip this successor iV) if a node with the same position as successor is in the CLOSED list which has a lower f than successor, skip this successor otherwise, add the node to the open list end (for loop) e) push q on the closed list end (while loop) from collections import deque class Graph : def __init__ ( self , adjac_lis ): self . adjac_lis = adjac_lis def get_neighbors ( self , v ): return self . adjac_lis [ v ] # This is heuristic function which is having equal values for all nodes def h ( self , n ): H = { 'A' : 1 , 'B' : 1 , 'C' : 1 , 'D' : 1 } return H [ n ] def a_star_algorithm ( self , start , stop ): # In this open_lst is a lisy of nodes which have been visited, but who's # neighbours haven't all been always inspected, It starts off with the start #node # And closed_lst is a list of nodes which have been visited # and who's neighbors have been always inspected open_lst = set ([ start ]) closed_lst = set ([]) # poo has present distances from start to all other nodes # the default value is +infinity poo = {} poo [ start ] = 0 # par contains an adjac mapping of all nodes par = {} par [ start ] = start while len ( open_lst ) > 0 : n = None # it will find a node with the lowest value of f() - for v in open_lst : if n == None or poo [ v ] + self . h ( v ) < poo [ n ] + self . h ( n ): n = v ; if n == None : print ( 'Path does not exist!' ) return None # if the current node is the stop # then we start again from start if n == stop : reconst_path = [] while par [ n ] != n : reconst_path . append ( n ) n = par [ n ] reconst_path . append ( start ) reconst_path . reverse () print ( 'Path found: {} ' . format ( reconst_path )) return reconst_path # for all the neighbors of the current node do for ( m , weight ) in self . get_neighbors ( n ): # if the current node is not presentin both open_lst and closed_lst # add it to open_lst and note n as it's par if m not in open_lst and m not in closed_lst : open_lst . add ( m ) par [ m ] = n poo [ m ] = poo [ n ] + weight # otherwise, check if it's quicker to first visit n, then m # and if it is, update par data and poo data # and if the node was in the closed_lst, move it to open_lst else : if poo [ m ] > poo [ n ] + weight : poo [ m ] = poo [ n ] + weight par [ m ] = n if m in closed_lst : closed_lst . remove ( m ) open_lst . add ( m ) # remove n from the open_lst, and add it to closed_lst # because all of his neighbors were inspected open_lst . remove ( n ) closed_lst . add ( n ) print ( 'Path does not exist!' ) return None adjac_lis = { 'A' : [( 'B' , 1 ), ( 'C' , 3 ), ( 'D' , 7 )], 'B' : [( 'D' , 5 )], 'C' : [( 'D' , 12 )] } graph1 = Graph ( adjac_lis ) graph1 . a_star_algorithm ( 'A' , 'D' ) Path found: ['A', 'B', 'D'] ['A', 'B', 'D'] A-star can be implemented using a priority QUEUE. # Credit for this: Nicholas Swift # as found at https://medium.com/@nicholas.w.swift/easy-a-star-pathfinding-7e6689c7f7b2 from warnings import warn import heapq class Node : \"\"\" A node class for A* Pathfinding \"\"\" def __init__ ( self , parent = None , position = None ): self . parent = parent self . position = position self . g = 0 self . h = 0 self . f = 0 def __eq__ ( self , other ): return self . position == other . position def __repr__ ( self ): return f \" { self . position } - g: { self . g } h: { self . h } f: { self . f } \" # defining less than for purposes of heap queue def __lt__ ( self , other ): return self . f < other . f # defining greater than for purposes of heap queue def __gt__ ( self , other ): return self . f > other . f def return_path ( current_node ): path = [] current = current_node while current is not None : path . append ( current . position ) current = current . parent return path [:: - 1 ] # Return reversed path def astar ( maze , start , end , allow_diagonal_movement = False ): \"\"\" Returns a list of tuples as a path from the given start to the given end in the given maze :param maze: :param start: :param end: :return: \"\"\" # Create start and end node start_node = Node ( None , start ) start_node . g = start_node . h = start_node . f = 0 end_node = Node ( None , end ) end_node . g = end_node . h = end_node . f = 0 # Initialize both open and closed list open_list = [] closed_list = [] # Heapify the open_list and Add the start node heapq . heapify ( open_list ) heapq . heappush ( open_list , start_node ) # Adding a stop condition outer_iterations = 0 max_iterations = ( len ( maze [ 0 ]) * len ( maze ) // 2 ) # what squares do we search adjacent_squares = (( 0 , - 1 ), ( 0 , 1 ), ( - 1 , 0 ), ( 1 , 0 ),) if allow_diagonal_movement : adjacent_squares = (( 0 , - 1 ), ( 0 , 1 ), ( - 1 , 0 ), ( 1 , 0 ), ( - 1 , - 1 ), ( - 1 , 1 ), ( 1 , - 1 ), ( 1 , 1 ),) # Loop until you find the end while len ( open_list ) > 0 : outer_iterations += 1 if outer_iterations > max_iterations : # if we hit this point return the path such as it is # it will not contain the destination warn ( \"giving up on pathfinding too many iterations\" ) return return_path ( current_node ) # Get the current node current_node = heapq . heappop ( open_list ) closed_list . append ( current_node ) # Found the goal if current_node == end_node : return return_path ( current_node ) # Generate children children = [] for new_position in adjacent_squares : # Adjacent squares # Get node position node_position = ( current_node . position [ 0 ] + new_position [ 0 ], current_node . position [ 1 ] + new_position [ 1 ]) # Make sure within range if node_position [ 0 ] > ( len ( maze ) - 1 ) or node_position [ 0 ] < 0 or node_position [ 1 ] > ( len ( maze [ len ( maze ) - 1 ]) - 1 ) or node_position [ 1 ] < 0 : continue # Make sure walkable terrain if maze [ node_position [ 0 ]][ node_position [ 1 ]] != 0 : continue # Create new node new_node = Node ( current_node , node_position ) # Append children . append ( new_node ) # Loop through children for child in children : # Child is on the closed list if len ([ closed_child for closed_child in closed_list if closed_child == child ]) > 0 : continue # Create the f, g, and h values child . g = current_node . g + 1 child . h = (( child . position [ 0 ] - end_node . position [ 0 ]) ** 2 ) + (( child . position [ 1 ] - end_node . position [ 1 ]) ** 2 ) child . f = child . g + child . h # Child is already in the open list if len ([ open_node for open_node in open_list if child . position == open_node . position and child . g > open_node . g ]) > 0 : continue # Add the child to the open list heapq . heappush ( open_list , child ) warn ( \"Couldn't get a path to destination\" ) return None def example ( print_maze = True ): maze = [[ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ,] * 2 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ,] * 2 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ,] * 2 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ,] * 2 , [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 1 , 0 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 1 , 1 , 1 , 1 , 0 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 0 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 1 , 1 , 0 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 1 , 0 , 1 , 1 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 1 , 0 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 1 , 0 , 1 , 0 , 0 , 0 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 1 , 0 , 1 , 1 , 1 , 0 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 1 , 1 , 1 , 1 , 0 , 1 , 0 , 0 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 1 , 0 , 0 , 0 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 1 , 1 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 ,] * 2 , [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 ,] * 2 ,] start = ( 0 , 0 ) end = ( len ( maze ) - 1 , len ( maze [ 0 ]) - 1 ) path = astar ( maze , start , end ) if print_maze : for step in path : maze [ step [ 0 ]][ step [ 1 ]] = 2 for row in maze : line = [] for col in row : if col == 1 : line . append ( \" \\u2588 \" ) elif col == 0 : line . append ( \" \" ) elif col == 2 : line . append ( \".\" ) print ( \"\" . join ( line )) print ( path )","title":"Searching Algorithms"},{"location":"Algo/pySearch/#binary-search","text":"Binary search only works when your list is in sorted order. For example, the names in a phone book are sorted in alphabetical order, so you can use binary search to look for a name. Binary search is a lot faster than simple search. His time complexity is O(log n) is faster than O(n) -Simple search, but it gets a lot faster once the list of items you\u2019re searching through grows. Algorithm speed isn\u2019t measured in seconds. Algorithm times are measured in terms of growth of an algorithm. Algorithm times are written in Big O notation. def binary_search ( list , item ): low = 0 high = len ( list ) - 1 while low <= high : mid = ( low + high ) guess = list [ mid ] if guess == item : return mid if guess > item : high = mid - 1 else : low = mid + 1 return None my_list = [ 1 , 3 , 5 , 7 , 9 ] print ( binary_search ( my_list , 3 )) # => 1 print ( binary_search ( my_list , - 1 )) # => None 1 None","title":"Binary Search"},{"location":"Algo/pySearch/#hash-tables","text":"Hashes are good for: Modeling relationships from one thing to another thing Filtering out duplicates Caching/memorizing data instead of making your server do work Python has hash tables; they\u2019re called dictionaries. You can make a new hash table using the dict function: book = dict () book [ 'apple' ] = 0.67 book [ 'milk' ] = 1.49 book [ 'avocado' ] = 1.49 print ( book ) print ( book [ 'avocado' ]) {'apple': 0.67, 'milk': 1.49, 'avocado': 1.49} 1.49 A hash table has keys and values. In the book hash, the names of produce are the keys, and their prices are the values. A hash table maps keys to values. Hash Table (Average) Hash Table (Worst) Arrays Linked List Search O(1) O(n) O(1) O(n) Insert O(1) O(n) O(n) O(1) Delete O(1) O(n) O(n) O(1) You can make a hash table by combining a hash function with an array. Collisions are bad. You need a hash function that minimizes collisions. Hash tables have really fast search, insert, and delete. Hash tables are good for modeling relationships from one item to another item. Once your load factor is greater than .07, it\u2019s time to resize your hash table. Hash tables are used for caching data (for example, with a web server). Hash tables are great for catching duplicates. # 2-Sum Interview Problem Solution def two_sum_hash_table ( arr , total ): hash_table = {} for idx , val in enumerate ( arr ): complement = total - val if complement in hash_table : return idx , hash_table [ complement ] else : # The hash table stores the values from the array as keys, and the indices of # these values as the values in the table. print ( f \"Adding to hash table: key: { val } , value: { idx } \" ) hash_table [ val ] = idx return None assert two_sum_hash_table ([ 1 , 2 , 3 ], 4 ) in [( 0 , 2 ), ( 2 , 0 )] assert two_sum_hash_table ([ 1234 , 5678 , 9012 ], 14690 ) in [( 1 , 2 ), ( 2 , 1 )] assert two_sum_hash_table ([ 2 , 2 , 3 ], 4 ) in [( 0 , 1 ), ( 1 , 0 )] assert two_sum_hash_table ([ 2 , 2 ], 4 ) in [( 0 , 1 ), ( 1 , 0 )] assert two_sum_hash_table ([ 8 , 7 , 2 , 5 , 3 , 1 ], 10 ) in [( 0 , 2 ), ( 2 , 0 ), ( 1 , 4 ), ( 4 , 1 )] Adding to hash table: key:1, value: 0 Adding to hash table: key:2, value: 1 Adding to hash table: key:1234, value: 0 Adding to hash table: key:5678, value: 1 Adding to hash table: key:2, value: 0 Adding to hash table: key:2, value: 0 Adding to hash table: key:8, value: 0 Adding to hash table: key:7, value: 1 Ransom note example: def ransom_note ( magazine , note ): mag_words = {} for word in magazine : if word in mag_words : mag_words [ word ] += 1 else : mag_words [ word ] = 1 for word in note : if mag_words . get ( word , 0 ) < 1 : return False else : mag_words [ word ] -= 1 return True # # This criminal has no regard for punctuation magazine = \"give me one grand today night\" . split () note = \"give one grand today\" . split () assert ransom_note ( magazine , note ) is True magazine = \"two times three is not four\" . split () note = \"two times two is four\" . split () assert ransom_note ( magazine , note ) is False from collections import Counter def ransom_note ( magazine , note ): mag_counter = Counter ( magazine ) note_counter = Counter ( note ) # intersection operator &. This returns the minimum of corresponding counts. return mag_counter & note_counter == note_counter # # This criminal has no regard for punctuation magazine = \"give me one grand today night\" . split () note = \"give one grand today\" . split () assert ransom_note ( magazine , note ) is True magazine = \"two times three is not four\" . split () note = \"two times two is four\" . split () assert ransom_note ( magazine , note ) is False","title":"Hash Tables"},{"location":"Algo/pySearch/#breadth-first-search","text":"Make a queue to start. In Python, you use the double-ended queue (deque) function for this: graph = {} graph [ 'you' ] = [ 'alice' , 'bob' , 'claire' ] graph [ 'bob' ] = [ 'anuj' , 'peggy' ] graph [ 'alice' ] = [ 'peggy' ] graph [ 'claire' ] = [ 'thom' , 'jonny' ] graph [ 'anuj' ] = [] graph [ 'peggy' ] = [] graph [ 'thom' ] = [] graph [ 'jonny' ] = [] from collections import deque def person_is_seller ( name ): return name [ - 1 ] == 'm' def search ( name ): search_queue = deque () search_queue += graph [ name ] searched = [] while search_queue : person = search_queue . popleft () if not person in searched : if person_is_seller ( person ): print ( person + ' is a mango seller!' ) return True else : search_queue += graph [ person ] searched . append ( person ) return False search ( 'you' ) thom is a mango seller! True Breadth-first search tells you if there\u2019s a path from A to B. If there\u2019s a path, breadth-first search will find the shortest path. If you have a problem like \u201cfind the shortest X,\u201d try modeling your problem as a graph, and use breadth-first search to solve. A directed graph has arrows, and the relationship follows the direction of the arrow (rama -> adit means \u201crama owes adit money\u201d). Undirected graphs don\u2019t have arrows, and the relationship goes both ways (ross - rachel means \u201cross dated rachel and rachel dated ross\u201d). Queues are FIFO (First In, First Out). Stacks are LIFO (Last In, First Out). You need to check people in the order they were added to the search list, so the search list needs to be a queue. Otherwise, you won\u2019t get the shortest path. Once you check someone, make sure you don\u2019t check them again. Otherwise, you might end up in an infinite loop.","title":"Breadth-first search"},{"location":"Algo/pySearch/#depth-first-search","text":"BFS uses queues and DFS uses Stacks DFS starts at the root of the tree and selects the first child. If the child has children, it selects the first child again. When it gets to a node with no children, it backtracks, moving up the tree to the parent node, where it selects the next child if there is one; otherwise it backtracks again. When it has explored the last child of the root, it\u2019s done. There are two common ways to implement DFS, recursively and iteratively. The recursive implementation looks like this: html_doc = \"\"\" <html><head><title>The Dormouse's story</title></head> <body> <p class=\"title\"><b>The Dormouse's story</b></p> <p class=\"story\">Once upon a time there were three little sisters; and their names were <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>, <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and <a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>; and they lived at the bottom of a well.</p> <p class=\"story\">...</p> \"\"\" from bs4 import BeautifulSoup soup = BeautifulSoup ( html_doc ) type ( soup ) #bs4.BeautifulSoup from bs4 import Tag , NavigableString def print_element ( element ): if isinstance ( element , Tag ): print ( f ' { type ( element ) . __name__ } < { element . name } >' ) if isinstance ( element , NavigableString ): print ( type ( element ) . __name__ ) element = soup . contents [ 0 ] print_element ( element ) # Tag<html> def recursive_DFS ( element ): if isinstance ( element , NavigableString ): print ( element , end = '' ) return for child in element . children : recursive_DFS ( child ) recursive_DFS ( soup ) The Dormouse's story The Dormouse's story Once upon a time there were three little sisters; and their names were Elsie, Lacie and Tillie; and they lived at the bottom of a well. ... Here is an iterative version of DFS that uses a list to represent a stack of elements: def iterative_DFS ( root ): stack = [ root ] while ( stack ): element = stack . pop () if isinstance ( element , NavigableString ): print ( element , end = '' ) else : children = reversed ( element . contents ) stack . extend ( children ) The parameter, root, is the root of the tree we want to traverse, so we start by creating the stack and pushing the root onto it. The loop continues until the stack is empty. Each time through, it pops a PageElement off the stack. If it gets a NavigableString, it prints the contents. Then it pushes the children onto the stack. In order to process the children in the right order, we have to push them onto the stack in reverse order. iterative_DFS ( soup ) The Dormouse's story The Dormouse's story Once upon a time there were three little sisters; and their names were Elsie, Lacie and Tillie; and they lived at the bottom of a well. ...","title":"Depth-first search"},{"location":"Algo/pySearch/#a-search-algorithm","text":"A* Search algorithm is one of the best and popular technique used in path-finding and graph traversals. Consider a square grid having many obstacles and we are given a starting cell and a target cell. We want to reach the target cell (if possible) from the starting cell as quickly as possible. Here A* Search Algorithm comes to the rescue. What A* Search Algorithm does is that at each step it picks the node according to a value-\u2018f\u2019 which is a parameter equal to the sum of two other parameters \u2013 \u2018g\u2019 and \u2018h\u2019. At each step it picks the node/cell having the lowest \u2018f\u2019, and process that node/cell. f = g + h g = the movement cost to move from the starting point to a given square on the grid, following the path generated to get there. h = the estimated movement cost to move from that given square on the grid to the final destination. This is often referred to as the heuristic, which is nothing but a kind of smart guess. We really don\u2019t know the actual distance until we find the path, because all sorts of things can be in the way (walls, water, etc.). There can be many ways to calculate this \u2018h\u2019. Algorithm We create two lists \u2013 Open List and Closed List (just like Dijkstra Algorithm) // A* Search Algorithm 1. Initialize the open list 2. Initialize the closed list put the starting node on the open list (you can leave its f at zero) 3. while the open list is not empty a) find the node with the least f on the open list, call it \"q\" b) pop q off the open list c) generate q's 8 successors and set their parents to q d) for each successor i) if successor is the goal, stop search ii) else, compute both g and h for successor successor.g = q.g + distance between successor and q successor.h = distance from goal to successor (This can be done using many ways, we will discuss three heuristics- Manhattan, Diagonal and Euclidean Heuristics) successor.f = successor.g + successor.h iii) if a node with the same position as successor is in the OPEN list which has a lower f than successor, skip this successor iV) if a node with the same position as successor is in the CLOSED list which has a lower f than successor, skip this successor otherwise, add the node to the open list end (for loop) e) push q on the closed list end (while loop) from collections import deque class Graph : def __init__ ( self , adjac_lis ): self . adjac_lis = adjac_lis def get_neighbors ( self , v ): return self . adjac_lis [ v ] # This is heuristic function which is having equal values for all nodes def h ( self , n ): H = { 'A' : 1 , 'B' : 1 , 'C' : 1 , 'D' : 1 } return H [ n ] def a_star_algorithm ( self , start , stop ): # In this open_lst is a lisy of nodes which have been visited, but who's # neighbours haven't all been always inspected, It starts off with the start #node # And closed_lst is a list of nodes which have been visited # and who's neighbors have been always inspected open_lst = set ([ start ]) closed_lst = set ([]) # poo has present distances from start to all other nodes # the default value is +infinity poo = {} poo [ start ] = 0 # par contains an adjac mapping of all nodes par = {} par [ start ] = start while len ( open_lst ) > 0 : n = None # it will find a node with the lowest value of f() - for v in open_lst : if n == None or poo [ v ] + self . h ( v ) < poo [ n ] + self . h ( n ): n = v ; if n == None : print ( 'Path does not exist!' ) return None # if the current node is the stop # then we start again from start if n == stop : reconst_path = [] while par [ n ] != n : reconst_path . append ( n ) n = par [ n ] reconst_path . append ( start ) reconst_path . reverse () print ( 'Path found: {} ' . format ( reconst_path )) return reconst_path # for all the neighbors of the current node do for ( m , weight ) in self . get_neighbors ( n ): # if the current node is not presentin both open_lst and closed_lst # add it to open_lst and note n as it's par if m not in open_lst and m not in closed_lst : open_lst . add ( m ) par [ m ] = n poo [ m ] = poo [ n ] + weight # otherwise, check if it's quicker to first visit n, then m # and if it is, update par data and poo data # and if the node was in the closed_lst, move it to open_lst else : if poo [ m ] > poo [ n ] + weight : poo [ m ] = poo [ n ] + weight par [ m ] = n if m in closed_lst : closed_lst . remove ( m ) open_lst . add ( m ) # remove n from the open_lst, and add it to closed_lst # because all of his neighbors were inspected open_lst . remove ( n ) closed_lst . add ( n ) print ( 'Path does not exist!' ) return None adjac_lis = { 'A' : [( 'B' , 1 ), ( 'C' , 3 ), ( 'D' , 7 )], 'B' : [( 'D' , 5 )], 'C' : [( 'D' , 12 )] } graph1 = Graph ( adjac_lis ) graph1 . a_star_algorithm ( 'A' , 'D' ) Path found: ['A', 'B', 'D'] ['A', 'B', 'D'] A-star can be implemented using a priority QUEUE. # Credit for this: Nicholas Swift # as found at https://medium.com/@nicholas.w.swift/easy-a-star-pathfinding-7e6689c7f7b2 from warnings import warn import heapq class Node : \"\"\" A node class for A* Pathfinding \"\"\" def __init__ ( self , parent = None , position = None ): self . parent = parent self . position = position self . g = 0 self . h = 0 self . f = 0 def __eq__ ( self , other ): return self . position == other . position def __repr__ ( self ): return f \" { self . position } - g: { self . g } h: { self . h } f: { self . f } \" # defining less than for purposes of heap queue def __lt__ ( self , other ): return self . f < other . f # defining greater than for purposes of heap queue def __gt__ ( self , other ): return self . f > other . f def return_path ( current_node ): path = [] current = current_node while current is not None : path . append ( current . position ) current = current . parent return path [:: - 1 ] # Return reversed path def astar ( maze , start , end , allow_diagonal_movement = False ): \"\"\" Returns a list of tuples as a path from the given start to the given end in the given maze :param maze: :param start: :param end: :return: \"\"\" # Create start and end node start_node = Node ( None , start ) start_node . g = start_node . h = start_node . f = 0 end_node = Node ( None , end ) end_node . g = end_node . h = end_node . f = 0 # Initialize both open and closed list open_list = [] closed_list = [] # Heapify the open_list and Add the start node heapq . heapify ( open_list ) heapq . heappush ( open_list , start_node ) # Adding a stop condition outer_iterations = 0 max_iterations = ( len ( maze [ 0 ]) * len ( maze ) // 2 ) # what squares do we search adjacent_squares = (( 0 , - 1 ), ( 0 , 1 ), ( - 1 , 0 ), ( 1 , 0 ),) if allow_diagonal_movement : adjacent_squares = (( 0 , - 1 ), ( 0 , 1 ), ( - 1 , 0 ), ( 1 , 0 ), ( - 1 , - 1 ), ( - 1 , 1 ), ( 1 , - 1 ), ( 1 , 1 ),) # Loop until you find the end while len ( open_list ) > 0 : outer_iterations += 1 if outer_iterations > max_iterations : # if we hit this point return the path such as it is # it will not contain the destination warn ( \"giving up on pathfinding too many iterations\" ) return return_path ( current_node ) # Get the current node current_node = heapq . heappop ( open_list ) closed_list . append ( current_node ) # Found the goal if current_node == end_node : return return_path ( current_node ) # Generate children children = [] for new_position in adjacent_squares : # Adjacent squares # Get node position node_position = ( current_node . position [ 0 ] + new_position [ 0 ], current_node . position [ 1 ] + new_position [ 1 ]) # Make sure within range if node_position [ 0 ] > ( len ( maze ) - 1 ) or node_position [ 0 ] < 0 or node_position [ 1 ] > ( len ( maze [ len ( maze ) - 1 ]) - 1 ) or node_position [ 1 ] < 0 : continue # Make sure walkable terrain if maze [ node_position [ 0 ]][ node_position [ 1 ]] != 0 : continue # Create new node new_node = Node ( current_node , node_position ) # Append children . append ( new_node ) # Loop through children for child in children : # Child is on the closed list if len ([ closed_child for closed_child in closed_list if closed_child == child ]) > 0 : continue # Create the f, g, and h values child . g = current_node . g + 1 child . h = (( child . position [ 0 ] - end_node . position [ 0 ]) ** 2 ) + (( child . position [ 1 ] - end_node . position [ 1 ]) ** 2 ) child . f = child . g + child . h # Child is already in the open list if len ([ open_node for open_node in open_list if child . position == open_node . position and child . g > open_node . g ]) > 0 : continue # Add the child to the open list heapq . heappush ( open_list , child ) warn ( \"Couldn't get a path to destination\" ) return None def example ( print_maze = True ): maze = [[ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ,] * 2 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ,] * 2 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ,] * 2 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ,] * 2 , [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 1 , 0 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 1 , 1 , 1 , 1 , 0 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 0 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 1 , 1 , 0 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 1 , 0 , 1 , 1 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 1 , 0 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 1 , 0 , 1 , 0 , 0 , 0 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 1 , 0 , 1 , 1 , 1 , 0 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 1 , 1 , 1 , 1 , 0 , 1 , 0 , 0 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 1 , 0 , 0 , 0 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 1 , 1 ,] * 2 , [ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 ,] * 2 , [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 ,] * 2 ,] start = ( 0 , 0 ) end = ( len ( maze ) - 1 , len ( maze [ 0 ]) - 1 ) path = astar ( maze , start , end ) if print_maze : for step in path : maze [ step [ 0 ]][ step [ 1 ]] = 2 for row in maze : line = [] for col in row : if col == 1 : line . append ( \" \\u2588 \" ) elif col == 0 : line . append ( \" \" ) elif col == 2 : line . append ( \".\" ) print ( \"\" . join ( line )) print ( path )","title":"A* Search Algorithm"},{"location":"Algo/pySort/","text":"There are two types of sorting algorithms: Comparison sorting algorithms Non-comparison sorting algorithms Complexity for Comparison Sorting Algorithm Best Average Worst Memory Stable Method Selection Sort O(n^2) O(n^2) O(n^2) 1 No Selection Bubble Sort O(n) O(n^2) O(n^2) 1 Yes Exchanging Insertion Sort O(n) O(n^2) O(n^2) 1 Yes Insertion Heap Sort O(n log(n)) O(n log(n)) O(n log(n)) 1 No Selection Quick Sort O(n log(n)) O(n log(n)) O(n^2) log(n) No Partitioning Merge Sort O(n log(n)) O(n log(n)) O(n log(n)) n Yes Merging NumPy provides implementations of three sorting algorithms, quicksort, mergesort, and heapsort. Complexity for Non-Comparison Sorting The following table describes integer sorting algorithms and other sorting algorithms that are not comparison sorts. As such, they are not limited to \u03a9(n log n). Algorithm Best Average Worst Memory Stable Bucket Sort O(n+k) O(n+k) O(n^2) O(n*k) Yes Radix Sort O(nk) O(nk) O(nk) O(n+2^d) Yes Counting Sort O(n+k) O(n+k) O(n+k) O(n+r) Yes Bubble Sort Bubble Sort is the simplest sorting algorithm that works by repeatedly swapping the adjacent elements if they are in wrong order. Two values are compared to each other incrementally, the largest value is shifted until it is at the top. Complexity: O(n^2) Worst and Average Case Time Complexity: O(n*n). Worst case occurs when array is reverse sorted. Best Case Time Complexity: O(n). Best case occurs when array is already sorted. Auxiliary Space: O(1) Boundary Cases: Bubble sort takes minimum time (Order of n) when elements are already sorted. Sorting In Place: Yes Stable: Yes # Python program for implementation of Bubble Sort def bubbleSort ( arr ): n = len ( arr ) # Traverse through all array elements for i in range ( n ): # Last i elements are already in place for j in range ( 0 , n - i - 1 ): # traverse the array from 0 to n-i-1 # Swap if the element found is greater # than the next element if arr [ j ] > arr [ j + 1 ] : arr [ j ], arr [ j + 1 ] = arr [ j + 1 ], arr [ j ] # Driver code to test above arr = [ 64 , 34 , 25 , 12 , 22 , 11 , 90 ] bubbleSort ( arr ) print ( \"Sorted array is:\" ) for i in range ( len ( arr )): print ( \" %d \" % arr [ i ]), Sorted array is: 11 12 22 25 34 64 90 Selection Sort When you want to store multiple elements, use an array or a list. \u2022 With an array, all your elements are stored right next to each other. \u2022 With a list, elements are strewn all over, and one element stores the address of the next one. \u2022 Arrays allow fast reads. \u2022 Linked lists allow fast inserts and deletes. \u2022 All elements in the array should be the same type (all ints, all doubles, and so on). The selection sort algorithm sorts an array by repeatedly finding the minimum element (considering ascending order) from unsorted part and putting it at the beginning. The algorithm maintains two subarrays in a given array. 1) The subarray which is already sorted. 2) Remaining subarray which is unsorted. In every iteration of selection sort, the minimum element (considering ascending order) from the unsorted subarray is picked and moved to the sorted subarray. Time Complexity: O(n2) as there are two nested loops. Auxiliary Space: O(1) The good thing about selection sort is it never makes more than O(n) swaps and can be useful when memory write is a costly operation. Stability : The default implementation is not stable. However it can be made stable. Please see stable selection sort for details. In Place : Yes, it does not require extra space. A sorting algorithm is said to be stable if two objects with equal or same keys appear in the same order in sorted output as they appear in the input array to be sorted. def findSmallest ( arr ): smallest = arr [ 0 ] #Stores the smallest value smallest_index = 0 #Stores the index of the smallest value for i in range ( 1 , len ( arr )): if arr [ i ] < smallest : smallest = arr [ i ] smallest_index = i return smallest_index def selectionSort ( arr ): #Sorts an array newArr = [] for i in range ( len ( arr )): smallest = findSmallest ( arr ) newArr . append ( arr . pop ( smallest )) return newArr print ( selectionSort ([ 5 , 3 , 6 , 2 , 10 ])) [2, 3, 5, 6, 10] # Python program for implementation of Selection # Sort import sys A = [ 64 , 25 , 12 , 22 , 11 ] # Traverse through all array elements for i in range ( len ( A )): # Find the minimum element in remaining # unsorted array min_idx = i for j in range ( i + 1 , len ( A )): if A [ min_idx ] > A [ j ]: min_idx = j # Swap the found minimum element with # the first element A [ i ], A [ min_idx ] = A [ min_idx ], A [ i ] # Driver code to test above print ( \"Sorted array\" ) for i in range ( len ( A )): print ( \" %d \" % A [ i ]), Sorted array 11 12 22 25 64 QuickSort (Divide and Conquer) Quicksort is a sorting algorithm. It\u2019s much faster than selection sort and is frequently used in real life. For example, the C standard library has a function called qsort, which is its implementation of quicksort. A quick sort sets a pit point to partition a data set. High and low index values are then used to rearrange data values that are on the \"wrong\" side of the pivot point. Like Merge Sort, QuickSort is a Divide and Conquer algorithm. It picks an element as pivot and partitions the given array around the picked pivot. There are many different versions of quickSort that pick pivot in different ways. Always pick first element as pivot. Always pick last element as pivot (implemented below) Pick a random element as pivot. Pick median as pivot. The key process in quickSort is partition(). Target of partitions is, given an array and an element x of array as pivot, put x at its correct position in sorted array and put all smaller elements (smaller than x) before x, and put all greater elements (greater than x) after x. All this should be done in linear time. # Python3 implementation of QuickSort # This Function handles sorting part of quick sort # start and end points to first and last element of # an array respectively def partition ( start , end , array ): # Initializing pivot's index to start pivot_index = start pivot = array [ pivot_index ] # This loop runs till start pointer crosses # end pointer, and when it does we swap the # pivot with element on end pointer while start < end : # Increment the start pointer till it finds an # element greater than pivot while start < len ( array ) and array [ start ] <= pivot : start += 1 # Decrement the end pointer till it finds an # element less than pivot while array [ end ] > pivot : end -= 1 # If start and end have not crossed each other, # swap the numbers on start and end if ( start < end ): array [ start ], array [ end ] = array [ end ], array [ start ] # Swap pivot element with element on end pointer. # This puts pivot on its correct sorted place. array [ end ], array [ pivot_index ] = array [ pivot_index ], array [ end ] # Returning end pointer to divide the array into 2 return end # The main function that implements QuickSort def quick_sort ( start , end , array ): if ( start < end ): # p is partitioning index, array[p] # is at right place p = partition ( start , end , array ) # Sort elements before partition # and after partition quick_sort ( start , p - 1 , array ) quick_sort ( p + 1 , end , array ) # Driver code array = [ 10 , 7 , 8 , 9 , 1 , 5 ] quick_sort ( 0 , len ( array ) - 1 , array ) print ( f 'Sorted array: { array } ' ) Sorted array: [1, 5, 7, 8, 9, 10] def quicksort ( array ): if len ( array ) < 2 : return array #Base case: arrays with 0 or 1 element are already \u201csorted.\u201d else : pivot = array [ 0 ] #Recursive case less = [ i for i in array [ 1 :] if i <= pivot ] #Sub-array of all the elements less than the pivot greater = [ i for i in array [ 1 :] if i > pivot ] #Sub-array of all the elements greater than the pivot return quicksort ( less ) + [ pivot ] + quicksort ( greater ) print ( quicksort ([ 10 , 5 , 2 , 3 ])) [2, 3, 5, 10] def quicksort ( arr ): if len ( arr ) <= 1 : return arr pivot = arr [ len ( arr ) // 2 ] left = [ x for x in arr if x < pivot ] middle = [ x for x in arr if x == pivot ] right = [ x for x in arr if x > pivot ] return quicksort ( left ) + middle + quicksort ( right ) def quicksort_verbose ( arr ): print ( f \"Calling quicksort on { arr } \" ) if len ( arr ) <= 1 : print ( f \"returning { arr } \" ) return arr pivot = arr [ len ( arr ) // 2 ] left = [ x for x in arr if x < pivot ] print ( f \"left: { left } ; \" , end = \"\" ) middle = [ x for x in arr if x == pivot ] print ( f \"middle: { middle } ; \" , end = \"\" ) right = [ x for x in arr if x > pivot ] print ( f \"right: { right } \" ) to_return = quicksort_verbose ( left ) + middle + quicksort_verbose ( right ) print ( f \"returning: { to_return } \" ) return to_return data = [ 3 , 1 , 4 , 2 ] # print(quicksort(data)) # print(quicksort_verbose(data)) # What about data with duplicates? data = [ 1 , 6 , 5 , 5 , 2 , 6 , 1 ] print ( quicksort ( data )) Merge Sort A merge sort uses recursion. It breaks down the data into smaller manageable sets. As it sorts the smaller sets, it gradually rebuilds and works its way up to the original full data set. Like QuickSort, Merge Sort is a Divide and Conquer algorithm. It divides the input array into two halves, calls itself for the two halves, and then merges the two sorted halves. The merge() function is used for merging two halves. The merge(arr, l, m, r) is a key process that assumes that arr[l..m] and arr[m+1..r] are sorted and merges the two sorted sub-arrays into one. Time Complexity: Sorting arrays on different machines. Merge Sort is a recursive algorithm and time complexity can be expressed as following recurrence relation. T(n) = 2T(n/2) + \u03b8(n) The above recurrence can be solved either using the Recurrence Tree method or the Master method. It falls in case II of Master Method and the solution of the recurrence is \u03b8(nLogn). Time complexity of Merge Sort is \u03b8(nLogn) in all 3 cases (worst, average and best) as merge sort always divides the array into two halves and takes linear time to merge two halves. Auxiliary Space: O(n) Algorithmic Paradigm: Divide and Conquer Sorting In Place: No in a typical implementation Stable: Yes Applications of Merge Sort Merge Sort is useful for sorting linked lists in O(nLogn) time. Inversion Count Problem Used in External Sorting Drawbacks of Merge Sort Slower comparative to the other sort algorithms for smaller tasks. Merge sort algorithm requires an additional memory space of 0(n) for the temporary array. It goes through the whole process even if the array is sorted. # Python program for implementation of MergeSort def mergeSort ( arr ): if len ( arr ) > 1 : # Finding the mid of the array mid = len ( arr ) // 2 # Dividing the array elements L = arr [: mid ] # into 2 halves R = arr [ mid :] # Sorting the first half mergeSort ( L ) # Sorting the second half mergeSort ( R ) i = j = k = 0 # Copy data to temp arrays L[] and R[] while i < len ( L ) and j < len ( R ): if L [ i ] < R [ j ]: arr [ k ] = L [ i ] i += 1 else : arr [ k ] = R [ j ] j += 1 k += 1 # Checking if any element was left while i < len ( L ): arr [ k ] = L [ i ] i += 1 k += 1 while j < len ( R ): arr [ k ] = R [ j ] j += 1 k += 1 # Code to print the list def printList ( arr ): for i in range ( len ( arr )): print ( arr [ i ], end = \" \" ) print () # Driver Code if __name__ == '__main__' : arr = [ 12 , 11 , 13 , 5 , 6 , 7 ] print ( \"Given array is\" , end = \" \\n \" ) printList ( arr ) mergeSort ( arr ) print ( \"Sorted array is: \" , end = \" \\n \" ) printList ( arr ) Given array is 12 11 13 5 6 7 Sorted array is: 5 6 7 11 12 13 Counting Sort Counting sort is a sorting technique based on keys between a specific range. It works by counting the number of objects having distinct key values (kind of hashing). Then doing some arithmetic to calculate the position of each object in the output sequence. Time Complexity: O(n+k) where n is the number of elements in input array and k is the range of input. Auxiliary Space: O(n+k) For simplicity, consider the data in the range 0 to 9. Input data: 1, 4, 1, 2, 7, 5, 2 1) Take a count array to store the count of each unique object. Index: 0 1 2 3 4 5 6 7 8 9 Count: 0 2 2 0 1 1 0 1 0 0 2) Modify the count array such that each element at each index stores the sum of previous counts. Index: 0 1 2 3 4 5 6 7 8 9 Count: 0 2 4 4 5 6 6 7 7 7 The modified count array indicates the position of each object in the output sequence. 3) Rotate the array clockwise for one time. Index: 0 1 2 3 4 5 6 7 8 9 Count: 0 0 2 4 4 5 6 6 7 7 4) Output each object from the input sequence followed by increasing its count by 1. Process the input data: 1, 4, 1, 2, 7, 5, 2. Position of 1 is 0. Put data 1 at index 0 in output. Increase count by 1 to place next data 1 at an index 1 greater than this index. Points to be noted : Counting sort is efficient if the range of input data is not significantly greater than the number of objects to be sorted. Consider the situation where the input sequence is between range 1 to 10K and the data is 10, 5, 10K, 5K. It is not a comparison based sorting. It running time complexity is O(n) with space proportional to the range of data. It is often used as a sub-routine to another sorting algorithm like radix sort. Counting sort uses a partial hashing to count the occurrence of the data object in O(1). Counting sort can be extended to work for negative inputs also. # Python program for counting sort # The main function that sort the given string arr[] in # alphabetical order def countSort ( arr ): # The output character array that will have sorted arr output = [ 0 for i in range ( len ( arr ))] # Create a count array to store count of individual # characters and initialize count array as 0 count = [ 0 for i in range ( 256 )] # For storing the resulting answer since the # string is immutable ans = [ \"\" for _ in arr ] # Store count of each character for i in arr : count [ ord ( i )] += 1 # Change count[i] so that count[i] now contains actual # position of this character in output array for i in range ( 256 ): count [ i ] += count [ i - 1 ] # Build the output character array for i in range ( len ( arr )): output [ count [ ord ( arr [ i ])] - 1 ] = arr [ i ] count [ ord ( arr [ i ])] -= 1 # Copy the output array to arr, so that arr now # contains sorted characters for i in range ( len ( arr )): ans [ i ] = output [ i ] return ans # Driver program to test above function arr = \"meatforall\" ans = countSort ( arr ) print ( \"Sorted character array is % s \" % ( \"\" . join ( ans ))) Sorted character array is aaefllmort Radix Sort The lower bound for Comparison based sorting algorithm (Merge Sort, Heap Sort, Quick-Sort .. etc) is \u03a9(nLogn), i.e., they cannot do better than nLogn. Counting sort is a linear time sorting algorithm that sort in O(n+k) time when elements are in the range from 1 to k. What if the elements are in the range from 1 to n2? We can\u2019t use counting sort because counting sort will take O(n2) which is worse than comparison-based sorting algorithms. Can we sort such an array in linear time? Radix Sort is the answer. The idea of Radix Sort is to do digit by digit sort starting from least significant digit to most significant digit. Radix sort uses counting sort as a subroutine to sort. Original, unsorted list: 170, 45, 75, 90, 802, 24, 2, 66 Sorting by least significant digit (1s place) gives: [*Notice that we keep 802 before 2, because 802 occurred before 2 in the original list, and similarly for pairs 170 & 90 and 45 & 75.] 170, 90, 802, 2, 24, 45, 75, 66 Sorting by next digit (10s place) gives: [*Notice that 802 again comes before 2 as 802 comes before 2 in the previous list.] 802, 2, 24, 45, 66, 170, 75, 90 Sorting by the most significant digit (100s place) gives: 2, 24, 45, 66, 75, 90, 170, 802 # Python program for implementation of Radix Sort # A function to do counting sort of arr[] according to # the digit represented by exp. def countingSort ( arr , exp1 ): n = len ( arr ) # The output array elements that will have sorted arr output = [ 0 ] * ( n ) # initialize count array as 0 count = [ 0 ] * ( 10 ) # Store count of occurrences in count[] for i in range ( 0 , n ): index = arr [ i ] // exp1 count [ index % 10 ] += 1 # Change count[i] so that count[i] now contains actual # position of this digit in output array for i in range ( 1 , 10 ): count [ i ] += count [ i - 1 ] # Build the output array i = n - 1 while i >= 0 : index = arr [ i ] // exp1 output [ count [ index % 10 ] - 1 ] = arr [ i ] count [ index % 10 ] -= 1 i -= 1 # Copying the output array to arr[], # so that arr now contains sorted numbers i = 0 for i in range ( 0 , len ( arr )): arr [ i ] = output [ i ] # Method to do Radix Sort def radixSort ( arr ): # Find the maximum number to know number of digits max1 = max ( arr ) # Do counting sort for every digit. Note that instead # of passing digit number, exp is passed. exp is 10^i # where i is current digit number exp = 1 while max1 / exp > 1 : countingSort ( arr , exp ) exp *= 10 # Driver code arr = [ 170 , 45 , 75 , 90 , 802 , 24 , 2 , 66 ] # Function Call radixSort ( arr ) for i in range ( len ( arr )): print ( arr [ i ], end = \" \" ) 2 24 45 66 75 90 170 802","title":"Sorting Algorithms"},{"location":"Algo/pySort/#complexity-for-comparison-sorting","text":"Algorithm Best Average Worst Memory Stable Method Selection Sort O(n^2) O(n^2) O(n^2) 1 No Selection Bubble Sort O(n) O(n^2) O(n^2) 1 Yes Exchanging Insertion Sort O(n) O(n^2) O(n^2) 1 Yes Insertion Heap Sort O(n log(n)) O(n log(n)) O(n log(n)) 1 No Selection Quick Sort O(n log(n)) O(n log(n)) O(n^2) log(n) No Partitioning Merge Sort O(n log(n)) O(n log(n)) O(n log(n)) n Yes Merging NumPy provides implementations of three sorting algorithms, quicksort, mergesort, and heapsort.","title":"Complexity for Comparison Sorting"},{"location":"Algo/pySort/#complexity-for-non-comparison-sorting","text":"The following table describes integer sorting algorithms and other sorting algorithms that are not comparison sorts. As such, they are not limited to \u03a9(n log n). Algorithm Best Average Worst Memory Stable Bucket Sort O(n+k) O(n+k) O(n^2) O(n*k) Yes Radix Sort O(nk) O(nk) O(nk) O(n+2^d) Yes Counting Sort O(n+k) O(n+k) O(n+k) O(n+r) Yes","title":"Complexity for Non-Comparison Sorting"},{"location":"Algo/pySort/#bubble-sort","text":"Bubble Sort is the simplest sorting algorithm that works by repeatedly swapping the adjacent elements if they are in wrong order. Two values are compared to each other incrementally, the largest value is shifted until it is at the top. Complexity: O(n^2) Worst and Average Case Time Complexity: O(n*n). Worst case occurs when array is reverse sorted. Best Case Time Complexity: O(n). Best case occurs when array is already sorted. Auxiliary Space: O(1) Boundary Cases: Bubble sort takes minimum time (Order of n) when elements are already sorted. Sorting In Place: Yes Stable: Yes # Python program for implementation of Bubble Sort def bubbleSort ( arr ): n = len ( arr ) # Traverse through all array elements for i in range ( n ): # Last i elements are already in place for j in range ( 0 , n - i - 1 ): # traverse the array from 0 to n-i-1 # Swap if the element found is greater # than the next element if arr [ j ] > arr [ j + 1 ] : arr [ j ], arr [ j + 1 ] = arr [ j + 1 ], arr [ j ] # Driver code to test above arr = [ 64 , 34 , 25 , 12 , 22 , 11 , 90 ] bubbleSort ( arr ) print ( \"Sorted array is:\" ) for i in range ( len ( arr )): print ( \" %d \" % arr [ i ]), Sorted array is: 11 12 22 25 34 64 90","title":"Bubble Sort"},{"location":"Algo/pySort/#selection-sort","text":"When you want to store multiple elements, use an array or a list. \u2022 With an array, all your elements are stored right next to each other. \u2022 With a list, elements are strewn all over, and one element stores the address of the next one. \u2022 Arrays allow fast reads. \u2022 Linked lists allow fast inserts and deletes. \u2022 All elements in the array should be the same type (all ints, all doubles, and so on). The selection sort algorithm sorts an array by repeatedly finding the minimum element (considering ascending order) from unsorted part and putting it at the beginning. The algorithm maintains two subarrays in a given array. 1) The subarray which is already sorted. 2) Remaining subarray which is unsorted. In every iteration of selection sort, the minimum element (considering ascending order) from the unsorted subarray is picked and moved to the sorted subarray. Time Complexity: O(n2) as there are two nested loops. Auxiliary Space: O(1) The good thing about selection sort is it never makes more than O(n) swaps and can be useful when memory write is a costly operation. Stability : The default implementation is not stable. However it can be made stable. Please see stable selection sort for details. In Place : Yes, it does not require extra space. A sorting algorithm is said to be stable if two objects with equal or same keys appear in the same order in sorted output as they appear in the input array to be sorted. def findSmallest ( arr ): smallest = arr [ 0 ] #Stores the smallest value smallest_index = 0 #Stores the index of the smallest value for i in range ( 1 , len ( arr )): if arr [ i ] < smallest : smallest = arr [ i ] smallest_index = i return smallest_index def selectionSort ( arr ): #Sorts an array newArr = [] for i in range ( len ( arr )): smallest = findSmallest ( arr ) newArr . append ( arr . pop ( smallest )) return newArr print ( selectionSort ([ 5 , 3 , 6 , 2 , 10 ])) [2, 3, 5, 6, 10] # Python program for implementation of Selection # Sort import sys A = [ 64 , 25 , 12 , 22 , 11 ] # Traverse through all array elements for i in range ( len ( A )): # Find the minimum element in remaining # unsorted array min_idx = i for j in range ( i + 1 , len ( A )): if A [ min_idx ] > A [ j ]: min_idx = j # Swap the found minimum element with # the first element A [ i ], A [ min_idx ] = A [ min_idx ], A [ i ] # Driver code to test above print ( \"Sorted array\" ) for i in range ( len ( A )): print ( \" %d \" % A [ i ]), Sorted array 11 12 22 25 64","title":"Selection Sort"},{"location":"Algo/pySort/#quicksort-divide-and-conquer","text":"Quicksort is a sorting algorithm. It\u2019s much faster than selection sort and is frequently used in real life. For example, the C standard library has a function called qsort, which is its implementation of quicksort. A quick sort sets a pit point to partition a data set. High and low index values are then used to rearrange data values that are on the \"wrong\" side of the pivot point. Like Merge Sort, QuickSort is a Divide and Conquer algorithm. It picks an element as pivot and partitions the given array around the picked pivot. There are many different versions of quickSort that pick pivot in different ways. Always pick first element as pivot. Always pick last element as pivot (implemented below) Pick a random element as pivot. Pick median as pivot. The key process in quickSort is partition(). Target of partitions is, given an array and an element x of array as pivot, put x at its correct position in sorted array and put all smaller elements (smaller than x) before x, and put all greater elements (greater than x) after x. All this should be done in linear time. # Python3 implementation of QuickSort # This Function handles sorting part of quick sort # start and end points to first and last element of # an array respectively def partition ( start , end , array ): # Initializing pivot's index to start pivot_index = start pivot = array [ pivot_index ] # This loop runs till start pointer crosses # end pointer, and when it does we swap the # pivot with element on end pointer while start < end : # Increment the start pointer till it finds an # element greater than pivot while start < len ( array ) and array [ start ] <= pivot : start += 1 # Decrement the end pointer till it finds an # element less than pivot while array [ end ] > pivot : end -= 1 # If start and end have not crossed each other, # swap the numbers on start and end if ( start < end ): array [ start ], array [ end ] = array [ end ], array [ start ] # Swap pivot element with element on end pointer. # This puts pivot on its correct sorted place. array [ end ], array [ pivot_index ] = array [ pivot_index ], array [ end ] # Returning end pointer to divide the array into 2 return end # The main function that implements QuickSort def quick_sort ( start , end , array ): if ( start < end ): # p is partitioning index, array[p] # is at right place p = partition ( start , end , array ) # Sort elements before partition # and after partition quick_sort ( start , p - 1 , array ) quick_sort ( p + 1 , end , array ) # Driver code array = [ 10 , 7 , 8 , 9 , 1 , 5 ] quick_sort ( 0 , len ( array ) - 1 , array ) print ( f 'Sorted array: { array } ' ) Sorted array: [1, 5, 7, 8, 9, 10] def quicksort ( array ): if len ( array ) < 2 : return array #Base case: arrays with 0 or 1 element are already \u201csorted.\u201d else : pivot = array [ 0 ] #Recursive case less = [ i for i in array [ 1 :] if i <= pivot ] #Sub-array of all the elements less than the pivot greater = [ i for i in array [ 1 :] if i > pivot ] #Sub-array of all the elements greater than the pivot return quicksort ( less ) + [ pivot ] + quicksort ( greater ) print ( quicksort ([ 10 , 5 , 2 , 3 ])) [2, 3, 5, 10] def quicksort ( arr ): if len ( arr ) <= 1 : return arr pivot = arr [ len ( arr ) // 2 ] left = [ x for x in arr if x < pivot ] middle = [ x for x in arr if x == pivot ] right = [ x for x in arr if x > pivot ] return quicksort ( left ) + middle + quicksort ( right ) def quicksort_verbose ( arr ): print ( f \"Calling quicksort on { arr } \" ) if len ( arr ) <= 1 : print ( f \"returning { arr } \" ) return arr pivot = arr [ len ( arr ) // 2 ] left = [ x for x in arr if x < pivot ] print ( f \"left: { left } ; \" , end = \"\" ) middle = [ x for x in arr if x == pivot ] print ( f \"middle: { middle } ; \" , end = \"\" ) right = [ x for x in arr if x > pivot ] print ( f \"right: { right } \" ) to_return = quicksort_verbose ( left ) + middle + quicksort_verbose ( right ) print ( f \"returning: { to_return } \" ) return to_return data = [ 3 , 1 , 4 , 2 ] # print(quicksort(data)) # print(quicksort_verbose(data)) # What about data with duplicates? data = [ 1 , 6 , 5 , 5 , 2 , 6 , 1 ] print ( quicksort ( data ))","title":"QuickSort (Divide and Conquer)"},{"location":"Algo/pySort/#merge-sort","text":"A merge sort uses recursion. It breaks down the data into smaller manageable sets. As it sorts the smaller sets, it gradually rebuilds and works its way up to the original full data set. Like QuickSort, Merge Sort is a Divide and Conquer algorithm. It divides the input array into two halves, calls itself for the two halves, and then merges the two sorted halves. The merge() function is used for merging two halves. The merge(arr, l, m, r) is a key process that assumes that arr[l..m] and arr[m+1..r] are sorted and merges the two sorted sub-arrays into one. Time Complexity: Sorting arrays on different machines. Merge Sort is a recursive algorithm and time complexity can be expressed as following recurrence relation. T(n) = 2T(n/2) + \u03b8(n) The above recurrence can be solved either using the Recurrence Tree method or the Master method. It falls in case II of Master Method and the solution of the recurrence is \u03b8(nLogn). Time complexity of Merge Sort is \u03b8(nLogn) in all 3 cases (worst, average and best) as merge sort always divides the array into two halves and takes linear time to merge two halves. Auxiliary Space: O(n) Algorithmic Paradigm: Divide and Conquer Sorting In Place: No in a typical implementation Stable: Yes","title":"Merge Sort"},{"location":"Algo/pySort/#applications-of-merge-sort","text":"Merge Sort is useful for sorting linked lists in O(nLogn) time. Inversion Count Problem Used in External Sorting","title":"Applications of Merge Sort"},{"location":"Algo/pySort/#drawbacks-of-merge-sort","text":"Slower comparative to the other sort algorithms for smaller tasks. Merge sort algorithm requires an additional memory space of 0(n) for the temporary array. It goes through the whole process even if the array is sorted. # Python program for implementation of MergeSort def mergeSort ( arr ): if len ( arr ) > 1 : # Finding the mid of the array mid = len ( arr ) // 2 # Dividing the array elements L = arr [: mid ] # into 2 halves R = arr [ mid :] # Sorting the first half mergeSort ( L ) # Sorting the second half mergeSort ( R ) i = j = k = 0 # Copy data to temp arrays L[] and R[] while i < len ( L ) and j < len ( R ): if L [ i ] < R [ j ]: arr [ k ] = L [ i ] i += 1 else : arr [ k ] = R [ j ] j += 1 k += 1 # Checking if any element was left while i < len ( L ): arr [ k ] = L [ i ] i += 1 k += 1 while j < len ( R ): arr [ k ] = R [ j ] j += 1 k += 1 # Code to print the list def printList ( arr ): for i in range ( len ( arr )): print ( arr [ i ], end = \" \" ) print () # Driver Code if __name__ == '__main__' : arr = [ 12 , 11 , 13 , 5 , 6 , 7 ] print ( \"Given array is\" , end = \" \\n \" ) printList ( arr ) mergeSort ( arr ) print ( \"Sorted array is: \" , end = \" \\n \" ) printList ( arr ) Given array is 12 11 13 5 6 7 Sorted array is: 5 6 7 11 12 13","title":"Drawbacks of Merge Sort"},{"location":"Algo/pySort/#counting-sort","text":"Counting sort is a sorting technique based on keys between a specific range. It works by counting the number of objects having distinct key values (kind of hashing). Then doing some arithmetic to calculate the position of each object in the output sequence. Time Complexity: O(n+k) where n is the number of elements in input array and k is the range of input. Auxiliary Space: O(n+k) For simplicity, consider the data in the range 0 to 9. Input data: 1, 4, 1, 2, 7, 5, 2 1) Take a count array to store the count of each unique object. Index: 0 1 2 3 4 5 6 7 8 9 Count: 0 2 2 0 1 1 0 1 0 0 2) Modify the count array such that each element at each index stores the sum of previous counts. Index: 0 1 2 3 4 5 6 7 8 9 Count: 0 2 4 4 5 6 6 7 7 7 The modified count array indicates the position of each object in the output sequence. 3) Rotate the array clockwise for one time. Index: 0 1 2 3 4 5 6 7 8 9 Count: 0 0 2 4 4 5 6 6 7 7 4) Output each object from the input sequence followed by increasing its count by 1. Process the input data: 1, 4, 1, 2, 7, 5, 2. Position of 1 is 0. Put data 1 at index 0 in output. Increase count by 1 to place next data 1 at an index 1 greater than this index. Points to be noted : Counting sort is efficient if the range of input data is not significantly greater than the number of objects to be sorted. Consider the situation where the input sequence is between range 1 to 10K and the data is 10, 5, 10K, 5K. It is not a comparison based sorting. It running time complexity is O(n) with space proportional to the range of data. It is often used as a sub-routine to another sorting algorithm like radix sort. Counting sort uses a partial hashing to count the occurrence of the data object in O(1). Counting sort can be extended to work for negative inputs also. # Python program for counting sort # The main function that sort the given string arr[] in # alphabetical order def countSort ( arr ): # The output character array that will have sorted arr output = [ 0 for i in range ( len ( arr ))] # Create a count array to store count of individual # characters and initialize count array as 0 count = [ 0 for i in range ( 256 )] # For storing the resulting answer since the # string is immutable ans = [ \"\" for _ in arr ] # Store count of each character for i in arr : count [ ord ( i )] += 1 # Change count[i] so that count[i] now contains actual # position of this character in output array for i in range ( 256 ): count [ i ] += count [ i - 1 ] # Build the output character array for i in range ( len ( arr )): output [ count [ ord ( arr [ i ])] - 1 ] = arr [ i ] count [ ord ( arr [ i ])] -= 1 # Copy the output array to arr, so that arr now # contains sorted characters for i in range ( len ( arr )): ans [ i ] = output [ i ] return ans # Driver program to test above function arr = \"meatforall\" ans = countSort ( arr ) print ( \"Sorted character array is % s \" % ( \"\" . join ( ans ))) Sorted character array is aaefllmort","title":"Counting Sort"},{"location":"Algo/pySort/#radix-sort","text":"The lower bound for Comparison based sorting algorithm (Merge Sort, Heap Sort, Quick-Sort .. etc) is \u03a9(nLogn), i.e., they cannot do better than nLogn. Counting sort is a linear time sorting algorithm that sort in O(n+k) time when elements are in the range from 1 to k. What if the elements are in the range from 1 to n2? We can\u2019t use counting sort because counting sort will take O(n2) which is worse than comparison-based sorting algorithms. Can we sort such an array in linear time? Radix Sort is the answer. The idea of Radix Sort is to do digit by digit sort starting from least significant digit to most significant digit. Radix sort uses counting sort as a subroutine to sort. Original, unsorted list: 170, 45, 75, 90, 802, 24, 2, 66 Sorting by least significant digit (1s place) gives: [*Notice that we keep 802 before 2, because 802 occurred before 2 in the original list, and similarly for pairs 170 & 90 and 45 & 75.] 170, 90, 802, 2, 24, 45, 75, 66 Sorting by next digit (10s place) gives: [*Notice that 802 again comes before 2 as 802 comes before 2 in the previous list.] 802, 2, 24, 45, 66, 170, 75, 90 Sorting by the most significant digit (100s place) gives: 2, 24, 45, 66, 75, 90, 170, 802 # Python program for implementation of Radix Sort # A function to do counting sort of arr[] according to # the digit represented by exp. def countingSort ( arr , exp1 ): n = len ( arr ) # The output array elements that will have sorted arr output = [ 0 ] * ( n ) # initialize count array as 0 count = [ 0 ] * ( 10 ) # Store count of occurrences in count[] for i in range ( 0 , n ): index = arr [ i ] // exp1 count [ index % 10 ] += 1 # Change count[i] so that count[i] now contains actual # position of this digit in output array for i in range ( 1 , 10 ): count [ i ] += count [ i - 1 ] # Build the output array i = n - 1 while i >= 0 : index = arr [ i ] // exp1 output [ count [ index % 10 ] - 1 ] = arr [ i ] count [ index % 10 ] -= 1 i -= 1 # Copying the output array to arr[], # so that arr now contains sorted numbers i = 0 for i in range ( 0 , len ( arr )): arr [ i ] = output [ i ] # Method to do Radix Sort def radixSort ( arr ): # Find the maximum number to know number of digits max1 = max ( arr ) # Do counting sort for every digit. Note that instead # of passing digit number, exp is passed. exp is 10^i # where i is current digit number exp = 1 while max1 / exp > 1 : countingSort ( arr , exp ) exp *= 10 # Driver code arr = [ 170 , 45 , 75 , 90 , 802 , 24 , 2 , 66 ] # Function Call radixSort ( arr ) for i in range ( len ( arr )): print ( arr [ i ], end = \" \" ) 2 24 45 66 75 90 170 802","title":"Radix Sort"},{"location":"Cls/pyAbsCls/","text":"An abstract class can be considered as a blueprint for other classes. It allows you to create a set of methods that must be created within any child classes built from the abstract class. A class which contains one or more abstract methods is called an abstract class. An abstract method is a method that has a declaration but does not have an implementation. While we are designing large functional units we use an abstract class. When we want to provide a common interface for different implementations of a component, we use an abstract class. Why use Abstract Base Classes : By defining an abstract base class, you can define a common Application Program Interface(API) for a set of subclasses. This capability is especially useful in situations where a third-party is going to provide implementations, such as with plugins, but can also help you when working in a large team or with a large code-base where keeping all classes in your mind is difficult or not possible. How Abstract Base classes work: By default, Python does not provide abstract classes. Python comes with a module that provides the base for defining Abstract Base classes(ABC) and that module name is ABC. ABC works by decorating methods of the base class as abstract and then registering concrete classes as implementations of the abstract base. A method becomes abstract when decorated with the keyword @abstractmethod. # Using Abstract Base Classes to enforce class constraints from abc import ABC , abstractmethod class GraphicShape ( ABC ): # Inheriting from ABC indicates that this is an abstract base class def __init__ ( self ): super () . __init__ () # declaring a method as abstract requires a subclass to implement it @abstractmethod def calcArea ( self ): pass class Circle ( GraphicShape ): def __init__ ( self , radius ): self . radius = radius def calcArea ( self ): return 3.14 * ( self . radius ** 2 ) class Square ( GraphicShape ): def __init__ ( self , side ): self . side = side def calcArea ( self ): return self . side * self . side # Abstract classes can't be instantiated themselves # g = GraphicShape() # this will error c = Circle ( 10 ) print ( c . calcArea ()) s = Square ( 12 ) print ( s . calcArea ()) 314.0 144","title":"Abstract Classes"},{"location":"Cls/pyClsStaMet/","text":"Methods of objects we've looked at so far are called by an instance of a class, which is then passed to the self parameter of the method. Class methods are different - they are called by a class, which is passed to the cls parameter of the method. A common use of these are factory methods, which instantiate an instance of a class, using different parameters than those usually passed to the class constructor. Class methods are marked with a classmethod decorator. Technically, the parameters self and cls are just conventions; they could be changed to anything else. However, they are universally followed, so it is wise to stick to using them. Static methods behave like plain functions, except for the fact that you can call them from an instance of the class. class Rectangle : def __init__ ( self , w , h ): self . w = w self . h = h def area ( self ): return self . w * self . h @classmethod def new_square ( cls , side_length ): return cls ( side_length , side_length ) @staticmethod def spam ( x ): print ( \"Spam and eggs\" + x ) square = Rectangle . new_square ( 5 ) print ( square . area ()) Rectangle . spam ( \" and Ham\" ) 25 Spam and eggs and Ham Class method vs Static Method A class method takes cls as the first parameter while a static method needs no specific parameters. A class method can access or modify the class state while a static method can\u2019t access or modify it. In general, static methods know nothing about the class state. They are utility-type methods that take some parameters and work upon those parameters. On the other hand class methods must have class as a parameter. We use @classmethod decorator in python to create a class method and we use @staticmethod decorator to create a static method in python. When to use what? We generally use class method to create factory methods. Factory methods return class objects ( similar to a constructor ) for different use cases. We generally use static methods to create utility functions. # Python program to demonstrate # use of class method and static method. from datetime import date class Person : def __init__ ( self , name , age ): self . name = name self . age = age # a class method to create a Person object by birth year. @classmethod def fromBirthYear ( cls , name , year ): return cls ( name , date . today () . year - year ) # a static method to check if a Person is adult or not. @staticmethod def isAdult ( age ): return age > 18 person1 = Person ( 'mayank' , 21 ) person2 = Person . fromBirthYear ( 'mayank' , 1996 ) print ( person1 . age ) print ( person2 . age ) # print the result print ( Person . isAdult ( 22 )) 21 25 True","title":"Class & Static Methods"},{"location":"Cls/pyConstructor/","text":"The init method is the constructor. All methods must have self as their first parameter, although it isn't explicitly passed, Python adds the self argument to the list for you; you do not need to include it when you call the methods. Within a method definition, self refers to the instance calling the method. Classes can have other methods defined to add functionality to them. Remember, that all methods must have self as their first parameter. Classes can also have class attributes, created by assigning variables within the body of the class. These can be accessed either from instances of the class, or the class itself. Class attributes are shared by all instances of the class. Trying to access an attribute of an instance that isn't defined causes an AttributeError. This also applies when you call an undefined method. class Dog : legs = 4 def __init__ ( self , name , color ): self . name = name self . color = color def bark ( self ): print ( \"WooF!\" ) fido = Dog ( \"Fido\" , \"Brown\" ) print ( fido . legs ) fido . bark () print ( Dog . legs ) 4 WooF! 4 Polymorphism Python does not support explicit multiple constructors, yet there are some ways in which using the multiple constructors can be achieved. If multiple init methods are written for the same class, then the latest one overwrites all the previous constructors. The class constructors can be made to exhibit polymorphism in three ways which are listed below. Overloading constructors based on arguments. Calling methods from init . Using @classmethod decorator. With python3, you can use Implementing Multiple Dispatch with Function Annotations as Python Cookbook Overloading constructors based on arguments class sample : # constructor overloading based on args def __init__ ( self , * args ): # if args are more than 1 # sum of args if len ( args ) > 1 : self . ans = 0 for i in args : self . ans += i # if arg is an integer # square the arg elif isinstance ( args [ 0 ], int ): self . ans = args [ 0 ] * args [ 0 ] # if arg is string # Print with hello elif isinstance ( args [ 0 ], str ): self . ans = \"Hello! \" + args [ 0 ] + \".\" s1 = sample ( 1 , 2 , 3 , 4 , 5 ) print ( \"Sum of list :\" , s1 . ans ) s2 = sample ( 5 ) print ( \"Square of int :\" , s2 . ans ) s3 = sample ( \"PolloPitas\" ) print ( \"String :\" , s3 . ans ) Sum of list : 15 Square of int : 25 String : Hello! PolloPitas. Calling methods from init class eval_equations : # single constructor to call other methods def __init__ ( self , * inp ): # when 2 arguments are passed if len ( inp ) == 2 : self . ans = self . eq2 ( inp ) # when 3 arguments are passed elif len ( inp ) == 3 : self . ans = self . eq1 ( inp ) # when more than 3 arguments are passed else : self . ans = self . eq3 ( inp ) def eq1 ( self , args ): x = ( args [ 0 ] * args [ 0 ]) + ( args [ 1 ] * args [ 1 ]) - args [ 2 ] return x def eq2 ( self , args ): y = ( args [ 0 ] * args [ 0 ]) - ( args [ 1 ] * args [ 1 ]) return y def eq3 ( self , args ): temp = 0 for i in range ( 0 , len ( args )): temp += args [ i ] * args [ i ] temp = temp / max ( args ) z = temp return z inp1 = eval_equations ( 1 , 2 ) inp2 = eval_equations ( 1 , 2 , 3 ) inp3 = eval_equations ( 1 , 2 , 3 , 4 , 5 ) print ( \"equation 2 :\" , inp1 . ans ) print ( \"equation 1 :\" , inp2 . ans ) print ( \"equation 3 :\" , inp3 . ans ) equation 2 : -3 equation 1 : 2 equation 3 : 11.0 Using @classmethod decorator class eval_equations : # basic constructor def __init__ ( self , a ): self . ans = a # expression 1 @classmethod def eq1 ( cls , args ): # create an object for the class to return x = cls (( args [ 0 ] * args [ 0 ]) + ( args [ 1 ] * args [ 1 ]) - args [ 2 ]) return x # expression 2 @classmethod def eq2 ( cls , args ): y = cls (( args [ 0 ] * args [ 0 ]) - ( args [ 1 ] * args [ 1 ])) return y # expression 3 @classmethod def eq3 ( cls , args ): temp = 0 # square of each element for i in range ( 0 , len ( args )): temp += args [ i ] * args [ i ] temp = temp / max ( args ) z = cls ( temp ) return z li = [[ 1 , 2 ], [ 1 , 2 , 3 ], [ 1 , 2 , 3 , 4 , 5 ]] i = 0 # loop to get input three times while i < 3 : inp = li [ i ] # no.of.arguments = 2 if len ( inp ) == 2 : p = eval_equations . eq2 ( inp ) print ( \"equation 2 :\" , p . ans ) # no.of.arguments = 3 elif len ( inp ) == 3 : p = eval_equations . eq1 ( inp ) print ( \"equation 1 :\" , p . ans ) # More than three arguments else : p = eval_equations . eq3 ( inp ) print ( \"equation 3 :\" , p . ans ) #increment loop i += 1 equation 2 : -3 equation 1 : 2 equation 3 : 11.0","title":"Constructor"},{"location":"Cls/pyConstructor/#polymorphism","text":"Python does not support explicit multiple constructors, yet there are some ways in which using the multiple constructors can be achieved. If multiple init methods are written for the same class, then the latest one overwrites all the previous constructors. The class constructors can be made to exhibit polymorphism in three ways which are listed below. Overloading constructors based on arguments. Calling methods from init . Using @classmethod decorator. With python3, you can use Implementing Multiple Dispatch with Function Annotations as Python Cookbook","title":"Polymorphism"},{"location":"Cls/pyConstructor/#overloading-constructors-based-on-arguments","text":"class sample : # constructor overloading based on args def __init__ ( self , * args ): # if args are more than 1 # sum of args if len ( args ) > 1 : self . ans = 0 for i in args : self . ans += i # if arg is an integer # square the arg elif isinstance ( args [ 0 ], int ): self . ans = args [ 0 ] * args [ 0 ] # if arg is string # Print with hello elif isinstance ( args [ 0 ], str ): self . ans = \"Hello! \" + args [ 0 ] + \".\" s1 = sample ( 1 , 2 , 3 , 4 , 5 ) print ( \"Sum of list :\" , s1 . ans ) s2 = sample ( 5 ) print ( \"Square of int :\" , s2 . ans ) s3 = sample ( \"PolloPitas\" ) print ( \"String :\" , s3 . ans ) Sum of list : 15 Square of int : 25 String : Hello! PolloPitas.","title":"Overloading constructors based on arguments"},{"location":"Cls/pyConstructor/#calling-methods-from-init","text":"class eval_equations : # single constructor to call other methods def __init__ ( self , * inp ): # when 2 arguments are passed if len ( inp ) == 2 : self . ans = self . eq2 ( inp ) # when 3 arguments are passed elif len ( inp ) == 3 : self . ans = self . eq1 ( inp ) # when more than 3 arguments are passed else : self . ans = self . eq3 ( inp ) def eq1 ( self , args ): x = ( args [ 0 ] * args [ 0 ]) + ( args [ 1 ] * args [ 1 ]) - args [ 2 ] return x def eq2 ( self , args ): y = ( args [ 0 ] * args [ 0 ]) - ( args [ 1 ] * args [ 1 ]) return y def eq3 ( self , args ): temp = 0 for i in range ( 0 , len ( args )): temp += args [ i ] * args [ i ] temp = temp / max ( args ) z = temp return z inp1 = eval_equations ( 1 , 2 ) inp2 = eval_equations ( 1 , 2 , 3 ) inp3 = eval_equations ( 1 , 2 , 3 , 4 , 5 ) print ( \"equation 2 :\" , inp1 . ans ) print ( \"equation 1 :\" , inp2 . ans ) print ( \"equation 3 :\" , inp3 . ans ) equation 2 : -3 equation 1 : 2 equation 3 : 11.0","title":"Calling methods from init"},{"location":"Cls/pyConstructor/#using-classmethod-decorator","text":"class eval_equations : # basic constructor def __init__ ( self , a ): self . ans = a # expression 1 @classmethod def eq1 ( cls , args ): # create an object for the class to return x = cls (( args [ 0 ] * args [ 0 ]) + ( args [ 1 ] * args [ 1 ]) - args [ 2 ]) return x # expression 2 @classmethod def eq2 ( cls , args ): y = cls (( args [ 0 ] * args [ 0 ]) - ( args [ 1 ] * args [ 1 ])) return y # expression 3 @classmethod def eq3 ( cls , args ): temp = 0 # square of each element for i in range ( 0 , len ( args )): temp += args [ i ] * args [ i ] temp = temp / max ( args ) z = cls ( temp ) return z li = [[ 1 , 2 ], [ 1 , 2 , 3 ], [ 1 , 2 , 3 , 4 , 5 ]] i = 0 # loop to get input three times while i < 3 : inp = li [ i ] # no.of.arguments = 2 if len ( inp ) == 2 : p = eval_equations . eq2 ( inp ) print ( \"equation 2 :\" , p . ans ) # no.of.arguments = 3 elif len ( inp ) == 3 : p = eval_equations . eq1 ( inp ) print ( \"equation 1 :\" , p . ans ) # More than three arguments else : p = eval_equations . eq3 ( inp ) print ( \"equation 3 :\" , p . ans ) #increment loop i += 1 equation 2 : -3 equation 1 : 2 equation 3 : 11.0","title":"Using @classmethod decorator"},{"location":"Cls/pyDataCls/","text":"dataclass module is introduced in Python 3.7 as a utility tool to make structured classes specially for storing data. These classes hold certain properties and functions to deal specifically with the data and its representation. Although the module was introduced in Python3.7, one can also use it in Python3.6 by installing dataclasses library. pip install dataclasses The DataClasses are implemented by using decorators with classes. Attributes are declared using Type Hints in Python which is essentially, specifying data type for variables in python. # A basic Data Class # Importing dataclass module from dataclasses import dataclass @dataclass class Mycle (): \"\"\"A class for holding an article content\"\"\" # Attributes Declaration # using Type Hints title : str author : str language : str upvotes : int # A DataClass object article = Mycle ( \"DataClasses\" , \"vynnyl\" , \"Python\" , 0 ) print ( article ) Mycle(title='DataClasses', author='vynnyl', language='Python', upvotes=0) The two noticeable points in above code: Without a __init__() constructor, the class accepted values and assigned it to appropriate variables. The output of printing object is a neat representation of the data present in it, without any explicit function coded to do this. That means it has a modified __repr__() function. __post_init__(): This function when made, is called by in-built __init__() after initialization of all the attributes of DataClass. Basically, object creation of DataClass starts with __init__() (constructor-calling) and ends with __post__init__() (post-init processing). from dataclasses import dataclass , field name = { 'viwal' : 'Vi Awal' } @dataclass class GArticle : title : str language : str author : str author_name : str = field ( init = False ) upvotes : int = 0 #default value def __post_init__ ( self ): self . author_name = name [ self . author ] dClassObj = GArticle ( \"DataClass\" , \"Python3\" , \"viwal\" ) print ( dClassObj ) GArticle(title='DataClass', language='Python3', author='viwal', author_name='Vi Awal', upvotes=0) Immutable data classes # Creating immutable data classes from dataclasses import dataclass @dataclass ( frozen = True ) # \"The \"frozen\" parameter makes the class immutable class ImmutableClass : value1 : str = \"Value 1\" value2 : int = 0 def somefunc ( self , newval ): self . value2 = newval obj = ImmutableClass () print ( obj . value1 ) # attempting to change the value of an immutable class throws an exception #obj.value1 = \"Another value\" #print(obj.value1) # Frozen classes can't modify themselves either #obj.somefunc(20) Value 1","title":"DataClasses"},{"location":"Cls/pyDataCls/#immutable-data-classes","text":"# Creating immutable data classes from dataclasses import dataclass @dataclass ( frozen = True ) # \"The \"frozen\" parameter makes the class immutable class ImmutableClass : value1 : str = \"Value 1\" value2 : int = 0 def somefunc ( self , newval ): self . value2 = newval obj = ImmutableClass () print ( obj . value1 ) # attempting to change the value of an immutable class throws an exception #obj.value1 = \"Another value\" #print(obj.value1) # Frozen classes can't modify themselves either #obj.somefunc(20) Value 1","title":"Immutable data classes"},{"location":"Cls/pyDataHid/","text":"A key part of object-oriented programming is encapsulation , which involves packaging of related variables and functions into a single easy-to-use object - an instance of a class. A related concept is data hiding , which states that implementation details of a class should be hidden, and a clean standard interface be presented for those who want to use the class. In other programming languages, this is usually done with private methods and attributes, which block external access to certain methods and attributes in a class. The Python philosophy is slightly different. It is often stated as \"we are all consenting adults here\", meaning that you shouldn't put arbitrary restrictions on accessing parts of a class. Hence there are no ways of enforcing a method or attribute be strictly private. Weakly private methods Weakly private methods and attributes have a single underscore at the beginning. This signals that they are private, and shouldn't be used by external code. However, it is mostly only a convention, and does not stop external code from accessing them. Its only actual effect is that from module_name import * won't import variables that start with a single underscore. class Queue : def __init__ ( self , contents ): self . _hiddenlist = list ( contents ) def push ( self , value ): self . _hiddenlist . insert ( 0 , value ) def pop ( self ): return self . _hiddenlist . pop ( - 1 ) def __repr__ ( self ): return \"Queue( {} )\" . format ( self . _hiddenlist ) queue = Queue ([ 1 , 2 , 3 ]) print ( queue ) queue . push ( 0 ) print ( queue ) queue . pop () print ( queue ) print ( queue . _hiddenlist ) Queue([1, 2, 3]) Queue([0, 1, 2, 3]) Queue([0, 1, 2]) [0, 1, 2] Strongly private methods Strongly private methods and attributes have a double underscore at the beginning of their names. This causes their names to be mangled, which means that they can't be accessed from outside the class. The purpose of this isn't to ensure that they are kept private, but to avoid bugs if there are subclasses that have methods or attributes with the same names. Name mangled methods can still be accessed externally, but by a different name. The method __privatemethod of class Spam could be accessed externally with _Spam__privatemethod. class Spam : __egg = 7 def print_egg ( self ): print ( self . __egg ) s = Spam () s . print_egg () print ( s . _Spam__egg ) #print(s.__egg) #AttributeError 7 7","title":"Data Hiding"},{"location":"Cls/pyDataHid/#weakly-private-methods","text":"Weakly private methods and attributes have a single underscore at the beginning. This signals that they are private, and shouldn't be used by external code. However, it is mostly only a convention, and does not stop external code from accessing them. Its only actual effect is that from module_name import * won't import variables that start with a single underscore. class Queue : def __init__ ( self , contents ): self . _hiddenlist = list ( contents ) def push ( self , value ): self . _hiddenlist . insert ( 0 , value ) def pop ( self ): return self . _hiddenlist . pop ( - 1 ) def __repr__ ( self ): return \"Queue( {} )\" . format ( self . _hiddenlist ) queue = Queue ([ 1 , 2 , 3 ]) print ( queue ) queue . push ( 0 ) print ( queue ) queue . pop () print ( queue ) print ( queue . _hiddenlist ) Queue([1, 2, 3]) Queue([0, 1, 2, 3]) Queue([0, 1, 2]) [0, 1, 2]","title":"Weakly private methods"},{"location":"Cls/pyDataHid/#strongly-private-methods","text":"Strongly private methods and attributes have a double underscore at the beginning of their names. This causes their names to be mangled, which means that they can't be accessed from outside the class. The purpose of this isn't to ensure that they are kept private, but to avoid bugs if there are subclasses that have methods or attributes with the same names. Name mangled methods can still be accessed externally, but by a different name. The method __privatemethod of class Spam could be accessed externally with _Spam__privatemethod. class Spam : __egg = 7 def print_egg ( self ): print ( self . __egg ) s = Spam () s . print_egg () print ( s . _Spam__egg ) #print(s.__egg) #AttributeError 7 7","title":"Strongly private methods"},{"location":"Cls/pyEnum/","text":"Enumerations in Python are implemented by using the module named \u201cenum\u201c.Enumerations are created using classes. Enums have names and values associated with them. Properties of enum: Enums can be displayed as string or repr. Enums can be checked for their types using type(). \u201cname\u201d keyword is used to display the name of the enum member. Enumerations are iterable. They can be iterated using loops Enumerations support hashing. Enums can be used in dictionaries or sets. # Python code to demonstrate enumerations # importing enum for enumerations import enum # creating enumerations using class class Animal ( enum . Enum ): dog = 1 cat = 2 lion = 3 # printing enum member as string print ( \"The string representation of enum member is : \" , end = \"\" ) print ( Animal . dog ) # printing enum member as repr print ( \"The repr representation of enum member is : \" , end = \"\" ) print ( repr ( Animal . dog )) # printing the type of enum member using type() print ( \"The type of enum member is : \" , end = \"\" ) print ( type ( Animal . dog )) # printing name of enum member using \"name\" keyword print ( \"The name of enum member is : \" , end = \"\" ) print ( Animal . dog . name ) The string representation of enum member is : Animal.dog The repr representation of enum member is : <Animal.dog: 1> The type of enum member is : <enum 'Animal'> The name of enum member is : dog # Python code to demonstrate enumerations # iterations and hashing # importing enum for enumerations import enum # creating enumerations using class class Animal ( enum . Enum ): dog = 1 cat = 2 lion = 3 # printing all enum members using loop print ( \"All the enum values are : \" ) for Anim in ( Animal ): print ( Anim ) # Hashing enum member as dictionary di = {} di [ Animal . dog ] = 'bark' di [ Animal . lion ] = 'roar' # checking if enum values are hashed successfully if di == { Animal . dog : 'bark' , Animal . lion : 'roar' }: print ( \"Enum is hashed\" ) else : print ( \"Enum is not hashed\" ) All the enum values are : Animal.dog Animal.cat Animal.lion Enum is hashed Accessing Modes : Enum members can be accessed by two ways By value :- In this method, the value of enum member is passed. By name :- In this method, the name of enum member is passed. Separate value or name can also be accessed using \u201cname\u201d or \u201cvalue\u201d keyword. Comparison : Enumerations supports two types of comparisons Identity :- These are checked using keywords \u201cis\u201d and \u201cis not\u201c. Equality :- Equality comparisons of \u201c==\u201d and \u201c!=\u201d types are also supported. # Python code to demonstrate enumerations # Access and comparison # importing enum for enumerations import enum # creating enumerations using class class Animal ( enum . Enum ): dog = 1 cat = 2 lion = 3 # Accessing enum member using value print ( \"The enum member associated with value 2 is : \" , end = \"\" ) print ( Animal ( 2 )) # Accessing enum member using name print ( \"The enum member associated with name lion is : \" , end = \"\" ) print ( Animal [ 'lion' ]) # Assigning enum member mem = Animal . dog # Displaying value print ( \"The value associated with dog is : \" , end = \"\" ) print ( mem . value ) # Displaying name print ( \"The name associated with dog is : \" , end = \"\" ) print ( mem . name ) # Comparison using \"is\" if Animal . dog is Animal . cat : print ( \"Dog and cat are same animals\" ) else : print ( \"Dog and cat are different animals\" ) # Comparison using \"!=\" if Animal . lion != Animal . cat : print ( \"Lions and cat are different\" ) else : print ( \"Lions and cat are same\" ) The enum member associated with value 2 is : Animal.cat The enum member associated with name lion is : Animal.lion The value associated with dog is : 1 The name associated with dog is : dog Dog and cat are different animals Lions and cat are different # define enumerations using the Enum base class from enum import Enum , unique , auto #Enum do not permit repeated names howewver #To avoid repeated values the unique decorator is used @unique class Fruit ( Enum ): APPLE = 1 BANANA = 2 ORANGE = 3 TOMATO = 4 PEAR = auto () def main (): # enums have human-readable values and types print ( Fruit . APPLE ) print ( type ( Fruit . APPLE )) print ( repr ( Fruit . APPLE )) # enums have name and value properties print ( Fruit . APPLE . name , Fruit . APPLE . value ) # print the auto-generated value print ( Fruit . PEAR . value ) # enums are hashable - can be used as keys myFruits = {} myFruits [ Fruit . BANANA ] = \"Come Mr. Tally-man\" print ( myFruits [ Fruit . BANANA ]) if __name__ == \"__main__\" : main () Fruit.APPLE <enum 'Fruit'> <Fruit.APPLE: 1> APPLE 1 5 Come Mr. Tally-man","title":"Enumerations"},{"location":"Cls/pyGen/","text":"Besides the first two paradigms of programming - imperative (using statements, loops, and functions as subroutines), and functional (using pure functions, higher-order functions, and recursion), there is the paradigm of object-oriented programming (OOP). Objects are created using classes, which are actually the focal point of OOP. The class describes what the object will be, but is separate from the object itself. In other words, a class can be described as an object's blueprint, description, or definition. You can use the same class as a blueprint for creating multiple different objects. Classes are created using the keyword class and an indented block, which contains class methods (which are functions). class Cat : def __init__ ( self , color , legs ): self . color = color self . legs = legs felix = Cat ( \"ginger\" , 4 ) rover = Cat ( \"dog-colored\" , 4 ) stumpy = Cat ( \"brown\" , 3 )","title":"General"},{"location":"Cls/pyInheritance/","text":"Inheritance provides a way to share functionality between classes. To inherit a class from another class, put the superclass name in parentheses after the class name. A class that inherits from another class is called a subclass. A class that is inherited from is called a superclass. If a class inherits from another with the same attributes or methods, it overrides them. One class can inherit from another, and that class can inherit from a third class. However, circular inheritance is not possible. The function super is a useful inheritance-related function that refers to the parent class. It can be used to find the method with a certain name in an object's superclass. class Animal : def __init__ ( self , name , color ): self . name = name self . color = color def walk ( self ): print ( \"walking...\" ) class Cat ( Animal ): def purr ( self ): print ( \"prrrr\" ) class Dog ( Animal ): def bark ( self ): print ( \"woof!\" ) super () . walk () fido = Dog ( \"fido\" , \"brown\" ) print ( fido . color ) fido . bark () brown woof! walking... class RevStr ( str ): def __str__ ( self ): return self [:: - 1 ] hello = RevStr ( 'Hello, World!' ) print ( hello ) !dlroW ,olleH Multiple Inheritance Method resolution order: In Python, every class whether built-in or user-defined is derived from the object class and all the objects are instances of the class object. Hence, the object class is the base class for all the other classes. In the case of multiple inheritance, a given attribute is first searched in the current class if it\u2019s not found then it\u2019s searched in the parent classes. The parent classes are searched in a depth-first, left-right fashion and each class is searched once. If we see the above example then the order of search for the attributes will be Derived, Base1, Base2, object. The order that is followed is known as a linearization of the class Derived and this order is found out using a set of rules called Method Resolution Order (MRO). To view the MRO of a class: Use the mro() method, it returns a list Eg. Class4.mro() Use the mro attribute, it returns a tuple Eg. Class4. mro # Understanding multiple inheritance class A : def __init__ ( self ): super () . __init__ () self . foo = \"foo\" self . name = \"Class A\" class B : def __init__ ( self ): super () . __init__ () self . bar = \"bar\" self . name = \"Class B\" class C ( B , A ): def __init__ ( self ): super () . __init__ () def showprops ( self ): print ( self . foo ) print ( self . bar ) print ( self . name ) # create the class and call showname() c = C () print ( C . mro ()) print ( C . __mro__ ) #Method resolution order c . showprops () [<class '__main__.C'>, <class '__main__.B'>, <class '__main__.A'>, <class 'object'>] (<class '__main__.C'>, <class '__main__.B'>, <class '__main__.A'>, <class 'object'>) foo bar Class B Interfaces Interfaces are not necessary in Python. This is because Python has proper multiple inheritance, and also ducktyping, which means that the places where you must have interfaces in Java, you don't have to have them in Python. That said, there are still several uses for interfaces. Some of them are covered by Pythons Abstract Base Classes, introduced in Python 2.6. They are useful, if you want to make base classes that cannot be instantiated, but provide a specific interface or part of an implementation. Another usage is if you somehow want to specify that an object implements a specific interface, and you can use ABC's for that too by subclassing from them. Another way is zope.interface, a module that is a part of the Zope Component Architecture, a really awesomely cool component framework. Here you don't subclass from the interfaces, but instead mark classes (or even instances) as implementing an interface. This can also be used to look up components from a component registry. Interface acts as a blueprint for designing classes, so interfaces are implemented using implementer decorator on class. If a class implements an interface, then the instances of the class provide the interface. Objects can provide interfaces directly, in addition to what their classes implement. pip install zope . interface Collecting zope.interface Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 251 kB 9.9 MB/s Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zope.interface) (57.4.0) Installing collected packages: zope.interface Successfully installed zope.interface-5.4.0 import zope.interface class MyInterface ( zope . interface . Interface ): x = zope . interface . Attribute ( 'foo' ) def method1 ( self , x , y , z ): pass def method2 ( self ): pass @zope . interface . implementer ( MyInterface ) class MyClass : def method1 ( self , x ): return x ** 2 def method2 ( self ): return \"foo\" obj = MyClass () # ask an interface whether it # is implemented by a class: print ( MyInterface . implementedBy ( MyClass )) # MyClass does not provide # MyInterface but implements it: print ( MyInterface . providedBy ( MyClass )) # ask whether an interface # is provided by an object: print ( MyInterface . providedBy ( obj )) # ask what interfaces are # implemented by a class: print ( list ( zope . interface . implementedBy ( MyClass ))) # ask what interfaces are # provided by an object: print ( list ( zope . interface . providedBy ( obj ))) # class does not provide interface print ( list ( zope . interface . providedBy ( MyClass ))) True False True [<InterfaceClass __main__.MyInterface>] [<InterfaceClass __main__.MyInterface>] []","title":"Inheritance"},{"location":"Cls/pyInheritance/#multiple-inheritance","text":"Method resolution order: In Python, every class whether built-in or user-defined is derived from the object class and all the objects are instances of the class object. Hence, the object class is the base class for all the other classes. In the case of multiple inheritance, a given attribute is first searched in the current class if it\u2019s not found then it\u2019s searched in the parent classes. The parent classes are searched in a depth-first, left-right fashion and each class is searched once. If we see the above example then the order of search for the attributes will be Derived, Base1, Base2, object. The order that is followed is known as a linearization of the class Derived and this order is found out using a set of rules called Method Resolution Order (MRO). To view the MRO of a class: Use the mro() method, it returns a list Eg. Class4.mro() Use the mro attribute, it returns a tuple Eg. Class4. mro # Understanding multiple inheritance class A : def __init__ ( self ): super () . __init__ () self . foo = \"foo\" self . name = \"Class A\" class B : def __init__ ( self ): super () . __init__ () self . bar = \"bar\" self . name = \"Class B\" class C ( B , A ): def __init__ ( self ): super () . __init__ () def showprops ( self ): print ( self . foo ) print ( self . bar ) print ( self . name ) # create the class and call showname() c = C () print ( C . mro ()) print ( C . __mro__ ) #Method resolution order c . showprops () [<class '__main__.C'>, <class '__main__.B'>, <class '__main__.A'>, <class 'object'>] (<class '__main__.C'>, <class '__main__.B'>, <class '__main__.A'>, <class 'object'>) foo bar Class B","title":"Multiple Inheritance"},{"location":"Cls/pyInheritance/#interfaces","text":"Interfaces are not necessary in Python. This is because Python has proper multiple inheritance, and also ducktyping, which means that the places where you must have interfaces in Java, you don't have to have them in Python. That said, there are still several uses for interfaces. Some of them are covered by Pythons Abstract Base Classes, introduced in Python 2.6. They are useful, if you want to make base classes that cannot be instantiated, but provide a specific interface or part of an implementation. Another usage is if you somehow want to specify that an object implements a specific interface, and you can use ABC's for that too by subclassing from them. Another way is zope.interface, a module that is a part of the Zope Component Architecture, a really awesomely cool component framework. Here you don't subclass from the interfaces, but instead mark classes (or even instances) as implementing an interface. This can also be used to look up components from a component registry. Interface acts as a blueprint for designing classes, so interfaces are implemented using implementer decorator on class. If a class implements an interface, then the instances of the class provide the interface. Objects can provide interfaces directly, in addition to what their classes implement. pip install zope . interface Collecting zope.interface Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 251 kB 9.9 MB/s Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zope.interface) (57.4.0) Installing collected packages: zope.interface Successfully installed zope.interface-5.4.0 import zope.interface class MyInterface ( zope . interface . Interface ): x = zope . interface . Attribute ( 'foo' ) def method1 ( self , x , y , z ): pass def method2 ( self ): pass @zope . interface . implementer ( MyInterface ) class MyClass : def method1 ( self , x ): return x ** 2 def method2 ( self ): return \"foo\" obj = MyClass () # ask an interface whether it # is implemented by a class: print ( MyInterface . implementedBy ( MyClass )) # MyClass does not provide # MyInterface but implements it: print ( MyInterface . providedBy ( MyClass )) # ask whether an interface # is provided by an object: print ( MyInterface . providedBy ( obj )) # ask what interfaces are # implemented by a class: print ( list ( zope . interface . implementedBy ( MyClass ))) # ask what interfaces are # provided by an object: print ( list ( zope . interface . providedBy ( obj ))) # class does not provide interface print ( list ( zope . interface . providedBy ( MyClass ))) True False True [<InterfaceClass __main__.MyInterface>] [<InterfaceClass __main__.MyInterface>] []","title":"Interfaces"},{"location":"Cls/pyMagicMet/","text":"Magic methods are special methods which have double underscores at the beginning and end of their names. They are also known as dunders . So far, the only one we have encountered is __init__ , but there are several others. They are used to create functionality that can't be represented as a normal method. One common use of them is operator overloading. This means defining operators for custom classes that allow operators such as + and * to be used on them. An example magic method is __add__ for +. The __add__ method allows for the definition of a custom behavior for the + operator in our class. class Vector2D : def __init__ ( self , x , y ): self . x = x self . y = y def __add__ ( self , other ): return Vector2D ( self . x + other . x , self . y + other . y ) first = Vector2D ( 5 , 7 ) second = Vector2D ( 3 , 9 ) result = first + second print ( result . x ) print ( result . y ) 8 16 Common OPs More magic methods for common operators: __sub__ for - __mul__ for * __floordiv__ for // __truediv__ for / __mod__ for % __pow__ for ** __and__ for & __xor__ for ^ __or__ for | The expression x + y is translated into x.__add__(y). However, if x hasn't implemented __add__, and x and y are of different types, then y.__radd__(x) is called. There are equivalent r methods for all magic methods just mentioned. class SpecialString : def __init__ ( self , cont ): self . cont = cont def __truediv__ ( self , other ): line = \"=\" * len ( other . cont ) return \" \\n \" . join ([ self . cont , line , other . cont ]) spam = SpecialString ( \"spam\" ) hello = SpecialString ( \"Hello world!\" ) print ( spam / hello ) spam ============ Hello world! Comparisons Python also provides magic methods for comparisons. __lt__ for < __le__ for <= __eq__ for == __ne__ for != __gt__ for > __ge__ for >= If __ne__ is not implemented, it returns the opposite of __eq__. There are no other relationships between the other operators. class SpecialString : def __init__ ( self , cont ): self . cont = cont def __gt__ ( self , other ): for i in range ( len ( other . cont ) + 1 ): res = other . cont [: i ] + \">\" + self . cont res += \">\" + other . cont [ i :] print ( res ) spam = SpecialString ( \"spam\" ) eggs = SpecialString ( \"eggs\" ) spam > eggs >spam>eggs e>spam>ggs eg>spam>gs egg>spam>s eggs>spam> String Magic methods for String representation: __str__ :The function is used to return a user-friendly string representation of the object __repr__ Attribute Access Magic methods for Attribute access: __getattribute__ __setattr__ __getattr__ class Book : def __init__ ( self , title , author , price ): super () . __init__ () self . title = title self . author = author self . price = price self . _discount = 0.1 # The __str__ function is used to return a user-friendly string # representation of the object def __str__ ( self ): return f \" { self . title } by { self . author } , costs { self . price } \" # Called when an attribute is retrieved. Be aware that you can't # directly access the attr name otherwise a recursive loop is created def __getattribute__ ( self , name ): if ( name == \"price\" ): p = super () . __getattribute__ ( \"price\" ) d = super () . __getattribute__ ( \"_discount\" ) return p - ( p * d ) return super () . __getattribute__ ( name ) # __setattr__ called when an attribute value is set. Don't set the attr # directly here otherwise a recursive loop causes a crash def __setattr__ ( self , name , value ): if ( name == \"price\" ): if type ( value ) is not float : raise ValueError ( \"The 'price' attribute must be a float\" ) return super () . __setattr__ ( name , value ) # __getattr__ called when __getattribute__ lookup fails - you can # pretty much generate attributes on the fly with this method def __getattr__ ( self , name ): return name + \" is not here!\" b1 = Book ( \"War and Peace\" , \"Leo Tolstoy\" , 39.95 ) b2 = Book ( \"The Catcher in the Rye\" , \"JD Salinger\" , 29.95 ) # Try setting and accessing the price b1 . price = 38.95 print ( b1 ) b2 . price = float ( 40 ) # using an int will raise an exception print ( b2 ) # If an attribute doesn't exist, __getattr__ will be called print ( b1 . randomprop ) War and Peace by Leo Tolstoy, costs 35.055 The Catcher in the Rye by JD Salinger, costs 36.0 randomprop is not here! Callable Objects class Book : def __init__ ( self , title , author , price ): super () . __init__ () self . title = title self . author = author self . price = price def __str__ ( self ): return f \" { self . title } by { self . author } , costs { self . price } \" # the __call__ method can be used to call the object like a function def __call__ ( self , title , author , price ): self . title = title self . author = author self . price = price b1 = Book ( \"War and Peace\" , \"Leo Tolstoy\" , 39.95 ) b2 = Book ( \"The Catcher in the Rye\" , \"JD Salinger\" , 29.95 ) # call the object as if it were a function print ( b1 ) b1 ( \"Anna Karenina\" , \"Leo Tolstoy\" , 49.95 ) print ( b1 ) War and Peace by Leo Tolstoy, costs 39.95 Anna Karenina by Leo Tolstoy, costs 49.95 Containers There are several magic methods for making classes act like containers. __len__ for len() __getitem__ for indexing __setitem__ for assigning to indexed values __delitem__ for deleting indexed values __iter__ for iteration over objects (e.g., in for loops) __contains__ for in import random class VagueList : def __init__ ( self , cont ): self . cont = cont def __getitem__ ( self , index ): return self . cont [ index + random . randint ( - 1 , 1 )] def __len__ ( self ): return random . randint ( 0 , len ( self . cont ) * 2 ) vague_list = VagueList ([ \"A\" , \"B\" , \"C\" , \"D\" , \"E\" ]) print ( len ( vague_list )) print ( len ( vague_list )) print ( vague_list [ 2 ]) print ( vague_list [ 2 ]) 10 0 D C Destructor Destructors are called when an object gets destroyed. In Python, destructors are not needed as much needed in C++ because Python has a garbage collector that handles memory management automatically. The __del__() method is a known as a destructor method in Python. It is called when all references to the object have been deleted i.e when an object is garbage collected. def __del__ ( self ): print ( \"Destructor called\" )","title":"Magic Methods"},{"location":"Cls/pyMagicMet/#common-ops","text":"More magic methods for common operators: __sub__ for - __mul__ for * __floordiv__ for // __truediv__ for / __mod__ for % __pow__ for ** __and__ for & __xor__ for ^ __or__ for | The expression x + y is translated into x.__add__(y). However, if x hasn't implemented __add__, and x and y are of different types, then y.__radd__(x) is called. There are equivalent r methods for all magic methods just mentioned. class SpecialString : def __init__ ( self , cont ): self . cont = cont def __truediv__ ( self , other ): line = \"=\" * len ( other . cont ) return \" \\n \" . join ([ self . cont , line , other . cont ]) spam = SpecialString ( \"spam\" ) hello = SpecialString ( \"Hello world!\" ) print ( spam / hello ) spam ============ Hello world!","title":"Common OPs"},{"location":"Cls/pyMagicMet/#comparisons","text":"Python also provides magic methods for comparisons. __lt__ for < __le__ for <= __eq__ for == __ne__ for != __gt__ for > __ge__ for >= If __ne__ is not implemented, it returns the opposite of __eq__. There are no other relationships between the other operators. class SpecialString : def __init__ ( self , cont ): self . cont = cont def __gt__ ( self , other ): for i in range ( len ( other . cont ) + 1 ): res = other . cont [: i ] + \">\" + self . cont res += \">\" + other . cont [ i :] print ( res ) spam = SpecialString ( \"spam\" ) eggs = SpecialString ( \"eggs\" ) spam > eggs >spam>eggs e>spam>ggs eg>spam>gs egg>spam>s eggs>spam>","title":"Comparisons"},{"location":"Cls/pyMagicMet/#string","text":"Magic methods for String representation: __str__ :The function is used to return a user-friendly string representation of the object __repr__","title":"String"},{"location":"Cls/pyMagicMet/#attribute-access","text":"Magic methods for Attribute access: __getattribute__ __setattr__ __getattr__ class Book : def __init__ ( self , title , author , price ): super () . __init__ () self . title = title self . author = author self . price = price self . _discount = 0.1 # The __str__ function is used to return a user-friendly string # representation of the object def __str__ ( self ): return f \" { self . title } by { self . author } , costs { self . price } \" # Called when an attribute is retrieved. Be aware that you can't # directly access the attr name otherwise a recursive loop is created def __getattribute__ ( self , name ): if ( name == \"price\" ): p = super () . __getattribute__ ( \"price\" ) d = super () . __getattribute__ ( \"_discount\" ) return p - ( p * d ) return super () . __getattribute__ ( name ) # __setattr__ called when an attribute value is set. Don't set the attr # directly here otherwise a recursive loop causes a crash def __setattr__ ( self , name , value ): if ( name == \"price\" ): if type ( value ) is not float : raise ValueError ( \"The 'price' attribute must be a float\" ) return super () . __setattr__ ( name , value ) # __getattr__ called when __getattribute__ lookup fails - you can # pretty much generate attributes on the fly with this method def __getattr__ ( self , name ): return name + \" is not here!\" b1 = Book ( \"War and Peace\" , \"Leo Tolstoy\" , 39.95 ) b2 = Book ( \"The Catcher in the Rye\" , \"JD Salinger\" , 29.95 ) # Try setting and accessing the price b1 . price = 38.95 print ( b1 ) b2 . price = float ( 40 ) # using an int will raise an exception print ( b2 ) # If an attribute doesn't exist, __getattr__ will be called print ( b1 . randomprop ) War and Peace by Leo Tolstoy, costs 35.055 The Catcher in the Rye by JD Salinger, costs 36.0 randomprop is not here!","title":"Attribute Access"},{"location":"Cls/pyMagicMet/#callable-objects","text":"class Book : def __init__ ( self , title , author , price ): super () . __init__ () self . title = title self . author = author self . price = price def __str__ ( self ): return f \" { self . title } by { self . author } , costs { self . price } \" # the __call__ method can be used to call the object like a function def __call__ ( self , title , author , price ): self . title = title self . author = author self . price = price b1 = Book ( \"War and Peace\" , \"Leo Tolstoy\" , 39.95 ) b2 = Book ( \"The Catcher in the Rye\" , \"JD Salinger\" , 29.95 ) # call the object as if it were a function print ( b1 ) b1 ( \"Anna Karenina\" , \"Leo Tolstoy\" , 49.95 ) print ( b1 ) War and Peace by Leo Tolstoy, costs 39.95 Anna Karenina by Leo Tolstoy, costs 49.95","title":"Callable Objects"},{"location":"Cls/pyMagicMet/#containers","text":"There are several magic methods for making classes act like containers. __len__ for len() __getitem__ for indexing __setitem__ for assigning to indexed values __delitem__ for deleting indexed values __iter__ for iteration over objects (e.g., in for loops) __contains__ for in import random class VagueList : def __init__ ( self , cont ): self . cont = cont def __getitem__ ( self , index ): return self . cont [ index + random . randint ( - 1 , 1 )] def __len__ ( self ): return random . randint ( 0 , len ( self . cont ) * 2 ) vague_list = VagueList ([ \"A\" , \"B\" , \"C\" , \"D\" , \"E\" ]) print ( len ( vague_list )) print ( len ( vague_list )) print ( vague_list [ 2 ]) print ( vague_list [ 2 ]) 10 0 D C","title":"Containers"},{"location":"Cls/pyMagicMet/#destructor","text":"Destructors are called when an object gets destroyed. In Python, destructors are not needed as much needed in C++ because Python has a garbage collector that handles memory management automatically. The __del__() method is a known as a destructor method in Python. It is called when all references to the object have been deleted i.e when an object is garbage collected. def __del__ ( self ): print ( \"Destructor called\" )","title":"Destructor"},{"location":"Cls/pyProp/","text":"Properties provide a way of customizing access to instance attributes. They are created by putting the property decorator above a method, which means when the instance attribute with the same name as the method is accessed, the method will be called instead. One common use of a property is to make an attribute read-only. class Pizza : def __init__ ( self , toppings ): self . toppings = toppings @property def pinapple_allowed ( self ): return False pizza = Pizza ([ \"cheese\" , \"tomato\" ]) print ( pizza . pinapple_allowed ) #pizza.pinapple_allowed=True #Read_only False Properties can also be set by defining setter/getter functions. The setter function sets the corresponding property's value. The getter gets the value. To define a setter, you need to use a decorator of the same name as the property, followed by a dot and the setter keyword. The same applies to defining getter functions. class Pizza : def __init__ ( self , toppings ): self . toppings = toppings self . _pinapple_allowed = False @property def pinapple_allowed ( self ): return self . _pinapple_allowed @pinapple_allowed . setter def pinapple_allowed ( self , value ): if value : password = input ( \"Enter password: \" ) if password == \"Sword\" : self . _pinapple_allowed = value else : raise ValueError ( \"Intruder\" ) pizza = Pizza ([ \"cheese\" , \"tomato\" ]) print ( pizza . pinapple_allowed ) pizza . pinapple_allowed = True print ( pizza . pinapple_allowed ) False Enter password: Sword True","title":"Properties"},{"location":"DS/pyArray/","text":"An array is a collection of items stored at contiguous memory locations. The idea is to store multiple items of the same type together. This makes it easier to calculate the position of each element by simply adding an offset to a base value, i.e., the memory location of the first element of the array # Python program to demonstrate # Creation of Array # importing \"array\" for array creations import array as arr # creating an array with integer type a = arr . array ( 'i' , [ 1 , 2 , 3 ]) # printing original array print ( \"The new created array is : \" , end = \" \" ) for i in range ( 0 , 3 ): print ( a [ i ], end = \" \" ) print () # creating an array with float type b = arr . array ( 'd' , [ 2.5 , 3.2 , 3.3 ]) # printing original array print ( \"The new created array is : \" , end = \" \" ) for i in range ( 0 , 3 ): print ( b [ i ], end = \" \" ) The new created array is : 1 2 3 The new created array is : 2.5 3.2 3.3 Here are the differences between List and Array in Python : List Array Can consist of elements belonging to different data types Only consists of elements belonging to the same data type No need to explicitly import a module for declaration Need to explicitly import a module for declaration Cannot directly handle arithmetic operations Can directly handle arithmetic operations Can be nested to contain different type of elements Must contain either all nested elements of same size Preferred for shorter sequence of data items Preferred for longer sequence of data items Greater flexibility allows easy modification (addition, deletion) of data Less flexibility since addition, deletion has to be done element wise The entire list can be printed without any explicit looping A loop has to be formed to print or access the components of array Consume larger memory for easy addition of elements Comparatively more compact in memory size","title":"Array"},{"location":"DS/pyChainMap/","text":"A ChainMap encapsulates many dictionaries into a single unit and returns a list of dictionaries. # Python program to demonstrate ChainMap from collections import ChainMap d1 = { 'a' : 1 , 'b' : 2 } d2 = { 'c' : 3 , 'd' : 4 } d3 = { 'e' : 5 , 'f' : 6 } # Defining the chainmap c = ChainMap ( d1 , d2 , d3 ) print ( c ) ChainMap({'a': 1, 'b': 2}, {'c': 3, 'd': 4}, {'e': 5, 'f': 6}) Accessing Keys and Values from ChainMap Values from ChainMap can be accessed using the key name. They can also be accessed by using the keys() and values() method. from collections import ChainMap d1 = { 'a' : 1 , 'b' : 2 } d2 = { 'c' : 3 , 'd' : 4 } d3 = { 'e' : 5 , 'f' : 6 } # Defining the chainmap c = ChainMap ( d1 , d2 , d3 ) # Accessing Values using key name print ( c [ 'a' ]) # Accessing values using values() # method print ( c . values ()) # Accessing keys using keys() # method print ( c . keys ()) 1 ValuesView(ChainMap({'a': 1, 'b': 2}, {'c': 3, 'd': 4}, {'e': 5, 'f': 6})) KeysView(ChainMap({'a': 1, 'b': 2}, {'c': 3, 'd': 4}, {'e': 5, 'f': 6})) Adding new dictionary A new dictionary can be added by using the new_child() method. The newly added dictionary is added at the beginning of the ChainMap. import collections # initializing dictionaries dic1 = { 'a' : 1 , 'b' : 2 } dic2 = { 'b' : 3 , 'c' : 4 } dic3 = { 'f' : 5 } # initializing ChainMap chain = collections . ChainMap ( dic1 , dic2 ) # printing chainMap print ( \"All the ChainMap contents are : \" ) print ( chain ) # using new_child() to add new dictionary chain1 = chain . new_child ( dic3 ) # printing chainMap print ( \"Displaying new ChainMap : \" ) print ( chain1 ) All the ChainMap contents are : ChainMap({'a': 1, 'b': 2}, {'b': 3, 'c': 4}) Displaying new ChainMap : ChainMap({'f': 5}, {'a': 1, 'b': 2}, {'b': 3, 'c': 4})","title":"ChainMap"},{"location":"DS/pyChainMap/#adding-new-dictionary","text":"A new dictionary can be added by using the new_child() method. The newly added dictionary is added at the beginning of the ChainMap. import collections # initializing dictionaries dic1 = { 'a' : 1 , 'b' : 2 } dic2 = { 'b' : 3 , 'c' : 4 } dic3 = { 'f' : 5 } # initializing ChainMap chain = collections . ChainMap ( dic1 , dic2 ) # printing chainMap print ( \"All the ChainMap contents are : \" ) print ( chain ) # using new_child() to add new dictionary chain1 = chain . new_child ( dic3 ) # printing chainMap print ( \"Displaying new ChainMap : \" ) print ( chain1 ) All the ChainMap contents are : ChainMap({'a': 1, 'b': 2}, {'b': 3, 'c': 4}) Displaying new ChainMap : ChainMap({'f': 5}, {'a': 1, 'b': 2}, {'b': 3, 'c': 4})","title":"Adding new dictionary"},{"location":"DS/pyCounter/","text":"A Counter is a subclass of dict. Therefore it is an unordered collection where elements and their respective count are stored as a dictionary. This is equivalent to a bag or multiset of other languages. It is used to keep the count of the elements in an iterable in the form of an unordered dictionary where the key represents the element in the iterable and value represents the count of that element in the iterable. Counter class is a special type of object data-set provided with the collections module in Python3. Collections module provides the user with specialized container datatypes, thus, providing an alternative to Python\u2019s general-purpose built-ins like dictionaries, lists and tuples. Counter is a sub-class that is used to count hashable objects. It implicitly creates a hash table of an iterable when invoked. Counter object along with its functions are used collectively for processing huge amounts of data. Initialization The constructor of counter can be called in any one of the following ways : With sequence of items With dictionary containing keys and counts With keyword arguments mapping string names to counts # A Python program to show different ways to create # Counter from collections import Counter # With sequence of items print ( Counter ([ 'B' , 'B' , 'A' , 'B' , 'C' , 'A' , 'B' , 'B' , 'A' , 'C' ])) # with dictionary print ( Counter ({ 'A' : 3 , 'B' : 5 , 'C' : 2 })) # with keyword arguments print ( Counter ( A = 3 , B = 5 , C = 2 )) Counter({'B': 5, 'A': 3, 'C': 2}) Counter({'B': 5, 'A': 3, 'C': 2}) Counter({'B': 5, 'A': 3, 'C': 2}) Updation We can also create an empty counter and can be updated via update() method # A Python program to demonstrate update() from collections import Counter coun = Counter () coun . update ([ 1 , 2 , 3 , 1 , 2 , 1 , 1 , 2 ]) print ( coun ) coun . update ([ 1 , 2 , 4 ]) print ( coun ) Counter({1: 4, 2: 3, 3: 1}) Counter({1: 5, 2: 4, 3: 1, 4: 1}) Data can be provided in any of the three ways as mentioned in initialization and the counter\u2019s data will be increased not replaced. Counts can be zero and negative also. # Python program to demonstrate that counts in # Counter can be 0 and negative from collections import Counter c1 = Counter ( A = 4 , B = 3 , C = 10 ) c2 = Counter ( A = 10 , B = 3 , C = 4 ) c1 . subtract ( c2 ) print ( c1 ) Counter({'C': 6, 'B': 0, 'A': -6}) We can use Counter to count distinct elements of a list or other collections. # An example program where different list items are # counted using counter from collections import Counter # Create a list z = [ 'blue' , 'red' , 'blue' , 'yellow' , 'blue' , 'red' ] # Count distinct elements and print Counter object print ( Counter ( z )) Counter({'blue': 3, 'red': 2, 'yellow': 1}) Once initialized, counters are accessed just like dictionaries. Also, it does not raise the KeyValue error (if key is not present) instead the value\u2019s count is shown as 0. # Python program to demonstrate accessing of # Counter elements from collections import Counter # Create a list z = [ 'blue' , 'red' , 'blue' , 'yellow' , 'blue' , 'red' ] col_count = Counter ( z ) print ( col_count ) col = [ 'blue' , 'red' , 'yellow' , 'green' ] # Here green is not in col_count # so count of green will be zero for color in col : print ( color , col_count [ color ]) Counter({'blue': 3, 'red': 2, 'yellow': 1}) blue 3 red 2 yellow 1 green 0 items() The Counter.items() method helps to see the elements of the list along with their respective frequencies in a tuple. # importing the module from collections import Counter # making a list list = [ 1 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 9 , 2 , 3 , 4 , 8 ] # instantiating a Counter object ob = Counter ( list ) # Counter.items() items = ob . items () print ( \"The datatype is \" + str ( type ( items ))) # displaying the dict_items print ( items ) # iterating over the dict_items for i in items : print ( i ) The datatype is <class 'dict_items'> dict_items([(1, 2), (2, 2), (3, 2), (4, 2), (5, 1), (6, 1), (7, 1), (9, 1), (8, 1)]) (1, 2) (2, 2) (3, 2) (4, 2) (5, 1) (6, 1) (7, 1) (9, 1) (8, 1) keys() The Counter.keys() method helps to see the unique elements in the list. # importing the module from collections import Counter # making a list list = [ 1 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 9 , 2 , 3 , 4 , 8 ] # instantiating a Counter object ob = Counter ( list ) # Counter.keys() keys = ob . keys () print ( \"The datatype is \" + str ( type ( keys ))) # displaying the dict_items print ( keys ) # iterating over the dict_items for i in keys : print ( i ) The datatype is <class 'dict_keys'> dict_keys([1, 2, 3, 4, 5, 6, 7, 9, 8]) 1 2 3 4 5 6 7 9 8 values() The Counter.values() method helps to see the frequencies of each unique element. # importing the module from collections import Counter # making a list list = [ 1 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 9 , 2 , 3 , 4 , 8 ] # instantiating a Counter object ob = Counter ( list ) # Counter.values() values = ob . values () print ( \"The datatype is \" + str ( type ( values ))) # displaying the dict_items print ( values ) # iterating over the dict_items for i in values : print ( i ) The datatype is <class 'dict_values'> dict_values([2, 2, 2, 2, 1, 1, 1, 1, 1]) 2 2 2 2 1 1 1 1 1 elements() elements() is one of the functions of Counter class, when invoked on the Counter object will return an itertool of all the known elements in the Counter object. The elements() method returns an iterator that produces all of the items known to the Counter. Note : Elements with count <= 0 are not included. # import counter class from collections module from collections import Counter # Creation of a Counter Class object using # a string as an iterable data container # Example - 1 a = Counter ( \"treatortrips\" ) # Elements of counter object for i in a . elements (): print ( i , end = \" \" ) print () # Example - 2 b = Counter ({ 'trips' : 4 , 'for' : 1 , 'gfg' : 2 , 'Trean' : 3 }) for i in b . elements (): print ( i , end = \" \" ) print () # Example - 3 c = Counter ([ 1 , 2 , 21 , 12 , 2 , 44 , 5 , 13 , 15 , 5 , 19 , 21 , 5 ]) for i in c . elements (): print ( i , end = \" \" ) print () # Example - 4 d = Counter ( a = 2 , b = 3 , c = 6 , d = 1 , e = 5 ) for i in d . elements (): print ( i , end = \" \" ) t t t r r r e a o i p s trips trips trips trips for gfg gfg Trean Trean Trean 1 2 2 21 21 12 44 5 5 5 13 15 19 a a b b b c c c c c c d e e e e e most_common() most_common() is used to produce a sequence of the n most frequently encountered input values and their respective counts. # Python example to demonstrate most_elements() on # Counter from collections import Counter coun = Counter ( a = 1 , b = 2 , c = 3 , d = 120 , e = 1 , f = 219 ) # This prints 3 most frequent characters for letter , count in coun . most_common ( 3 ): print ( ' %s : %d ' % ( letter , count )) f: 219 d: 120 c: 3","title":"Counter"},{"location":"DS/pyCounter/#initialization","text":"The constructor of counter can be called in any one of the following ways : With sequence of items With dictionary containing keys and counts With keyword arguments mapping string names to counts # A Python program to show different ways to create # Counter from collections import Counter # With sequence of items print ( Counter ([ 'B' , 'B' , 'A' , 'B' , 'C' , 'A' , 'B' , 'B' , 'A' , 'C' ])) # with dictionary print ( Counter ({ 'A' : 3 , 'B' : 5 , 'C' : 2 })) # with keyword arguments print ( Counter ( A = 3 , B = 5 , C = 2 )) Counter({'B': 5, 'A': 3, 'C': 2}) Counter({'B': 5, 'A': 3, 'C': 2}) Counter({'B': 5, 'A': 3, 'C': 2})","title":"Initialization"},{"location":"DS/pyCounter/#updation","text":"We can also create an empty counter and can be updated via update() method # A Python program to demonstrate update() from collections import Counter coun = Counter () coun . update ([ 1 , 2 , 3 , 1 , 2 , 1 , 1 , 2 ]) print ( coun ) coun . update ([ 1 , 2 , 4 ]) print ( coun ) Counter({1: 4, 2: 3, 3: 1}) Counter({1: 5, 2: 4, 3: 1, 4: 1}) Data can be provided in any of the three ways as mentioned in initialization and the counter\u2019s data will be increased not replaced. Counts can be zero and negative also. # Python program to demonstrate that counts in # Counter can be 0 and negative from collections import Counter c1 = Counter ( A = 4 , B = 3 , C = 10 ) c2 = Counter ( A = 10 , B = 3 , C = 4 ) c1 . subtract ( c2 ) print ( c1 ) Counter({'C': 6, 'B': 0, 'A': -6}) We can use Counter to count distinct elements of a list or other collections. # An example program where different list items are # counted using counter from collections import Counter # Create a list z = [ 'blue' , 'red' , 'blue' , 'yellow' , 'blue' , 'red' ] # Count distinct elements and print Counter object print ( Counter ( z )) Counter({'blue': 3, 'red': 2, 'yellow': 1}) Once initialized, counters are accessed just like dictionaries. Also, it does not raise the KeyValue error (if key is not present) instead the value\u2019s count is shown as 0. # Python program to demonstrate accessing of # Counter elements from collections import Counter # Create a list z = [ 'blue' , 'red' , 'blue' , 'yellow' , 'blue' , 'red' ] col_count = Counter ( z ) print ( col_count ) col = [ 'blue' , 'red' , 'yellow' , 'green' ] # Here green is not in col_count # so count of green will be zero for color in col : print ( color , col_count [ color ]) Counter({'blue': 3, 'red': 2, 'yellow': 1}) blue 3 red 2 yellow 1 green 0","title":"Updation"},{"location":"DS/pyCounter/#items","text":"The Counter.items() method helps to see the elements of the list along with their respective frequencies in a tuple. # importing the module from collections import Counter # making a list list = [ 1 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 9 , 2 , 3 , 4 , 8 ] # instantiating a Counter object ob = Counter ( list ) # Counter.items() items = ob . items () print ( \"The datatype is \" + str ( type ( items ))) # displaying the dict_items print ( items ) # iterating over the dict_items for i in items : print ( i ) The datatype is <class 'dict_items'> dict_items([(1, 2), (2, 2), (3, 2), (4, 2), (5, 1), (6, 1), (7, 1), (9, 1), (8, 1)]) (1, 2) (2, 2) (3, 2) (4, 2) (5, 1) (6, 1) (7, 1) (9, 1) (8, 1)","title":"items()"},{"location":"DS/pyCounter/#keys","text":"The Counter.keys() method helps to see the unique elements in the list. # importing the module from collections import Counter # making a list list = [ 1 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 9 , 2 , 3 , 4 , 8 ] # instantiating a Counter object ob = Counter ( list ) # Counter.keys() keys = ob . keys () print ( \"The datatype is \" + str ( type ( keys ))) # displaying the dict_items print ( keys ) # iterating over the dict_items for i in keys : print ( i ) The datatype is <class 'dict_keys'> dict_keys([1, 2, 3, 4, 5, 6, 7, 9, 8]) 1 2 3 4 5 6 7 9 8","title":"keys()"},{"location":"DS/pyCounter/#values","text":"The Counter.values() method helps to see the frequencies of each unique element. # importing the module from collections import Counter # making a list list = [ 1 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 9 , 2 , 3 , 4 , 8 ] # instantiating a Counter object ob = Counter ( list ) # Counter.values() values = ob . values () print ( \"The datatype is \" + str ( type ( values ))) # displaying the dict_items print ( values ) # iterating over the dict_items for i in values : print ( i ) The datatype is <class 'dict_values'> dict_values([2, 2, 2, 2, 1, 1, 1, 1, 1]) 2 2 2 2 1 1 1 1 1","title":"values()"},{"location":"DS/pyCounter/#elements","text":"elements() is one of the functions of Counter class, when invoked on the Counter object will return an itertool of all the known elements in the Counter object. The elements() method returns an iterator that produces all of the items known to the Counter. Note : Elements with count <= 0 are not included. # import counter class from collections module from collections import Counter # Creation of a Counter Class object using # a string as an iterable data container # Example - 1 a = Counter ( \"treatortrips\" ) # Elements of counter object for i in a . elements (): print ( i , end = \" \" ) print () # Example - 2 b = Counter ({ 'trips' : 4 , 'for' : 1 , 'gfg' : 2 , 'Trean' : 3 }) for i in b . elements (): print ( i , end = \" \" ) print () # Example - 3 c = Counter ([ 1 , 2 , 21 , 12 , 2 , 44 , 5 , 13 , 15 , 5 , 19 , 21 , 5 ]) for i in c . elements (): print ( i , end = \" \" ) print () # Example - 4 d = Counter ( a = 2 , b = 3 , c = 6 , d = 1 , e = 5 ) for i in d . elements (): print ( i , end = \" \" ) t t t r r r e a o i p s trips trips trips trips for gfg gfg Trean Trean Trean 1 2 2 21 21 12 44 5 5 5 13 15 19 a a b b b c c c c c c d e e e e e","title":"elements()"},{"location":"DS/pyCounter/#most_common","text":"most_common() is used to produce a sequence of the n most frequently encountered input values and their respective counts. # Python example to demonstrate most_elements() on # Counter from collections import Counter coun = Counter ( a = 1 , b = 2 , c = 3 , d = 120 , e = 1 , f = 219 ) # This prints 3 most frequent characters for letter , count in coun . most_common ( 3 ): print ( ' %s : %d ' % ( letter , count )) f: 219 d: 120 c: 3","title":"most_common()"},{"location":"DS/pyDS/","text":"Python Collections Iterable Anything that you can loop over using a for loop e.g: list, tuples, strings, set and dictionaries Sequence a subset of Iterables that have: 1. A length 2. An Index 3. Can be sliced e.g: Strings, list, tuples Not dictionaries, sets, files and generators Data Structures Python supports the following data structures: lists , dictionaries , tuples , sets . For arrays, see array or numpy array . When to use a dictionary: - When you need a logical association between a key:value pair. - When you need fast lookup for your data, based on a custom key. - When your data is being constantly modified. Remember, dictionaries are mutable. When to use the other types: - Use lists if you have a collection of data that does not need random access. Try to choose lists when you need a simple, iterable collection that is modified frequently. - Use a set if you need uniqueness for the elements. - Use tuples when your data cannot change. Many times, a tuple is used in combination with a dictionary, for example, a tuple might represent a key, because it's immutable. Tuples Lists Dict Sets ( ) [ ] {k:v} { } Immutable Mutable Mutable Mutable Ordered Ordered Ordered(>3.7) Unordered Iterable Iterable Iterable Iterable Constant time Linear time Constant time Constant time mytuple[0] mylist[0] mydict['somekey'] myset[0] Allow repetition Allow repetition Allow repetition Unique data len(mytuple) len(mylist) len(mydict) len(myset) .count() .append() and .insert() .keys(), .values() and .items() .add() and .update() .index() .pop() and .remove() .pop() or del mydict['somekey'] .remove() .reverse() and sort()","title":"General"},{"location":"DS/pyDS/#python-collections","text":"","title":"Python Collections"},{"location":"DS/pyDS/#iterable","text":"Anything that you can loop over using a for loop e.g: list, tuples, strings, set and dictionaries","title":"Iterable"},{"location":"DS/pyDS/#sequence","text":"a subset of Iterables that have: 1. A length 2. An Index 3. Can be sliced e.g: Strings, list, tuples Not dictionaries, sets, files and generators","title":"Sequence"},{"location":"DS/pyDS/#data-structures","text":"Python supports the following data structures: lists , dictionaries , tuples , sets . For arrays, see array or numpy array . When to use a dictionary: - When you need a logical association between a key:value pair. - When you need fast lookup for your data, based on a custom key. - When your data is being constantly modified. Remember, dictionaries are mutable. When to use the other types: - Use lists if you have a collection of data that does not need random access. Try to choose lists when you need a simple, iterable collection that is modified frequently. - Use a set if you need uniqueness for the elements. - Use tuples when your data cannot change. Many times, a tuple is used in combination with a dictionary, for example, a tuple might represent a key, because it's immutable. Tuples Lists Dict Sets ( ) [ ] {k:v} { } Immutable Mutable Mutable Mutable Ordered Ordered Ordered(>3.7) Unordered Iterable Iterable Iterable Iterable Constant time Linear time Constant time Constant time mytuple[0] mylist[0] mydict['somekey'] myset[0] Allow repetition Allow repetition Allow repetition Unique data len(mytuple) len(mylist) len(mydict) len(myset) .count() .append() and .insert() .keys(), .values() and .items() .add() and .update() .index() .pop() and .remove() .pop() or del mydict['somekey'] .remove() .reverse() and sort()","title":"Data Structures"},{"location":"DS/pyDefaultdict/","text":"Dictionary in Python is an unordered collection of data values that are used to store data values like a map. Unlike other Data Types that hold only single value as an element, the Dictionary holds key-value pair. In Dictionary, the key must be unique and immutable. This means that a Python Tuple can be a key whereas a Python List can not. A Dictionary can be created by placing a sequence of elements within curly {} braces, separated by \u2018comma\u2019. # Python program to demonstrate # dictionary Dict = { 1 : 'Treats' , 2 : 'For' , 3 : 'Pricks' } print ( \"Dictionary:\" ) print ( Dict ) print ( Dict [ 1 ]) # Uncommenting this print(Dict[4]) # will raise a KeyError as the # 4 is not present in the dictionary Dictionary: {1: 'Treats', 2: 'For', 3: 'Pricks'} Treats Sometimes, when the KeyError is raised, it might become a problem. To overcome this Python introduces another dictionary like container known as Defaultdict which is present inside the collections module. Defaultdict is a container like dictionaries present in the module collections. Defaultdict is a sub-class of the dictionary class that returns a dictionary-like object. The functionality of both dictionaries and defaultdict are almost same except for the fact that defaultdict never raises a KeyError. It provides a default value for the key that does not exists. # Python program to demonstrate defaultdict from collections import defaultdict # Function to return a default # values for keys that are not present def def_value (): return \"Not Present\" # Defining the dict d = defaultdict ( def_value ) d [ \"a\" ] = 1 d [ \"b\" ] = 2 print ( d [ \"a\" ]) print ( d [ \"b\" ]) print ( d [ \"c\" ]) 1 2 Not Present Inner Working of defaultdict Defaultdict adds one writable instance variable and one method in addition to the standard dictionary operations. The instance variable is the default_factory parameter and the method provided is missing. Default_factory: It is a function returning the default value for the dictionary defined. If this argument is absent then the dictionary raises a KeyError. # Python program to demonstrate # default_factory argument of # defaultdict from collections import defaultdict # Defining the dict and passing # lambda as default_factory argument d = defaultdict ( lambda : \"Not Present\" ) d [ \"a\" ] = 1 d [ \"b\" ] = 2 print ( d [ \"a\" ]) print ( d [ \"b\" ]) print ( d [ \"c\" ]) 1 2 Not Present from collections import defaultdict # Correct instantiation def_dict = defaultdict ( list ) # Pass list to .default_factory def_dict [ 'one' ] = 1 # Add a key-value pair def_dict [ 'missing' ] # Access a missing key returns an empty list [] def_dict [ 'another_missing' ] . append ( 4 ) # Modify a missing key def_dict defaultdict(list, {'another_missing': [4], 'missing': [], 'one': 1}) dep = [( 'Sales' , 'John Doe' ), ( 'Sales' , 'Martin Smith' ), ( 'Accounting' , 'Jane Doe' ), ( 'Marketing' , 'Elizabeth Smith' ), ( 'Marketing' , 'Adam Doe' )] from collections import defaultdict dep_dd = defaultdict ( list ) #For uniqueness instead of list use set for department , employee in dep : dep_dd [ department ] . append ( employee ) print ( dep_dd ) defaultdict(<class 'list'>, {'Sales': ['John Doe', 'Martin Smith'], 'Accounting': ['Jane Doe'], 'Marketing': ['Elizabeth Smith', 'Adam Doe']}) In this example, you group the employees by their department using a defaultdict with .default_factory set to list. To do this with a regular dictionary, you can use dict.setdefault() as follows: dep_d = dict () for department , employee in dep : dep_d . setdefault ( department , []) . append ( employee ) print ( dep_d ) {'Sales': ['John Doe', 'Martin Smith'], 'Accounting': ['Jane Doe'], 'Marketing': ['Elizabeth Smith', 'Adam Doe']} This code is straightforward, and you\u2019ll find similar code quite often in your work as a Python coder. However, the defaultdict version is arguably more readable, and for large datasets, it can also be a lot faster and more efficient. So, if speed is a concern for you, then you should consider using a defaultdict instead of a standard dict. Auto-vivification Defauldict are used to easily make nested dicts. It is called Auto-vivification. from collections import defaultdict import json def turtles (): return defaultdict ( turtles ) data = turtles () data [ \"Hello\" ] = \"Hey!\" data [ \"Foo\" ][ \"bar\" ][ \"baz\" ] = \"hmmm\" print ( json . dumps ( data , indent = 1 )) { \"Hello\": \"Hey!\", \"Foo\": { \"bar\": { \"baz\": \"hmmm\" } } }","title":"Defaultdict"},{"location":"DS/pyDefaultdict/#inner-working-of-defaultdict","text":"Defaultdict adds one writable instance variable and one method in addition to the standard dictionary operations. The instance variable is the default_factory parameter and the method provided is missing. Default_factory: It is a function returning the default value for the dictionary defined. If this argument is absent then the dictionary raises a KeyError. # Python program to demonstrate # default_factory argument of # defaultdict from collections import defaultdict # Defining the dict and passing # lambda as default_factory argument d = defaultdict ( lambda : \"Not Present\" ) d [ \"a\" ] = 1 d [ \"b\" ] = 2 print ( d [ \"a\" ]) print ( d [ \"b\" ]) print ( d [ \"c\" ]) 1 2 Not Present from collections import defaultdict # Correct instantiation def_dict = defaultdict ( list ) # Pass list to .default_factory def_dict [ 'one' ] = 1 # Add a key-value pair def_dict [ 'missing' ] # Access a missing key returns an empty list [] def_dict [ 'another_missing' ] . append ( 4 ) # Modify a missing key def_dict defaultdict(list, {'another_missing': [4], 'missing': [], 'one': 1}) dep = [( 'Sales' , 'John Doe' ), ( 'Sales' , 'Martin Smith' ), ( 'Accounting' , 'Jane Doe' ), ( 'Marketing' , 'Elizabeth Smith' ), ( 'Marketing' , 'Adam Doe' )] from collections import defaultdict dep_dd = defaultdict ( list ) #For uniqueness instead of list use set for department , employee in dep : dep_dd [ department ] . append ( employee ) print ( dep_dd ) defaultdict(<class 'list'>, {'Sales': ['John Doe', 'Martin Smith'], 'Accounting': ['Jane Doe'], 'Marketing': ['Elizabeth Smith', 'Adam Doe']}) In this example, you group the employees by their department using a defaultdict with .default_factory set to list. To do this with a regular dictionary, you can use dict.setdefault() as follows: dep_d = dict () for department , employee in dep : dep_d . setdefault ( department , []) . append ( employee ) print ( dep_d ) {'Sales': ['John Doe', 'Martin Smith'], 'Accounting': ['Jane Doe'], 'Marketing': ['Elizabeth Smith', 'Adam Doe']} This code is straightforward, and you\u2019ll find similar code quite often in your work as a Python coder. However, the defaultdict version is arguably more readable, and for large datasets, it can also be a lot faster and more efficient. So, if speed is a concern for you, then you should consider using a defaultdict instead of a standard dict.","title":"Inner Working of defaultdict"},{"location":"DS/pyDefaultdict/#auto-vivification","text":"Defauldict are used to easily make nested dicts. It is called Auto-vivification. from collections import defaultdict import json def turtles (): return defaultdict ( turtles ) data = turtles () data [ \"Hello\" ] = \"Hey!\" data [ \"Foo\" ][ \"bar\" ][ \"baz\" ] = \"hmmm\" print ( json . dumps ( data , indent = 1 )) { \"Hello\": \"Hey!\", \"Foo\": { \"bar\": { \"baz\": \"hmmm\" } } }","title":"Auto-vivification"},{"location":"DS/pyDict/","text":"Since you've seen parenthesis (for tuples) and square brackets (for lists), you may be wondering what curly braces are used for in Python. The answer: Python dictionaries. The defining feature of a Python dictionary is that it has keys and values that are associated with each other. When defining a dictionary, this association may be accomplished using the colon (:) as is done below. Dictionaries are data structures used to map arbitrary keys to values. Lists can be thought of as dictionaries with integer keys within a certain range. Dictionaries can be indexed in the same way as lists, using square brackets containing keys. Dict Creation An empty dictionary is defined as {}. Dictionary can also be created by the built-in function dict(). # Creating an empty Dictionary Dict = {} print ( \"Empty Dictionary: \" ) print ( Dict ) # Creating a Dictionary # with dict() method Dict = dict ({ 1 : 'Trips' , 2 : 'For' , 3 : 'chips' }) print ( \" \\n Dictionary with the use of dict(): \" ) print ( Dict ) # Creating a Dictionary # with each item as a Pair Dict = dict ([( 1 , 'Trips' ), ( 2 , 'For' )]) print ( \" \\n Dictionary with each item as a pair: \" ) print ( Dict ) Empty Dictionary: {} Dictionary with the use of dict(): {1: 'Trips', 2: 'For', 3: 'chips'} Dictionary with each item as a pair: {1: 'Trips', 2: 'For'} Dict = {} #Set default value Dict . setdefault ( 1 , 'Trips' ) Dict . setdefault ( 3 , 'chips' ) print ( Dict ) {1: 'Trips', 3: 'chips'} empty_dic = {} book_dictionary = { \"Title\" : \"Frankenstein\" , \"Author\" : \"Mary Shelley\" , \"Year\" : 1818 } print ( book_dictionary [ \"Author\" ]) Mary Shelley Above, the keys of the book_dictionary are \"Title\", \"Author\", and \"Year\", and each of these keys has a corresponding value associated with it. Notice that the key-value pairs are separated by a comma. Using keys allows us to access a piece of the dictionary by its name, rather than needing to know the index of the piece that we want, as is the case with lists and tuples. For instance, above we could get the author of Frankenstein using the \"Author\" key, rather than using an index. In fact, unlike in a list or tuple, the order of elements in a dictionary doesn't matter, and dictionaries cannot be indexed using integers, which we see below when we try to access the second element of the dictionary using an integer: print ( book_dictionary [ 1 ]) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-11-43bbaea82a52> in <module>() ----> 1 print(book_dictionary[1]) KeyError: 1 Just like lists, dictionary keys can be assigned to different values. However, unlike lists, a new dictionary key can also be assigned a value, not just ones that already exist. squares = { 1 : 1 , 2 : 4 , 3 : \"Error\" , 4 : 16 ,} squares [ 8 ] = 64 squares [ 3 ] = 9 print ( squares ) {1: 1, 2: 4, 3: 9, 4: 16, 8: 64} in operator To determine whether a key is in a dictionary, you can use in and not in, just as you can for a list. nums = { 1 : \"one\" , 2 : \"two\" , 3 : \"three\" ,} print ( 1 in nums ) print ( \"three\" in nums ) print ( 4 not in nums ) True False True Dictionary Functions get() A useful dictionary method is get. It does the same thing as indexing, but if the key is not found in the dictionary it returns another specified value instead ('None', by default). pairs = { 1 : \"apple\" , \"orange\" :[ 2 , 3 , 4 ], True : False , None : \"True\" } print ( pairs . get ( \"orange\" )) print ( pairs . get ( 7 )) print ( pairs . get ( 12345 , \"not in dictionary\" )) ##Default message when not found fib = { 1 : 1 , 2 : 1 , 3 : 2 , 4 : 3 ,} print ( fib . get ( 4 , 0 ) + fib . get ( 7 , 5 )) [2, 3, 4] None not in dictionary 8 values() To iterate over the values of a dictionary, you can use the .values() function: for value in data . values (): pass Other funtions: copy() They copy() method returns a shallow copy of the dictionary. clear() The clear() method removes all items from the dictionary. pop() Removes and returns an element from a dictionary having the given key. popitem() Removes the arbitrary key-value pair from the dictionary and returns it as tuple. get() It is a conventional method to access a value for a key. dictionary_name.values() returns a list of all the values available in a given dictionary. str() Produces a printable string representation of a dictionary. update() Adds dictionary dict2\u2019s key-values pairs to dict setdefault() Set dict[key]=default if key is not already in dict keys() Returns list of dictionary dict\u2019s keys items() Returns a list of dict\u2019s (key, value) tuple pairs has_key() Returns true if key in dictionary dict, false otherwise fromkeys() Create a new dictionary with keys from seq and values set to value. type() Returns the type of the passed variable. cmp() Compares elements of both dict.","title":"Dictionary"},{"location":"DS/pyDict/#dict-creation","text":"An empty dictionary is defined as {}. Dictionary can also be created by the built-in function dict(). # Creating an empty Dictionary Dict = {} print ( \"Empty Dictionary: \" ) print ( Dict ) # Creating a Dictionary # with dict() method Dict = dict ({ 1 : 'Trips' , 2 : 'For' , 3 : 'chips' }) print ( \" \\n Dictionary with the use of dict(): \" ) print ( Dict ) # Creating a Dictionary # with each item as a Pair Dict = dict ([( 1 , 'Trips' ), ( 2 , 'For' )]) print ( \" \\n Dictionary with each item as a pair: \" ) print ( Dict ) Empty Dictionary: {} Dictionary with the use of dict(): {1: 'Trips', 2: 'For', 3: 'chips'} Dictionary with each item as a pair: {1: 'Trips', 2: 'For'} Dict = {} #Set default value Dict . setdefault ( 1 , 'Trips' ) Dict . setdefault ( 3 , 'chips' ) print ( Dict ) {1: 'Trips', 3: 'chips'} empty_dic = {} book_dictionary = { \"Title\" : \"Frankenstein\" , \"Author\" : \"Mary Shelley\" , \"Year\" : 1818 } print ( book_dictionary [ \"Author\" ]) Mary Shelley Above, the keys of the book_dictionary are \"Title\", \"Author\", and \"Year\", and each of these keys has a corresponding value associated with it. Notice that the key-value pairs are separated by a comma. Using keys allows us to access a piece of the dictionary by its name, rather than needing to know the index of the piece that we want, as is the case with lists and tuples. For instance, above we could get the author of Frankenstein using the \"Author\" key, rather than using an index. In fact, unlike in a list or tuple, the order of elements in a dictionary doesn't matter, and dictionaries cannot be indexed using integers, which we see below when we try to access the second element of the dictionary using an integer: print ( book_dictionary [ 1 ]) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-11-43bbaea82a52> in <module>() ----> 1 print(book_dictionary[1]) KeyError: 1 Just like lists, dictionary keys can be assigned to different values. However, unlike lists, a new dictionary key can also be assigned a value, not just ones that already exist. squares = { 1 : 1 , 2 : 4 , 3 : \"Error\" , 4 : 16 ,} squares [ 8 ] = 64 squares [ 3 ] = 9 print ( squares ) {1: 1, 2: 4, 3: 9, 4: 16, 8: 64}","title":"Dict Creation"},{"location":"DS/pyDict/#in-operator","text":"To determine whether a key is in a dictionary, you can use in and not in, just as you can for a list. nums = { 1 : \"one\" , 2 : \"two\" , 3 : \"three\" ,} print ( 1 in nums ) print ( \"three\" in nums ) print ( 4 not in nums ) True False True","title":"in operator"},{"location":"DS/pyDict/#dictionary-functions","text":"","title":"Dictionary Functions"},{"location":"DS/pyDict/#get","text":"A useful dictionary method is get. It does the same thing as indexing, but if the key is not found in the dictionary it returns another specified value instead ('None', by default). pairs = { 1 : \"apple\" , \"orange\" :[ 2 , 3 , 4 ], True : False , None : \"True\" } print ( pairs . get ( \"orange\" )) print ( pairs . get ( 7 )) print ( pairs . get ( 12345 , \"not in dictionary\" )) ##Default message when not found fib = { 1 : 1 , 2 : 1 , 3 : 2 , 4 : 3 ,} print ( fib . get ( 4 , 0 ) + fib . get ( 7 , 5 )) [2, 3, 4] None not in dictionary 8","title":"get()"},{"location":"DS/pyDict/#values","text":"To iterate over the values of a dictionary, you can use the .values() function: for value in data . values (): pass","title":"values()"},{"location":"DS/pyDict/#other-funtions","text":"copy() They copy() method returns a shallow copy of the dictionary. clear() The clear() method removes all items from the dictionary. pop() Removes and returns an element from a dictionary having the given key. popitem() Removes the arbitrary key-value pair from the dictionary and returns it as tuple. get() It is a conventional method to access a value for a key. dictionary_name.values() returns a list of all the values available in a given dictionary. str() Produces a printable string representation of a dictionary. update() Adds dictionary dict2\u2019s key-values pairs to dict setdefault() Set dict[key]=default if key is not already in dict keys() Returns list of dictionary dict\u2019s keys items() Returns a list of dict\u2019s (key, value) tuple pairs has_key() Returns true if key in dictionary dict, false otherwise fromkeys() Create a new dictionary with keys from seq and values set to value. type() Returns the type of the passed variable. cmp() Compares elements of both dict.","title":"Other funtions:"},{"location":"DS/pyGen/","text":"Generators are a type of iterable, like lists or tuples. Unlike lists, they don't allow indexing with arbitrary indices, but they can still be iterated through with for loops. They can be created using functions and the yield statement. The yield statement is used to define a generator, replacing the return of a function to provide a result to its caller without destroying local variables. Generators are a shortcut to write functions that implement iterators. def countdown (): i = 5 while i > 0 : yield i i -= 1 for i in countdown (): print ( i ) 5 4 3 2 1 Due to the fact that they yield one item at a time, generators don't have the memory restrictions of lists. In fact, they can be infinite! generators allow you to declare a function that behaves like an iterator, i.e. it can be used in a for loop. Finite generators can be converted into lists by passing them as arguments to the list function.Using generators results in improved performance, which is the result of the lazy (on demand) generation of values, which translates to lower memory usage. Furthermore, we do not need to wait until all the elements have been generated before we start to use them. def numbers ( x ): for i in range ( x ): if i % 2 == 0 : yield i print ( list ( numbers ( 11 ))) [0, 2, 4, 6, 8, 10]","title":"Generators"},{"location":"DS/pyGenX/","text":"These are almost comprehension, but we don\u00b4t want to collect the results in a list, dict or set. Rather, we want to consume them inmediately, one by one. No data structure is created so we save memory and time. Sum the squares of the first 10 even numbers sum ( i ** 2 for i in range ( 20 ) if i % 2 == 0 ) 1140 even = i ** 2 for i in range ( 20 ) if i % 2 == 0 even = i**2 for i in range(20) if i%2 == 0 ^ SyntaxError: invalid syntax It needs parentheses!! even = ( i ** 2 for i in range ( 20 ) if i % 2 == 0 ) What is even? It is a generator object. So it implements the iterator protocol. Therefore, we can used it in loops or feed it to functions like sum that accept iterators. even <generator object <genexpr> at 0x7f8ff8005cd0> even . __next__ () 0 even . __next__ () 4 even . __next__ () 16 When one line is not sufficient but you still want the convinience of iterators. You can write a function called generator which has the special keyword yield def fibonacci () : print ( \"Let's get set!\" ) f1 , f2 = 0 , 1 while True : yield f2 f1 , f2 = f2 , f1 + f2 f = fibonacci () f <generator object fibonacci at 0x7f2ec8ba3550> f . __next__ () f . __next__ () f . __next__ () #the same as next ( f ) next ( f ) for x in fibonacci (): if x > 1000 : break print ( x ) Let's get set! 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 def fibonacci ( fmax ) : print ( \"Let's get set!\" ) f1 , f2 = 0 , 1 while True : yield f2 f1 , f2 = f2 , f1 + f2 if f2 > fmax : return [ x for x in fibonacci ( 1000 )] Let's get set! [1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987] Advance concepts for generators Context Managers Coroutines","title":"Generator Expressions"},{"location":"DS/pyGenX/#advance-concepts-for-generators","text":"Context Managers Coroutines","title":"Advance concepts for generators"},{"location":"DS/pyGraph/","text":"Graphs are used to represent many real-life applications like networks, transportation paths of a city, and social network connections. A graph is a set of connected nodes where each node is called a Vertex and the connection between two of them is called an Edge. A graph can be represented using a square matrix, where each element represents the edges: 0 indicates no edge, while 1 indicates an edge. The rows and columns represent the vertices. This type of matrix is called an adjacency matrix, because it shows if the corresponding vertices are adjacent or not. class Graph (): def __init__ ( self , size ): self . adj = [ [ 0 ] * size for i in range ( size )] self . size = size def add_edge ( self , orig , dest ): if orig > self . size or dest > self . size or orig < 0 or dest < 0 : print ( \"Invalid Edge\" ) else : self . adj [ orig - 1 ][ dest - 1 ] = 1 self . adj [ dest - 1 ][ orig - 1 ] = 1 def remove_edge ( self , orig , dest ): if orig > self . size or dest > self . size or orig < 0 or dest < 0 : print ( \"Invalid Edge\" ) else : self . adj [ orig - 1 ][ dest - 1 ] = 0 self . adj [ dest - 1 ][ orig - 1 ] = 0 def display ( self ): for row in self . adj : print () for val in row : print ( ' {:4} ' . format ( val ), end = \"\" ) A graph is a collection of nodes and edges, where nodes often represent objects or ideas, and edges represent relationships among them. For example, in a graph that represents a social network, nodes might represent people and edges might represent friendships between them. NetworkX provides data structures to represent graphs and function that implement graph algorithms. To show how it works, we\u2019ll make a small graph that represents a social network. Here\u2019s how we make a graph and add nodes. import networkx as nx G = nx . Graph () G . add_node ( 'Alice' ) G . add_node ( 'Bob' , age = 23 ) G . add_node ( 'Carol' , cat = 'mittens' ) list ( G . nodes ()) ['Alice', 'Bob', 'Carol'] Optionally, you can provide attributes that are associated with the node. In this example, Bob has an age attribute and Carol has a cat. Here\u2019s how we add edges between nodes. G . add_edge ( 'Alice' , 'Bob' ) G . add_edge ( 'Alice' , 'Carol' , type = 'enemy' ) list ( G . edges ()) [('Alice', 'Bob'), ('Alice', 'Carol')] Optionally, you can provide attributes that are associated with the edge. In this example, the second edge has an attribute called type that indicates the nature of the relationship. Here\u2019s how to draw the graph. def draw_graph ( G ): nx . draw_circular ( G , node_size = 1500 , with_labels = True ) draw_graph ( G ) Graph Representation NetworkX represents graphs using a dictionary that maps from each node to a dictionary that maps from nodes to edges. If we select an element from the top-level dictionary, the result is a dictionary-like object. G [ 'Alice' ] AtlasView({'Bob': {}, 'Carol': {'type': 'enemy'}}) So we can iterate through the neighbors of a node like this: for neighbor in G [ 'Alice' ]: print ( neighbor ) Bob Carol Or enumerate the neighbors and edges like this: for key , value in G [ 'Alice' ] . items (): print ( key , value ) Bob {} Carol {'type': 'enemy'} Edges are represented by dictionaries of attributes. In this example, the first edge has no attributes and the second has an attribute named type. We can select an edge like this: G [ 'Alice' ][ 'Carol' ] {'type': 'enemy'} To check whether there is an edge from one node to another, we can use the in operator: def has_edge ( G , u , v ): return v in G [ u ] has_edge ( G , 'Alice' , 'Bob' ) True But there\u2019s a method that does the same thing. G . has_edge ( 'Alice' , 'Bob' ) True Complete Graphs In a complete graph, all nodes are connected to each other. To make a complete graph, we\u2019ll use the following generator function, iterates through all pairs of nodes. def all_pairs ( nodes ): for i , u in enumerate ( nodes ): for j , v in enumerate ( nodes ): if i < j : yield u , v Here\u2019s a complete graph with 10 nodes: def make_complete_graph ( n ): nodes = range ( n ) G = nx . Graph () G . add_nodes_from ( nodes ) G . add_edges_from ( all_pairs ( nodes )) return G complete = make_complete_graph ( 10 ) And here\u2019s what it looks like. draw_graph ( complete ) Random Graphs Next we\u2019ll make an Erdos-Renyi graph, which is a random graph where the probability of an edge between each pair of nodes is . The helper function flip returns True with probability p and False with probability 1-p import random def flip ( p ): return random . random () < p random_pairs is a generator function that enumerates all possible pairs of nodes and yields each one with probability p def random_pairs ( nodes , p ): for edge in all_pairs ( nodes ): if flip ( p ): yield edge make_random_graph makes an ER graph where the probability of an edge between each pair of nodes is p. def make_random_graph ( n , p ): nodes = range ( n ) G = nx . Graph () G . add_nodes_from ( nodes ) G . add_edges_from ( random_pairs ( nodes , p )) return G Here\u2019s an example with n=10 and p=0.3 random_graph = make_random_graph ( 10 , 0.3 ) len ( random_graph . edges ()) 10 And here\u2019s what it looks like: draw_graph ( random_graph ) Connectivity A graph is connected if you can start from any node and follow a sequence of edges to reach any other node. To check whether a graph is connected, we\u2019ll use a version of a depth-first search. For most graphs, the basic version of DFS runs forever, because it visits the same nodes over and over. The solution is to keep track of the nodes we\u2019ve seen and avoid visiting them more than once. In the complete graph, starting from node 0, we can reach all nodes. In a random graph, it may or may not be possible to reach all nodes. Watts-Strogatz Graphs A Watts-Strogatz (WS) graph is a random graph, like an Erdos-Renyi graph, but the construction process is different. A WS graph starts with a ring lattice and randomly \u201crewires\u201d some of the edges. NetworkX provides a function that makes a WS graph, so we can see what it looks like. Here\u2019s an example with n=10 nodes, each connected to k=2 neighbors, with probability p=0 of rewiring each edge. import networkx as nx G = nx . watts_strogatz_graph ( n = 10 , k = 3 , p = 0 ) G . nodes () NodeView((0, 1, 2, 3, 4, 5, 6, 7, 8, 9)) The result is a ring where each node holds hands with its immediate neighbors. def draw_graph ( G ): nx . draw_circular ( G , node_size = 1000 , with_labels = True ) draw_graph ( G ) Note: If k is odd, it gets \u201crounded down\u201d to an even number. Depth-First Search def reachable_nodes ( G , start ): seen = set () stack = [ start ] while stack : node = stack . pop () if node not in seen : seen . add ( node ) stack . extend ( G [ node ]) return seen reachable_nodes ( G , 0 ) {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} Fast Breadth-First Search NetworkX provides a simple, fast implementation of BFS, available from the NetworkX repository on GitHub. Here is a version I modified to return a set of nodes: def plain_bfs ( G , start ): seen = set () nextlevel = { start } while nextlevel : thislevel = nextlevel nextlevel = set () for v in thislevel : if v not in seen : seen . add ( v ) nextlevel . update ( G [ v ]) return seen Let\u2019s compare this function to reachable_nodes_bfs and see which is faster. G = nx . watts_strogatz_graph ( 1000 , 10 , 0.01 ) % timeit plain_bfs ( G , 0 ) 100 loops, best of 5: 1.98 ms per loop","title":"Graphs"},{"location":"DS/pyGraph/#graph-representation","text":"NetworkX represents graphs using a dictionary that maps from each node to a dictionary that maps from nodes to edges. If we select an element from the top-level dictionary, the result is a dictionary-like object. G [ 'Alice' ] AtlasView({'Bob': {}, 'Carol': {'type': 'enemy'}}) So we can iterate through the neighbors of a node like this: for neighbor in G [ 'Alice' ]: print ( neighbor ) Bob Carol Or enumerate the neighbors and edges like this: for key , value in G [ 'Alice' ] . items (): print ( key , value ) Bob {} Carol {'type': 'enemy'} Edges are represented by dictionaries of attributes. In this example, the first edge has no attributes and the second has an attribute named type. We can select an edge like this: G [ 'Alice' ][ 'Carol' ] {'type': 'enemy'} To check whether there is an edge from one node to another, we can use the in operator: def has_edge ( G , u , v ): return v in G [ u ] has_edge ( G , 'Alice' , 'Bob' ) True But there\u2019s a method that does the same thing. G . has_edge ( 'Alice' , 'Bob' ) True","title":"Graph Representation"},{"location":"DS/pyGraph/#complete-graphs","text":"In a complete graph, all nodes are connected to each other. To make a complete graph, we\u2019ll use the following generator function, iterates through all pairs of nodes. def all_pairs ( nodes ): for i , u in enumerate ( nodes ): for j , v in enumerate ( nodes ): if i < j : yield u , v Here\u2019s a complete graph with 10 nodes: def make_complete_graph ( n ): nodes = range ( n ) G = nx . Graph () G . add_nodes_from ( nodes ) G . add_edges_from ( all_pairs ( nodes )) return G complete = make_complete_graph ( 10 ) And here\u2019s what it looks like. draw_graph ( complete )","title":"Complete Graphs"},{"location":"DS/pyGraph/#random-graphs","text":"Next we\u2019ll make an Erdos-Renyi graph, which is a random graph where the probability of an edge between each pair of nodes is . The helper function flip returns True with probability p and False with probability 1-p import random def flip ( p ): return random . random () < p random_pairs is a generator function that enumerates all possible pairs of nodes and yields each one with probability p def random_pairs ( nodes , p ): for edge in all_pairs ( nodes ): if flip ( p ): yield edge make_random_graph makes an ER graph where the probability of an edge between each pair of nodes is p. def make_random_graph ( n , p ): nodes = range ( n ) G = nx . Graph () G . add_nodes_from ( nodes ) G . add_edges_from ( random_pairs ( nodes , p )) return G Here\u2019s an example with n=10 and p=0.3 random_graph = make_random_graph ( 10 , 0.3 ) len ( random_graph . edges ()) 10 And here\u2019s what it looks like: draw_graph ( random_graph )","title":"Random Graphs"},{"location":"DS/pyGraph/#connectivity","text":"A graph is connected if you can start from any node and follow a sequence of edges to reach any other node. To check whether a graph is connected, we\u2019ll use a version of a depth-first search. For most graphs, the basic version of DFS runs forever, because it visits the same nodes over and over. The solution is to keep track of the nodes we\u2019ve seen and avoid visiting them more than once. In the complete graph, starting from node 0, we can reach all nodes. In a random graph, it may or may not be possible to reach all nodes.","title":"Connectivity"},{"location":"DS/pyGraph/#watts-strogatz-graphs","text":"A Watts-Strogatz (WS) graph is a random graph, like an Erdos-Renyi graph, but the construction process is different. A WS graph starts with a ring lattice and randomly \u201crewires\u201d some of the edges. NetworkX provides a function that makes a WS graph, so we can see what it looks like. Here\u2019s an example with n=10 nodes, each connected to k=2 neighbors, with probability p=0 of rewiring each edge. import networkx as nx G = nx . watts_strogatz_graph ( n = 10 , k = 3 , p = 0 ) G . nodes () NodeView((0, 1, 2, 3, 4, 5, 6, 7, 8, 9)) The result is a ring where each node holds hands with its immediate neighbors. def draw_graph ( G ): nx . draw_circular ( G , node_size = 1000 , with_labels = True ) draw_graph ( G ) Note: If k is odd, it gets \u201crounded down\u201d to an even number.","title":"Watts-Strogatz Graphs"},{"location":"DS/pyGraph/#depth-first-search","text":"def reachable_nodes ( G , start ): seen = set () stack = [ start ] while stack : node = stack . pop () if node not in seen : seen . add ( node ) stack . extend ( G [ node ]) return seen reachable_nodes ( G , 0 ) {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}","title":"Depth-First Search"},{"location":"DS/pyGraph/#fast-breadth-first-search","text":"NetworkX provides a simple, fast implementation of BFS, available from the NetworkX repository on GitHub. Here is a version I modified to return a set of nodes: def plain_bfs ( G , start ): seen = set () nextlevel = { start } while nextlevel : thislevel = nextlevel nextlevel = set () for v in thislevel : if v not in seen : seen . add ( v ) nextlevel . update ( G [ v ]) return seen Let\u2019s compare this function to reachable_nodes_bfs and see which is faster. G = nx . watts_strogatz_graph ( 1000 , 10 , 0.01 ) % timeit plain_bfs ( G , 0 ) 100 loops, best of 5: 1.98 ms per loop","title":"Fast Breadth-First Search"},{"location":"DS/pyHeapQueue/","text":"Heap data structure is mainly used to represent a priority queue. In Python, it is available using \u201cheapq\u201d module. The property of this data structure in Python is that each time the smallest of heap element is popped(min heap). Whenever elements are pushed or popped, heap structure is maintained. The heap[0] element also returns the smallest element each time. If the list is a heap, the tree should have the heap property: Every parent is less than or equal to its children. heapify(iterable) This function is used to convert the iterable into a heap data structure. i.e. in heap order. heappush(heap, ele) This function is used to insert the element mentioned in its arguments into heap. The order is adjusted, so as heap structure is maintained. heappop(heap) This function is used to remove and return the smallest element from heap. The order is adjusted, so as heap structure is maintained. # Python code to demonstrate working of # heapify(), heappush() and heappop() # importing \"heapq\" to implement heap queue import heapq # initializing list li = [ 5 , 7 , 9 , 1 , 3 ] # using heapify to convert list into heap heapq . heapify ( li ) # printing created heap print ( \"The created heap is : \" , end = \"\" ) print ( list ( li )) # using heappush() to push elements into heap # pushes 4 heapq . heappush ( li , 4 ) # printing modified heap print ( \"The modified heap after push is : \" , end = \"\" ) print ( list ( li )) # using heappop() to pop smallest element print ( \"The popped and smallest element is : \" , end = \"\" ) print ( heapq . heappop ( li )) The created heap is : [1, 3, 9, 7, 5] The modified heap after push is : [1, 3, 4, 7, 5, 9] The popped and smallest element is : 1 heappushpop(heap, ele) This function combines the functioning of both push and pop operations in one statement, increasing efficiency. Heap order is maintained after this operation. heapreplace(heap, ele) This function also inserts and pops element in one statement, but it is different from above function. In this, element is first popped, then the element is pushed.i.e, the value larger than the pushed value can be returned. heapreplace() returns the smallest value originally in heap regardless of the pushed element as opposed to heappushpop(). # Python code to demonstrate working of # heappushpop() and heapreplce() # importing \"heapq\" to implement heap queue import heapq # initializing list 1 li1 = [ 5 , 7 , 9 , 4 , 3 ] # initializing list 2 li2 = [ 5 , 7 , 9 , 4 , 3 ] # using heapify() to convert list into heap heapq . heapify ( li1 ) heapq . heapify ( li2 ) # using heappushpop() to push and pop items simultaneously # pops 2 print ( \"The popped item using heappushpop() is : \" , end = \"\" ) print ( heapq . heappushpop ( li1 , 2 )) # using heapreplace() to push and pop items simultaneously # pops 3 print ( \"The popped item using heapreplace() is : \" , end = \"\" ) print ( heapq . heapreplace ( li2 , 2 )) The popped item using heappushpop() is : 2 The popped item using heapreplace() is : 3 nlargest(k, iterable, key = fun) This function is used to return the k largest elements from the iterable specified and satisfying the key if mentioned. nsmallest(k, iterable, key = fun) This function is used to return the k smallest elements from the iterable specified and satisfying the key if mentioned. # Python code to demonstrate working of # nlargest() and nsmallest() # importing \"heapq\" to implement heap queue import heapq # initializing list li1 = [ 6 , 7 , 9 , 4 , 3 , 5 , 8 , 10 , 1 ] # using heapify() to convert list into heap heapq . heapify ( li1 ) # using nlargest to print 3 largest numbers # prints 10, 9 and 8 print ( \"The 3 largest numbers in list are : \" , end = \"\" ) print ( heapq . nlargest ( 3 , li1 )) # using nsmallest to print 3 smallest numbers # prints 1, 3 and 4 print ( \"The 3 smallest numbers in list are : \" , end = \"\" ) print ( heapq . nsmallest ( 3 , li1 )) The 3 largest numbers in list are : [10, 9, 8] The 3 smallest numbers in list are : [1, 3, 4]","title":"HeapQueue"},{"location":"DS/pyHeapQueue/#heapifyiterable","text":"This function is used to convert the iterable into a heap data structure. i.e. in heap order.","title":"heapify(iterable)"},{"location":"DS/pyHeapQueue/#heappushheap-ele","text":"This function is used to insert the element mentioned in its arguments into heap. The order is adjusted, so as heap structure is maintained.","title":"heappush(heap, ele)"},{"location":"DS/pyHeapQueue/#heappopheap","text":"This function is used to remove and return the smallest element from heap. The order is adjusted, so as heap structure is maintained. # Python code to demonstrate working of # heapify(), heappush() and heappop() # importing \"heapq\" to implement heap queue import heapq # initializing list li = [ 5 , 7 , 9 , 1 , 3 ] # using heapify to convert list into heap heapq . heapify ( li ) # printing created heap print ( \"The created heap is : \" , end = \"\" ) print ( list ( li )) # using heappush() to push elements into heap # pushes 4 heapq . heappush ( li , 4 ) # printing modified heap print ( \"The modified heap after push is : \" , end = \"\" ) print ( list ( li )) # using heappop() to pop smallest element print ( \"The popped and smallest element is : \" , end = \"\" ) print ( heapq . heappop ( li )) The created heap is : [1, 3, 9, 7, 5] The modified heap after push is : [1, 3, 4, 7, 5, 9] The popped and smallest element is : 1","title":"heappop(heap)"},{"location":"DS/pyHeapQueue/#heappushpopheap-ele","text":"This function combines the functioning of both push and pop operations in one statement, increasing efficiency. Heap order is maintained after this operation.","title":"heappushpop(heap, ele)"},{"location":"DS/pyHeapQueue/#heapreplaceheap-ele","text":"This function also inserts and pops element in one statement, but it is different from above function. In this, element is first popped, then the element is pushed.i.e, the value larger than the pushed value can be returned. heapreplace() returns the smallest value originally in heap regardless of the pushed element as opposed to heappushpop(). # Python code to demonstrate working of # heappushpop() and heapreplce() # importing \"heapq\" to implement heap queue import heapq # initializing list 1 li1 = [ 5 , 7 , 9 , 4 , 3 ] # initializing list 2 li2 = [ 5 , 7 , 9 , 4 , 3 ] # using heapify() to convert list into heap heapq . heapify ( li1 ) heapq . heapify ( li2 ) # using heappushpop() to push and pop items simultaneously # pops 2 print ( \"The popped item using heappushpop() is : \" , end = \"\" ) print ( heapq . heappushpop ( li1 , 2 )) # using heapreplace() to push and pop items simultaneously # pops 3 print ( \"The popped item using heapreplace() is : \" , end = \"\" ) print ( heapq . heapreplace ( li2 , 2 )) The popped item using heappushpop() is : 2 The popped item using heapreplace() is : 3","title":"heapreplace(heap, ele)"},{"location":"DS/pyHeapQueue/#nlargestk-iterable-key-fun","text":"This function is used to return the k largest elements from the iterable specified and satisfying the key if mentioned.","title":"nlargest(k, iterable, key = fun)"},{"location":"DS/pyHeapQueue/#nsmallestk-iterable-key-fun","text":"This function is used to return the k smallest elements from the iterable specified and satisfying the key if mentioned. # Python code to demonstrate working of # nlargest() and nsmallest() # importing \"heapq\" to implement heap queue import heapq # initializing list li1 = [ 6 , 7 , 9 , 4 , 3 , 5 , 8 , 10 , 1 ] # using heapify() to convert list into heap heapq . heapify ( li1 ) # using nlargest to print 3 largest numbers # prints 10, 9 and 8 print ( \"The 3 largest numbers in list are : \" , end = \"\" ) print ( heapq . nlargest ( 3 , li1 )) # using nsmallest to print 3 smallest numbers # prints 1, 3 and 4 print ( \"The 3 smallest numbers in list are : \" , end = \"\" ) print ( heapq . nsmallest ( 3 , li1 )) The 3 largest numbers in list are : [10, 9, 8] The 3 smallest numbers in list are : [1, 3, 4]","title":"nsmallest(k, iterable, key = fun)"},{"location":"DS/pyIterAllAny/","text":"Often used in conditional statements, all and any take a list as an argument, and return True if all or any (respectively) of their arguments evaluate to True (and False otherwise). nums = [ 55 , 44 , 33 , 22 , 11 ,] if all ([ i > 5 for i in nums ]): print ( \"All larger than 5\" ) if any ([ i % 2 == 0 for i in nums ]): print ( \"At least one is even\" ) All larger than 5 At least one is even","title":"All & Any"},{"location":"DS/pyIterEnumerate/","text":"The function enumerate can be used to iterate through the values and indices of a list simultaneously. Enumerate() method adds a counter to an iterable and returns it in a form of enumerating object. This enumerated object can then be used directly for loops or converted into a list of tuples using the list() method. nums = [ 55 , 44 , 33 , 22 , 11 ,] #Without enumerate use index variable index = 0 for value in nums : print ( index , value ) index += 1 0 55 1 44 2 33 3 22 4 11 nums = [ 55 , 44 , 33 , 22 , 11 ,] #Using range and len for index in range ( len ( nums )): value = nums [ index ] print ( index , value ) 0 55 1 44 2 33 3 22 4 11 #using enumarate nums = [ 55 , 44 , 33 , 22 , 11 ,] for v in enumerate ( nums ): print ( v ) (0, 55) (1, 44) (2, 33) (3, 22) (4, 11) nums = [ 55 , 44 , 33 , 22 , 11 ,] for i , v in enumerate ( nums ): print ( f \"value { v } is at { i } position\" ) value 55 is at 0 position value 44 is at 1 position value 33 is at 2 position value 22 is at 3 position value 11 is at 4 position nums = [ 55 , 44 , 33 , 22 , 11 ,] for i , v in enumerate ( nums , start = 10 ): print ( f \"value { v } is at { i } position\" ) value 55 is at 10 position value 44 is at 11 position value 33 is at 12 position value 22 is at 13 position value 11 is at 14 position","title":"Enumerate"},{"location":"DS/pyIterMapFilter/","text":"The built-in functions map and filter are very useful higher-order functions that operate on lists (or similar objects called iterables). Map The function map takes a function and an iterable as arguments, and returns a new iterable with the function applied to each argument. def add_five ( x ): return x + 5 nums = [ 11 , 22 , 33 , 44 , 55 ] result = list ( map ( add_five , nums )) print ( result ) <class 'list'> [16, 27, 38, 49, 60] #the same using lambda syntax result = list ( map ( lambda x : x + 5 , nums )) print ( result ) [16, 27, 38, 49, 60] Filter The function filter filters an iterable by removing items that don't match a predicate (a function that returns a Boolean). Like map, the result has to be explicitly converted to a list if you want to print it. nums = [ 11 , 22 , 33 , 44 , 55 ] res = list ( filter ( lambda x : x % 2 == 0 , nums )) print ( res ) [22, 44] # use transform functions like sorted, filter, map def filterFunc ( x ): if x % 2 == 0 : return False return True def filterFunc2 ( x ): if x . isupper (): return False return True def squareFunc ( x ): return x ** 2 def toGrade ( x ): if ( x >= 90 ): return \"A\" elif ( x >= 80 and x < 90 ): return \"B\" elif ( x >= 70 and x < 80 ): return \"C\" elif ( x >= 65 and x < 70 ): return \"D\" return \"F\" def main (): # define some sample sequences to operate on nums = ( 1 , 8 , 4 , 5 , 13 , 26 , 381 , 410 , 58 , 47 ) chars = \"abcDeFGHiJklmnoP\" grades = ( 81 , 89 , 94 , 78 , 61 , 66 , 99 , 74 ) # use filter to remove items from a list odds = list ( filter ( filterFunc , nums )) print ( odds ) # use filter on non-numeric sequence lowers = list ( filter ( filterFunc2 , chars )) print ( lowers ) # use map to create a new sequence of values squares = list ( map ( squareFunc , nums )) print ( squares ) # use sorted and map to change numbers to grades grades = sorted ( grades ) letters = list ( map ( toGrade , grades )) print ( letters ) if __name__ == \"__main__\" : main () [1, 5, 13, 381, 47] ['a', 'b', 'c', 'e', 'i', 'k', 'l', 'm', 'n', 'o'] [1, 64, 16, 25, 169, 676, 145161, 168100, 3364, 2209] ['F', 'D', 'C', 'C', 'B', 'B', 'A', 'A']","title":"Map & Filter"},{"location":"DS/pyIterMapFilter/#map","text":"The function map takes a function and an iterable as arguments, and returns a new iterable with the function applied to each argument. def add_five ( x ): return x + 5 nums = [ 11 , 22 , 33 , 44 , 55 ] result = list ( map ( add_five , nums )) print ( result ) <class 'list'> [16, 27, 38, 49, 60] #the same using lambda syntax result = list ( map ( lambda x : x + 5 , nums )) print ( result ) [16, 27, 38, 49, 60]","title":"Map"},{"location":"DS/pyIterMapFilter/#filter","text":"The function filter filters an iterable by removing items that don't match a predicate (a function that returns a Boolean). Like map, the result has to be explicitly converted to a list if you want to print it. nums = [ 11 , 22 , 33 , 44 , 55 ] res = list ( filter ( lambda x : x % 2 == 0 , nums )) print ( res ) [22, 44] # use transform functions like sorted, filter, map def filterFunc ( x ): if x % 2 == 0 : return False return True def filterFunc2 ( x ): if x . isupper (): return False return True def squareFunc ( x ): return x ** 2 def toGrade ( x ): if ( x >= 90 ): return \"A\" elif ( x >= 80 and x < 90 ): return \"B\" elif ( x >= 70 and x < 80 ): return \"C\" elif ( x >= 65 and x < 70 ): return \"D\" return \"F\" def main (): # define some sample sequences to operate on nums = ( 1 , 8 , 4 , 5 , 13 , 26 , 381 , 410 , 58 , 47 ) chars = \"abcDeFGHiJklmnoP\" grades = ( 81 , 89 , 94 , 78 , 61 , 66 , 99 , 74 ) # use filter to remove items from a list odds = list ( filter ( filterFunc , nums )) print ( odds ) # use filter on non-numeric sequence lowers = list ( filter ( filterFunc2 , chars )) print ( lowers ) # use map to create a new sequence of values squares = list ( map ( squareFunc , nums )) print ( squares ) # use sorted and map to change numbers to grades grades = sorted ( grades ) letters = list ( map ( toGrade , grades )) print ( letters ) if __name__ == \"__main__\" : main () [1, 5, 13, 381, 47] ['a', 'b', 'c', 'e', 'i', 'k', 'l', 'm', 'n', 'o'] [1, 64, 16, 25, 169, 676, 145161, 168100, 3364, 2209] ['F', 'D', 'C', 'C', 'B', 'B', 'A', 'A']","title":"Filter"},{"location":"DS/pyIterReversed/","text":"Returns an iterator Python reversed() method returns an iterator that accesses the given sequence in the reverse order. 3 ways to reverse a sequence: iter.reverse() reverses a mutable sequence in place and is not available for inmutable sequences Slicing [::-1] creates a reversed copy of a sequence, it is the fastest but creates a copy of the sequence. Memory considerations to reverse millions of items. Used for both mutable and inmutable sequences. reversed() returns a reversed iterator, scales well to millions of items. Used for both mutable and inmutable sequences. # Python code to demonstrate working of # reversed() # For tuple seqTuple = ( 'm' , 'o' , 'r' , 'p' , 's' ) print ( list ( reversed ( seqTuple ))) # For range seqRange = range ( 1 , 5 ) print ( list ( reversed ( seqRange ))) ['s', 'p', 'r', 'o', 'm'] [4, 3, 2, 1] class pyp : vowels = [ 'a' , 'e' , 'i' , 'o' , 'u' ] # Function to reverse the list def __reversed__ ( self ): return reversed ( self . vowels ) ['u', 'o', 'i', 'e', 'a']","title":"Reversed"},{"location":"DS/pyIterSorted/","text":"Python sorted() function returns a sorted list from the iterable object. Sorted() sorts any sequence (list, tuple) and always returns a list with the elements in a sorted manner, without modifying the original sequence. x = [ 2 , 8 , 1 , 4 , 6 , 3 , 7 ] print ( \"Sorted List returned :\" ), print ( sorted ( x )) print ( \" \\n Reverse sort :\" ), print ( sorted ( x , reverse = True )) print ( \" \\n Original list not modified :\" ), print ( x ) Sorted List returned : [1, 2, 3, 4, 6, 7, 8] Reverse sort : [8, 7, 6, 4, 3, 2, 1] Original list not modified : [2, 8, 1, 4, 6, 3, 7] # List x = [ 'q' , 'w' , 'r' , 'e' , 't' , 'y' ] print ( f 'List : { sorted ( x ) } ' ) # Tuple x = ( 'q' , 'w' , 'e' , 'r' , 't' , 'y' ) print ( f 'Tuple : { sorted ( x ) } ' ) # String-sorted based on ASCII translations x = \"python\" print ( f 'String : { sorted ( x ) } ' ) # Dictionary x = { 'q' : 1 , 'w' : 2 , 'e' : 3 , 'r' : 4 , 't' : 5 , 'y' : 6 } print ( f 'Dict : { sorted ( x ) } ' ) # Set x = { 'q' , 'w' , 'e' , 'r' , 't' , 'y' } print ( f 'Set : { sorted ( x ) } ' ) # Frozen Set x = frozenset (( 'q' , 'w' , 'e' , 'r' , 't' , 'y' )) print ( f 'Frozen Set : { sorted ( x ) } ' ) List : ['e', 'q', 'r', 't', 'w', 'y'] Tuple : ['e', 'q', 'r', 't', 'w', 'y'] String : ['h', 'n', 'o', 'p', 't', 'y'] Dict : ['e', 'q', 'r', 't', 'w', 'y'] Set : ['e', 'q', 'r', 't', 'w', 'y'] Frozen Set : ['e', 'q', 'r', 't', 'w', 'y'] L = [ \"cccc\" , \"b\" , \"dd\" , \"aaa\" ] print ( \"Normal sort :\" , sorted ( L )) print ( \"Sort with len :\" , sorted ( L , key = len )) Normal sort : ['aaa', 'b', 'cccc', 'dd'] Sort with len : ['b', 'dd', 'aaa', 'cccc'] # Sort a list of integers based on # their remainder on dividing from 7 def func ( x ): return x % 7 L = [ 15 , 3 , 11 , 7 ] print ( \"Normal sort :\" , sorted ( L )) print ( \"Sorted with key:\" , sorted ( L , key = func )) Normal sort : [3, 7, 11, 15] Sorted with key: [7, 15, 3, 11]","title":"Sorted"},{"location":"DS/pyIterZip/","text":"Zip Python zip() method takes iterable or containers and returns a single iterator object, having mapped values from all the containers. It is used to map the similar index of multiple containers so that they can be used just using a single entity. name = [ \"Manjeet\" , \"Nikhil\" , \"Shambhavi\" , \"Astha\" ] roll_no = [ 4 , 1 , 3 , 2 ] # using zip() to map values mapped = zip ( name , roll_no ) print ( set ( mapped )) {('Shambhavi', 3), ('Nikhil', 1), ('Astha', 2), ('Manjeet', 4)} names = [ 'Mukesh' , 'Roni' , 'Chari' ] ages = [ 24 , 50 , 18 ] for i , ( name , age ) in enumerate ( zip ( names , ages )): print ( i , name , age ) 0 Mukesh 24 1 Roni 50 2 Chari 18 stocks = [ 'reliance' , 'infosys' , 'tcs' ] prices = [ 2175 , 1127 , 2750 ] new_dict = { stocks : prices for stocks , prices in zip ( stocks , prices )} print ( new_dict ) {'reliance': 2175, 'infosys': 1127, 'tcs': 2750} Unzip How to unzip? Unzipping means converting the zipped values back to the individual self as they were. This is done with the help of \u201c*\u201d operator. # Python code to demonstrate the working of # unzip # initializing lists name = [ \"Manjeet\" , \"Nikhil\" , \"Shambhavi\" , \"Astha\" ] roll_no = [ 4 , 1 , 3 , 2 ] marks = [ 40 , 50 , 60 , 70 ] # using zip() to map values mapped = zip ( name , roll_no , marks ) # converting values to print as list mapped = list ( mapped ) # printing resultant values print ( \"The zipped result is : \" , end = \"\" ) print ( mapped ) print ( \" \\n \" ) # unzipping values namz , roll_noz , marksz = zip ( * mapped ) print ( \"The unzipped result: \\n \" , end = \"\" ) # printing initial lists print ( \"The name list is : \" , end = \"\" ) print ( namz ) print ( \"The roll_no list is : \" , end = \"\" ) print ( roll_noz ) print ( \"The marks list is : \" , end = \"\" ) print ( marksz ) The zipped result is : [('Manjeet', 4, 40), ('Nikhil', 1, 50), ('Shambhavi', 3, 60), ('Astha', 2, 70)] The unzipped result: The name list is : ('Manjeet', 'Nikhil', 'Shambhavi', 'Astha') The roll_no list is : (4, 1, 3, 2) The marks list is : (40, 50, 60, 70)","title":"Zip"},{"location":"DS/pyIterZip/#zip","text":"Python zip() method takes iterable or containers and returns a single iterator object, having mapped values from all the containers. It is used to map the similar index of multiple containers so that they can be used just using a single entity. name = [ \"Manjeet\" , \"Nikhil\" , \"Shambhavi\" , \"Astha\" ] roll_no = [ 4 , 1 , 3 , 2 ] # using zip() to map values mapped = zip ( name , roll_no ) print ( set ( mapped )) {('Shambhavi', 3), ('Nikhil', 1), ('Astha', 2), ('Manjeet', 4)} names = [ 'Mukesh' , 'Roni' , 'Chari' ] ages = [ 24 , 50 , 18 ] for i , ( name , age ) in enumerate ( zip ( names , ages )): print ( i , name , age ) 0 Mukesh 24 1 Roni 50 2 Chari 18 stocks = [ 'reliance' , 'infosys' , 'tcs' ] prices = [ 2175 , 1127 , 2750 ] new_dict = { stocks : prices for stocks , prices in zip ( stocks , prices )} print ( new_dict ) {'reliance': 2175, 'infosys': 1127, 'tcs': 2750}","title":"Zip"},{"location":"DS/pyIterZip/#unzip","text":"How to unzip? Unzipping means converting the zipped values back to the individual self as they were. This is done with the help of \u201c*\u201d operator. # Python code to demonstrate the working of # unzip # initializing lists name = [ \"Manjeet\" , \"Nikhil\" , \"Shambhavi\" , \"Astha\" ] roll_no = [ 4 , 1 , 3 , 2 ] marks = [ 40 , 50 , 60 , 70 ] # using zip() to map values mapped = zip ( name , roll_no , marks ) # converting values to print as list mapped = list ( mapped ) # printing resultant values print ( \"The zipped result is : \" , end = \"\" ) print ( mapped ) print ( \" \\n \" ) # unzipping values namz , roll_noz , marksz = zip ( * mapped ) print ( \"The unzipped result: \\n \" , end = \"\" ) # printing initial lists print ( \"The name list is : \" , end = \"\" ) print ( namz ) print ( \"The roll_no list is : \" , end = \"\" ) print ( roll_noz ) print ( \"The marks list is : \" , end = \"\" ) print ( marksz ) The zipped result is : [('Manjeet', 4, 40), ('Nikhil', 1, 50), ('Shambhavi', 3, 60), ('Astha', 2, 70)] The unzipped result: The name list is : ('Manjeet', 'Nikhil', 'Shambhavi', 'Astha') The roll_no list is : (4, 1, 3, 2) The marks list is : (40, 50, 60, 70)","title":"Unzip"},{"location":"DS/pyLinkedList/","text":"A linked list is a sequence of nodes where each node stores its own data and a link to the next node. One node links to another forming what can be thought of as a linked chain. The first node is called the head, and it's used as the starting point for any iteration through the list. The last node must have its link pointing to None to determine the end of the list. Unlike stacks and queues, you can insert and remove nodes in any position of the linked list (similar to a standard list). Applications Linked lists are useful when your data is linked. For example when you need undo/redo functionality, the nodes can represent the state with links to the previous and next states. Another example would be a playlist of music, where each clip is linked with the next one. Linked lists can also be used to create other data structures, such as stack, queues and graphs. class Node : def __init__ ( self , data , next ): self . data = data self . next = next class LinkedList : def __init__ ( self ): self . head = None def add_at_front ( self , data ): self . head = Node ( data , self . head ) def add_at_end ( self , data ): if not self . head : self . head = Node ( data , None ) return curr = self . head while curr . next : curr = curr . next curr . next = Node ( data , None ) def get_last_node ( self ): n = self . head while ( n . next != None ): n = n . next return n . data def print_list ( self ): n = self . head while n != None : print ( n . data , end = \" => \" ) n = n . next print () There are two types of linked list: Single-Linked List: In this, the nodes point to the node immediately after it Doubly Linked List: In this, the nodes not only reference the node next to it but also the node before it. To start with Python, it does not have a linked list library built into it like the classical programming languages. Python does have an inbuilt type list that works as a dynamic array but its operation shouldn\u2019t be confused with a typical function of a linked list. This doesn\u2019t mean one cannot implement a linked list in Python, they can but it will not be straight up. Using deque() package When to use deque() as a linked list? Inserting and deleting elements at front and back respectively is the only need. Inserting and removing elements from the middle becomes time-consuming. In-place reversal since Python now allows elements to be reversed in the place itself. Storage is preferred over performance and not all elements get a separate node of their own # importing module import collections # initialising a deque() of arbitrary length linked_lst = collections . deque () # filling deque() with elements linked_lst . append ( 'first' ) linked_lst . append ( 'second' ) linked_lst . append ( 'third' ) print ( \"elements in the linked_list:\" ) print ( linked_lst ) # adding element at an arbitrary position linked_lst . insert ( 1 , 'fourth' ) print ( \"elements in the linked_list:\" ) print ( linked_lst ) # deleting the last element linked_lst . pop () print ( \"elements in the linked_list:\" ) print ( linked_lst ) # removing a specific element linked_lst . remove ( 'fourth' ) print ( \"elements in the linked_list:\" ) print ( linked_lst ) elements in the linked_list: deque(['first', 'second', 'third']) elements in the linked_list: deque(['first', 'fourth', 'second', 'third']) elements in the linked_list: deque(['first', 'fourth', 'second']) elements in the linked_list: deque(['first', 'second']) Using llist package The llist is an extension module for CPython providing basic linked list data structures. pip install llist # importing packages import llist from llist import sllist , sllistnode # creating a singly linked list lst = sllist ([ 'first' , 'second' , 'third' ]) print ( lst ) print ( lst . first ) print ( lst . last ) print ( lst . size ) print () # adding and inserting values lst . append ( 'fourth' ) node = lst . nodeat ( 2 ) lst . insertafter ( 'fifth' , node ) print ( lst ) print ( lst . first ) print ( lst . last ) print ( lst . size ) print () # poping a value #i.e. removing the last entry # of the list lst . pop () print ( lst ) print ( lst . first ) print ( lst . last ) print ( lst . size ) print () # removing a specific element node = lst . nodeat ( 1 ) lst . remove ( node ) print ( lst ) print ( lst . first ) print ( lst . last ) print ( lst . size ) print () sllist([first, second, third]) sllistnode(first) sllistnode(third) 3 sllist([first, second, third, fifth, fourth]) sllistnode(first) sllistnode(fourth) 5 sllist([first, second, third, fifth]) sllistnode(first) sllistnode(fifth) 4 sllist([first, third, fifth]) sllistnode(first) sllistnode(fifth) 3 Using StructLinks package StructLinks is used to easily Access and visualize different Data structures including Linked lists, Doubly Linked lists, Binary trees, Graphs, Stacks, and Queues. The structlinks.LinkedList and structlinks.DoublyLikedList modules could be used to make linked lists. All the operations that could be performed with a list could also be performed with structlinks.LinkedList class. # 1. Download the repo and set it as the current directory git clone https : // github . com / eeshannarula29 / structlinks # 2. Add the project directory to the path import os , sys sys . path . append ( os . getcwd ()) cd LinkedList / import LinkedList as ll # create an empty linked list lst = ll . LinkedList () # create a linked list with initial values lst = ll . LinkedList ([ 1 , 10.0 , 'string' ]) print ( lst ) print () print ( 'Elements of list:' ) # elements of the list element0 = lst [ 0 ] element1 = lst [ 1 ] element2 = lst [ 2 ] print ( f 'first element : { element0 } ' ) print ( f 'second element : { element1 } ' ) print ( f 'third element : { element2 } ' ) print () print ( 'Length of list:' ) # Length of the list length = len ( lst ) print ( f 'size of the list : { length } ' ) print () print ( 'Set item:' ) # Set item lst [ 0 ] = 10 print ( f 'list after setting lst[0] to 10 : { lst } ' ) print () print ( 'Append And Insert:' ) # Append And Insert lst . append ( 'another string' ) lst . insert ( 1 , 0.0 ) print ( f 'list after appedning and inserting: { lst } ' ) print () print ( 'Pop and Remove' ) # Pop and Remove element = lst . pop ( 0 ) lst . remove ( 10.0 ) print ( f 'list after poping and removing : { lst } ' ) print ( f 'pop function also returns the element : { element } ' ) [1 -> 10 -> -3 -> 5] Elements of list: first element : 1 second element : 10 third element : -3 Length of list: size of the list : 4 Set item: list after setting lst[0] to 10 : [10 -> 10 -> -3 -> 5] Append And Insert: list after appedning and inserting: [10 -> 0.0 -> 10 -> -3 -> 5 -> another string] Pop and Remove list after poping and removing : [0.0 -> -3 -> 5 -> another string] pop function also returns the element : 10","title":"LinkedList"},{"location":"DS/pyLinkedList/#using-deque-package","text":"When to use deque() as a linked list? Inserting and deleting elements at front and back respectively is the only need. Inserting and removing elements from the middle becomes time-consuming. In-place reversal since Python now allows elements to be reversed in the place itself. Storage is preferred over performance and not all elements get a separate node of their own # importing module import collections # initialising a deque() of arbitrary length linked_lst = collections . deque () # filling deque() with elements linked_lst . append ( 'first' ) linked_lst . append ( 'second' ) linked_lst . append ( 'third' ) print ( \"elements in the linked_list:\" ) print ( linked_lst ) # adding element at an arbitrary position linked_lst . insert ( 1 , 'fourth' ) print ( \"elements in the linked_list:\" ) print ( linked_lst ) # deleting the last element linked_lst . pop () print ( \"elements in the linked_list:\" ) print ( linked_lst ) # removing a specific element linked_lst . remove ( 'fourth' ) print ( \"elements in the linked_list:\" ) print ( linked_lst ) elements in the linked_list: deque(['first', 'second', 'third']) elements in the linked_list: deque(['first', 'fourth', 'second', 'third']) elements in the linked_list: deque(['first', 'fourth', 'second']) elements in the linked_list: deque(['first', 'second'])","title":"Using deque() package"},{"location":"DS/pyLinkedList/#using-llist-package","text":"The llist is an extension module for CPython providing basic linked list data structures. pip install llist # importing packages import llist from llist import sllist , sllistnode # creating a singly linked list lst = sllist ([ 'first' , 'second' , 'third' ]) print ( lst ) print ( lst . first ) print ( lst . last ) print ( lst . size ) print () # adding and inserting values lst . append ( 'fourth' ) node = lst . nodeat ( 2 ) lst . insertafter ( 'fifth' , node ) print ( lst ) print ( lst . first ) print ( lst . last ) print ( lst . size ) print () # poping a value #i.e. removing the last entry # of the list lst . pop () print ( lst ) print ( lst . first ) print ( lst . last ) print ( lst . size ) print () # removing a specific element node = lst . nodeat ( 1 ) lst . remove ( node ) print ( lst ) print ( lst . first ) print ( lst . last ) print ( lst . size ) print () sllist([first, second, third]) sllistnode(first) sllistnode(third) 3 sllist([first, second, third, fifth, fourth]) sllistnode(first) sllistnode(fourth) 5 sllist([first, second, third, fifth]) sllistnode(first) sllistnode(fifth) 4 sllist([first, third, fifth]) sllistnode(first) sllistnode(fifth) 3","title":"Using llist package"},{"location":"DS/pyLinkedList/#using-structlinks-package","text":"StructLinks is used to easily Access and visualize different Data structures including Linked lists, Doubly Linked lists, Binary trees, Graphs, Stacks, and Queues. The structlinks.LinkedList and structlinks.DoublyLikedList modules could be used to make linked lists. All the operations that could be performed with a list could also be performed with structlinks.LinkedList class. # 1. Download the repo and set it as the current directory git clone https : // github . com / eeshannarula29 / structlinks # 2. Add the project directory to the path import os , sys sys . path . append ( os . getcwd ()) cd LinkedList / import LinkedList as ll # create an empty linked list lst = ll . LinkedList () # create a linked list with initial values lst = ll . LinkedList ([ 1 , 10.0 , 'string' ]) print ( lst ) print () print ( 'Elements of list:' ) # elements of the list element0 = lst [ 0 ] element1 = lst [ 1 ] element2 = lst [ 2 ] print ( f 'first element : { element0 } ' ) print ( f 'second element : { element1 } ' ) print ( f 'third element : { element2 } ' ) print () print ( 'Length of list:' ) # Length of the list length = len ( lst ) print ( f 'size of the list : { length } ' ) print () print ( 'Set item:' ) # Set item lst [ 0 ] = 10 print ( f 'list after setting lst[0] to 10 : { lst } ' ) print () print ( 'Append And Insert:' ) # Append And Insert lst . append ( 'another string' ) lst . insert ( 1 , 0.0 ) print ( f 'list after appedning and inserting: { lst } ' ) print () print ( 'Pop and Remove' ) # Pop and Remove element = lst . pop ( 0 ) lst . remove ( 10.0 ) print ( f 'list after poping and removing : { lst } ' ) print ( f 'pop function also returns the element : { element } ' ) [1 -> 10 -> -3 -> 5] Elements of list: first element : 1 second element : 10 third element : -3 Length of list: size of the list : 4 Set item: list after setting lst[0] to 10 : [10 -> 10 -> -3 -> 5] Append And Insert: list after appedning and inserting: [10 -> 0.0 -> 10 -> -3 -> 5 -> another string] Pop and Remove list after poping and removing : [0.0 -> -3 -> 5 -> another string] pop function also returns the element : 10","title":"Using StructLinks package"},{"location":"DS/pyList/","text":"A list comprises a sequence of objects, usually represented using square brackets with commas between the items in the sequence as is done below: my_list = [ 'a' , 'b' , 'c' , 'd' ] print ( my_list ) ['a', 'b', 'c', 'd'] Above, my_list contains a sequence of character objects. Lists, however, accomodate items of varying types of objects: varied_list = [ 'a' , 1 , 'b' , 3.14159 ] # a list with elements of char, integer, and float types nested_list = [ 'hello' , 'governor' , [ 1.618 , 42 ]] # a list within a list! Lists allow for what is called indexing, in which a specified element of the list may be obtained. For instance, say you wanted to grab the second element of varied_list above. Then you could index the list as so: second_element = varied_list [ 1 ] # Grab second element of varied_list print ( second_element ) Now is a good time to mention that Python is what's called a zero-indexed programming language. This simply means that the \"first\" element in a list or other collection of data items is indexed using \"0\" (zero) rather than \"1\". This is why, above, we grab the second element of varied_list using the integer index \"1\" instead of \"2\" as some might expect from a one-indexed language (like MATLab). Another feature of python indexing that comes in handy is the use of negative indexing. As we discussed above, the \"first\" element of a python list is denoted by index \"0\"; thus, it is almost natural to consider the last element of the list as being indexed by \"-1\". Observe the following examples of negative indexing: last_element = my_list [ - 1 ] # the last element of my_list last_element_2 = my_list [ len ( my_list ) - 1 ] # also the last element of my_list, obtained differently second_to_last_element = my_list [ - 2 ] Similar to indexing is list slicing, in which a contiguous section of list may be accessed. The colon (:) is used to perform slicing, with integers denoting the positions at which to begin and end the slice. Below, we show that the beginning or ending integer for a slice may be omited when one is slicing from the beginning or to the end of the list. Also note below that the index for slice beginning is included in the slice, but the index for the slice end is not included. NFL_list = [ \"Chargers\" , \"Broncos\" , \"Raiders\" , \"Chiefs\" , \"Panthers\" , \"Falcons\" , \"Cowboys\" , \"Eagles\" ] AFC_west_list = NFL_list [: 4 ] # Slice to grab list indices 0, 1, 2, 3 -- \"Chargers\", \"Broncos\", \"Raiders\", \"Chiefs\" NFC_south_list = NFL_list [ 4 : 6 ] # Slice list indices 4, 5 -- \"Panthers\", \"Falcons\" NFC_east_list = NFL_list [ 6 :] # Slice list indices 6, 7 -- \"Cowboys\", \"Eagles\" List slices can also have a third number, representing the step, to include only alternate values in the slice. NFL_list = [ \"Chargers\" , \"Broncos\" , \"Raiders\" , \"Chiefs\" , \"Panthers\" , \"Falcons\" , \"Cowboys\" , \"Eagles\" ] list1 = NFL_list [ 3 :: 2 ] print ( list1 ) ['Chiefs', 'Falcons', 'Eagles'] Negative values can be used in list slicing (and normal list indexing). When negative values are used for the first and second values in a slice (or a normal index), they count from the end of the list. If a negative value is used for the step, the slice is done backwards. Using [::-1] as a slice is a common and idiomatic way to reverse a list. squares = [ 0 , 1 , 4 , 9 , 16 , 25 , 36 , 49 , 64 , 81 ] print ( squares [ 1 : - 3 ]) print ( squares [:: - 1 ]) print ( squares [: 4 : - 1 ]) [1, 4, 9, 16, 25, 36] [81, 64, 49, 36, 25, 16, 9, 4, 1, 0] [81, 64, 49, 36, 25] Sometimes you need to create an empty list and populate it later during the program. For example, if you are creating a queue management program, the queue is going to be empty in the beginning and get populated with people data later. An empty list is created with an empty pair of square brackets. Nested lists can be used to represent 2D grids, such as matrices. Indexing strings behaves as though you are indexing a list containing each character in the string. empty_list = [] print ( empty_list ) m = [ [ 1 , 2 , 3 ], [ 4 , 5 , 6 ] ] print ( m ) s = \"Hello world\" print ( s [ 6 ]) [] [[1, 2, 3], [4, 5, 6]] w List Operations The item at a certain index in a list can be reassigned. Lists can be added and multiplied in the same way as strings. To check if an item is in a list, the in operator can be used. It returns True if the item occurs one or more times in the list, and False if it doesn't. The in operator is also used to determine whether or not a string is a substring of another string. M = [ 1 , 1 , 1 ] M [ 1 ] = \"hello\" print ( M ) print ( M * 3 ) print ( M + [ 1 , 2 ]) print ( 1 in M ) print ( \"Spam\" in M ) print ( not \"hello\" in M ) [1, 'hello', 1] [1, 'hello', 1, 1, 'hello', 1, 1, 'hello', 1] [1, 'hello', 1, 1, 2] True False False List Functions len(list): to get the number of items in a list. max(list): Returns the list item with the maximum value min(list): Returns the list item with minimum value list.append(item): adds an item to the end of an existing list. list.insert(index, item): is similar to append, except that it allows you to insert a new item at any position in the list, as opposed to just at the end. list.index(item): finds the first occurrence of a list item and returns its index. If the item isn't in the list, it raises a ValueError. list.count(item): Returns a count of how many times an item occurs in a list list.remove(item): Removes an object from a list list.pop(index) removes the item at the given index. list.reverse(): Reverses items in a list. list.sort() sorts the list. By default, the list is sorted ascending. You can specify reverse=True as the parameter, to sort descending. nums = [ 1 , 2 , 3 ] nums . append ( 4 ) print ( nums ) nums . insert ( 2 , \"hello\" ) print ( nums ) print ( len ( nums )) print ( nums . index ( 3 )) nums += [ 1 , 2 , 1 ] print ( nums . count ( 1 )) nums . remove ( \"hello\" ) print ( nums . reverse ()) print ( max ( nums )) print ( min ( nums )) [1, 2, 3, 4] [1, 2, 'hello', 3, 4] 5 3 3 None 4 1 # use iterator functions like enumerate, zip, iter, next # define a list of days in English and French days = [ \"Sun\" , \"Mon\" , \"Tue\" , \"Wed\" , \"Thu\" , \"Fri\" , \"Sat\" ] daysFr = [ \"Dim\" , \"Lun\" , \"Mar\" , \"Mer\" , \"Jeu\" , \"Ven\" , \"Sam\" ] # use iter to create an iterator over a collection i = iter ( days ) print ( next ( i )) # Sun print ( next ( i )) # Mon print ( next ( i )) # Tue # iterate using a function and a sentinel with open ( \"game.txt\" , \"r\" ) as fp : for line in iter ( fp . readline , '' ): print ( line ) # use regular interation over the days for m in range ( len ( days )): print ( m + 1 , days [ m ]) 1 Sun 2 Mon 3 Tue 4 Wed 5 Thu 6 Fri 7 Sat # using enumerate reduces code and provides a counter for i , m in enumerate ( days , start = 1 ): print ( i , m ) 1 Sun 2 Mon 3 Tue 4 Wed 5 Thu 6 Fri 7 Sat # use zip to combine sequences for m in zip ( days , daysFr ): print ( m ) for i , m in enumerate ( zip ( days , daysFr ), start = 1 ): print ( i , m [ 0 ], \"=\" , m [ 1 ], \"in French\" ) ('Sun', 'Dim') ('Mon', 'Lun') ('Tue', 'Mar') ('Wed', 'Mer') ('Thu', 'Jeu') ('Fri', 'Ven') ('Sat', 'Sam') 1 Sun = Dim in French 2 Mon = Lun in French 3 Tue = Mar in French 4 Wed = Mer in French 5 Thu = Jeu in French 6 Fri = Ven in French 7 Sat = Sam in French","title":"Lists"},{"location":"DS/pyList/#list-operations","text":"The item at a certain index in a list can be reassigned. Lists can be added and multiplied in the same way as strings. To check if an item is in a list, the in operator can be used. It returns True if the item occurs one or more times in the list, and False if it doesn't. The in operator is also used to determine whether or not a string is a substring of another string. M = [ 1 , 1 , 1 ] M [ 1 ] = \"hello\" print ( M ) print ( M * 3 ) print ( M + [ 1 , 2 ]) print ( 1 in M ) print ( \"Spam\" in M ) print ( not \"hello\" in M ) [1, 'hello', 1] [1, 'hello', 1, 1, 'hello', 1, 1, 'hello', 1] [1, 'hello', 1, 1, 2] True False False","title":"List Operations"},{"location":"DS/pyList/#list-functions","text":"len(list): to get the number of items in a list. max(list): Returns the list item with the maximum value min(list): Returns the list item with minimum value list.append(item): adds an item to the end of an existing list. list.insert(index, item): is similar to append, except that it allows you to insert a new item at any position in the list, as opposed to just at the end. list.index(item): finds the first occurrence of a list item and returns its index. If the item isn't in the list, it raises a ValueError. list.count(item): Returns a count of how many times an item occurs in a list list.remove(item): Removes an object from a list list.pop(index) removes the item at the given index. list.reverse(): Reverses items in a list. list.sort() sorts the list. By default, the list is sorted ascending. You can specify reverse=True as the parameter, to sort descending. nums = [ 1 , 2 , 3 ] nums . append ( 4 ) print ( nums ) nums . insert ( 2 , \"hello\" ) print ( nums ) print ( len ( nums )) print ( nums . index ( 3 )) nums += [ 1 , 2 , 1 ] print ( nums . count ( 1 )) nums . remove ( \"hello\" ) print ( nums . reverse ()) print ( max ( nums )) print ( min ( nums )) [1, 2, 3, 4] [1, 2, 'hello', 3, 4] 5 3 3 None 4 1 # use iterator functions like enumerate, zip, iter, next # define a list of days in English and French days = [ \"Sun\" , \"Mon\" , \"Tue\" , \"Wed\" , \"Thu\" , \"Fri\" , \"Sat\" ] daysFr = [ \"Dim\" , \"Lun\" , \"Mar\" , \"Mer\" , \"Jeu\" , \"Ven\" , \"Sam\" ] # use iter to create an iterator over a collection i = iter ( days ) print ( next ( i )) # Sun print ( next ( i )) # Mon print ( next ( i )) # Tue # iterate using a function and a sentinel with open ( \"game.txt\" , \"r\" ) as fp : for line in iter ( fp . readline , '' ): print ( line ) # use regular interation over the days for m in range ( len ( days )): print ( m + 1 , days [ m ]) 1 Sun 2 Mon 3 Tue 4 Wed 5 Thu 6 Fri 7 Sat # using enumerate reduces code and provides a counter for i , m in enumerate ( days , start = 1 ): print ( i , m ) 1 Sun 2 Mon 3 Tue 4 Wed 5 Thu 6 Fri 7 Sat # use zip to combine sequences for m in zip ( days , daysFr ): print ( m ) for i , m in enumerate ( zip ( days , daysFr ), start = 1 ): print ( i , m [ 0 ], \"=\" , m [ 1 ], \"in French\" ) ('Sun', 'Dim') ('Mon', 'Lun') ('Tue', 'Mar') ('Wed', 'Mer') ('Thu', 'Jeu') ('Fri', 'Ven') ('Sat', 'Sam') 1 Sun = Dim in French 2 Mon = Lun in French 3 Tue = Mar in French 4 Wed = Mer in French 5 Thu = Jeu in French 6 Fri = Ven in French 7 Sat = Sam in French","title":"List Functions"},{"location":"DS/pyNamedTuple/","text":"Named tuples are basically easy-to-create, lightweight object types. Named tuple instances can be referenced using object-like variable dereferencing or the standard tuple syntax. They can be used similarly to struct or other common record types, except that they are immutable. It is common to represent a point as a tuple (x, y). This leads to code like the following: pt1 = ( 1.0 , 5.0 ) pt2 = ( 2.5 , 1.5 ) from math import sqrt line_length = sqrt (( pt1 [ 0 ] - pt2 [ 0 ]) ** 2 + ( pt1 [ 1 ] - pt2 [ 1 ]) ** 2 ) print ( line_length ) 3.8078865529319543 Using a named tuple it becomes more readable: from collections import namedtuple Point = namedtuple ( 'Point' , 'x y' ) pt1 = Point ( 1.0 , 5.0 ) pt2 = Point ( 2.5 , 1.5 ) from math import sqrt line_length = sqrt (( pt1 . x - pt2 . x ) ** 2 + ( pt1 . y - pt2 . y ) ** 2 ) print ( line_length ) 3.8078865529319543 However, named tuples are still backwards compatible with normal tuples, so the following will still work: Point = namedtuple ( 'Point' , 'x y' ) pt1 = Point ( 1.0 , 5.0 ) pt2 = Point ( 2.5 , 1.5 ) from math import sqrt # use index referencing line_length = sqrt (( pt1 [ 0 ] - pt2 [ 0 ]) ** 2 + ( pt1 [ 1 ] - pt2 [ 1 ]) ** 2 ) print ( line_length ) # use tuple unpacking x1 , y1 = pt1 print ( x1 ) print ( y1 ) 3.8078865529319543 1.0 5.0 Thus, you should use named tuples instead of tuples anywhere you think object notation will make your code more pythonic and more easily readable. Use them to represent very simple value types, particularly when passing them as parameters to functions. It makes the functions more readable, without seeing the context of the tuple packing. NamedTuples, like dictionaries, contain keys that are hashed to a particular value. But on contrary, it supports both access from key-value and iteration, the functionality that dictionaries lack. # Python code to demonstrate namedtuple() from collections import namedtuple # Declaring namedtuple() Student = namedtuple ( 'Student' , [ 'name' , 'age' , 'DOB' ]) # Adding values S = Student ( 'Harry' , '12' , '31071980' ) # Access by index: The attribute values of namedtuple() # are ordered and can be accessed using the index number unlike dictionaries # which are not accessible by index. print ( \"The Student age using index is : \" , end = \"\" ) print ( S [ 1 ]) # Access using name : Access by keyname is also allowed as in dictionaries. print ( \"The Student name using keyname is : \" , end = \"\" ) print ( S . name ) # Access using getattr(): This is yet another way to access the value by # giving namedtuple and key value as its argument. print ( \"The Student DOB using getattr() is : \" , end = \"\" ) print ( getattr ( S , 'DOB' )) The Student age using index is : 12 The Student name using keyname is : Harry The Student DOB using getattr() is : 31071980 Conversion Operations _make() : This function is used to return a namedtuple() from the iterable passed as argument. _asdict() : This function returns the OrderedDict() as constructed from the mapped values of namedtuple(). using \u201c**\u201d (double star) operator : This function is used to convert a dictionary into the namedtuple(). # Python code to demonstrate namedtuple() and # _make(), _asdict() and \"**\" operator # importing \"collections\" for namedtuple() import collections # Declaring namedtuple() Student = collections . namedtuple ( 'Student' , [ 'name' , 'age' , 'DOB' ]) # Adding values S = Student ( 'Harry' , '12' , '31071980' ) # initializing iterable li = [ 'Ron' , '12' , '01031980' ] # initializing dict di = { 'name' : \"Hermione\" , 'age' : 12 , 'DOB' : '19091979' } # using _make() to return namedtuple() print ( \"The namedtuple instance using iterable is : \" ) print ( Student . _make ( li )) # using _asdict() to return an OrderedDict() print ( \"The OrderedDict instance using namedtuple is : \" ) print ( S . _asdict ()) # using ** operator to return namedtuple from dictionary print ( \"The namedtuple instance from dict is : \" ) print ( Student ( ** di )) The namedtuple instance using iterable is : Student(name='Ron', age='12', DOB='01031980') The OrderedDict instance using namedtuple is : OrderedDict([('name', 'Harry'), ('age', '12'), ('DOB', '31071980')]) The namedtuple instance from dict is : Student(name='Hermione', age=12, DOB='19091979') Additional Operation _fields : This function is used to return all the keynames of the namespace declared. _replace() : _replace() is like str.replace() but targets named fields( does not modify the original values) # Python code to demonstrate namedtuple() and # _fields and _replace() # importing \"collections\" for namedtuple() import collections # Declaring namedtuple() Student = collections . namedtuple ( 'Student' , [ 'name' , 'age' , 'DOB' ]) # Adding values S = Student ( 'Harry' , '12' , '31071980' ) # using _fields to display all the keynames of namedtuple() print ( \"All the fields of students are : \" ) print ( S . _fields ) # ._replace returns a new namedtuple, it does not modify the original print ( \"returns a new namedtuple : \" ) print ( S . _replace ( name = 'Ron' )) # original namedtuple print ( S ) All the fields of students are : ('name', 'age', 'DOB') returns a new namedtuple : Student(name='Ron', age='12', DOB='31071980') Student(name='Harry', age='12', DOB='31071980')","title":"NamedTuple"},{"location":"DS/pyNamedTuple/#conversion-operations","text":"_make() : This function is used to return a namedtuple() from the iterable passed as argument. _asdict() : This function returns the OrderedDict() as constructed from the mapped values of namedtuple(). using \u201c**\u201d (double star) operator : This function is used to convert a dictionary into the namedtuple(). # Python code to demonstrate namedtuple() and # _make(), _asdict() and \"**\" operator # importing \"collections\" for namedtuple() import collections # Declaring namedtuple() Student = collections . namedtuple ( 'Student' , [ 'name' , 'age' , 'DOB' ]) # Adding values S = Student ( 'Harry' , '12' , '31071980' ) # initializing iterable li = [ 'Ron' , '12' , '01031980' ] # initializing dict di = { 'name' : \"Hermione\" , 'age' : 12 , 'DOB' : '19091979' } # using _make() to return namedtuple() print ( \"The namedtuple instance using iterable is : \" ) print ( Student . _make ( li )) # using _asdict() to return an OrderedDict() print ( \"The OrderedDict instance using namedtuple is : \" ) print ( S . _asdict ()) # using ** operator to return namedtuple from dictionary print ( \"The namedtuple instance from dict is : \" ) print ( Student ( ** di )) The namedtuple instance using iterable is : Student(name='Ron', age='12', DOB='01031980') The OrderedDict instance using namedtuple is : OrderedDict([('name', 'Harry'), ('age', '12'), ('DOB', '31071980')]) The namedtuple instance from dict is : Student(name='Hermione', age=12, DOB='19091979')","title":"Conversion Operations"},{"location":"DS/pyNamedTuple/#additional-operation","text":"_fields : This function is used to return all the keynames of the namespace declared. _replace() : _replace() is like str.replace() but targets named fields( does not modify the original values) # Python code to demonstrate namedtuple() and # _fields and _replace() # importing \"collections\" for namedtuple() import collections # Declaring namedtuple() Student = collections . namedtuple ( 'Student' , [ 'name' , 'age' , 'DOB' ]) # Adding values S = Student ( 'Harry' , '12' , '31071980' ) # using _fields to display all the keynames of namedtuple() print ( \"All the fields of students are : \" ) print ( S . _fields ) # ._replace returns a new namedtuple, it does not modify the original print ( \"returns a new namedtuple : \" ) print ( S . _replace ( name = 'Ron' )) # original namedtuple print ( S ) All the fields of students are : ('name', 'age', 'DOB') returns a new namedtuple : Student(name='Ron', age='12', DOB='31071980') Student(name='Harry', age='12', DOB='31071980')","title":"Additional Operation"},{"location":"DS/pyOrdereddict/","text":"An OrderedDict is a dictionary subclass that remembers the order that keys were first inserted. The only difference between dict() and OrderedDict() is that: OrderedDict preserves the order in which the keys are inserted. A regular dict doesn\u2019t track the insertion order, and iterating it gives the values in an arbitrary order. By contrast, the order the items are inserted is remembered by OrderedDict. # A Python program to demonstrate working # of OrderedDict from collections import OrderedDict print ( \"This is a Dict: \\n \" ) d = {} d [ 'a' ] = 1 d [ 'b' ] = 2 d [ 'c' ] = 3 d [ 'd' ] = 4 for key , value in d . items (): print ( key , value ) print ( \" \\n This is an Ordered Dict: \\n \" ) od = OrderedDict () od [ 'a' ] = 1 od [ 'b' ] = 2 od [ 'c' ] = 3 od [ 'd' ] = 4 for key , value in od . items (): print ( key , value ) This is a Dict: a 1 b 2 c 3 d 4 This is an Ordered Dict: a 1 b 2 c 3 d 4 Key value Change If the value of a certain key is changed, the position of the key remains unchanged in OrderedDict. # A Python program to demonstrate working of key # value change in OrderedDict from collections import OrderedDict print ( \"Before: \\n \" ) od = OrderedDict () od [ 'a' ] = 1 od [ 'b' ] = 2 od [ 'c' ] = 3 od [ 'd' ] = 4 for key , value in od . items (): print ( key , value ) print ( \" \\n After: \\n \" ) od [ 'c' ] = 5 for key , value in od . items (): print ( key , value ) Before: a 1 b 2 c 3 d 4 After: a 1 b 2 c 5 d 4 Deletion and Re-Inserting Deleting and re-inserting the same key will push it to the back as OrderedDict, however, maintains the order of insertion. # A Python program to demonstrate working of deletion # re-insertion in OrderedDict from collections import OrderedDict print ( \"Before deleting: \\n \" ) od = OrderedDict () od [ 'a' ] = 1 od [ 'b' ] = 2 od [ 'c' ] = 3 od [ 'd' ] = 4 for key , value in od . items (): print ( key , value ) print ( \" \\n After deleting: \\n \" ) od . pop ( 'c' ) for key , value in od . items (): print ( key , value ) print ( \" \\n After re-inserting: \\n \" ) od [ 'c' ] = 3 for key , value in od . items (): print ( key , value ) Before deleting: a 1 b 2 c 3 d 4 After deleting: a 1 b 2 d 4 After re-inserting: a 1 b 2 d 4 c 3 from collections import OrderedDict def main (): # list of sport teams with wins and losses sportTeams = [( \"Royals\" , ( 18 , 12 )), ( \"Rockets\" , ( 24 , 6 )), ( \"Cardinals\" , ( 20 , 10 )), ( \"Dragons\" , ( 22 , 8 )), ( \"Kings\" , ( 15 , 15 )), ( \"Chargers\" , ( 20 , 10 )), ( \"Jets\" , ( 16 , 14 )), ( \"Warriors\" , ( 25 , 5 ))] # sort the teams by number of wins sortedTeams = sorted ( sportTeams , key = lambda t : t [ 1 ][ 0 ], reverse = True ) # create an ordered dictionary of the teams teams = OrderedDict ( sortedTeams ) print ( teams ) # Use popitem to remove the top item tm , wl = teams . popitem ( False ) print ( \"Top team: \" , tm , wl ) # What are next the top 4 teams? for i , team in enumerate ( teams , start = 1 ): print ( i , team ) if i == 4 : break # test for equality a = OrderedDict ({ \"a\" : 1 , \"b\" : 2 , \"c\" : 3 }) b = OrderedDict ({ \"a\" : 1 , \"c\" : 3 , \"b\" : 2 }) print ( \"Equality test: \" , a == b ) if __name__ == \"__main__\" : main () OrderedDict([('Warriors', (25, 5)), ('Rockets', (24, 6)), ('Dragons', (22, 8)), ('Cardinals', (20, 10)), ('Chargers', (20, 10)), ('Royals', (18, 12)), ('Jets', (16, 14)), ('Kings', (15, 15))]) Top team: Warriors (25, 5) 1 Rockets 2 Dragons 3 Cardinals 4 Chargers Equality test: False","title":"Ordereddict"},{"location":"DS/pyOrdereddict/#key-value-change","text":"If the value of a certain key is changed, the position of the key remains unchanged in OrderedDict. # A Python program to demonstrate working of key # value change in OrderedDict from collections import OrderedDict print ( \"Before: \\n \" ) od = OrderedDict () od [ 'a' ] = 1 od [ 'b' ] = 2 od [ 'c' ] = 3 od [ 'd' ] = 4 for key , value in od . items (): print ( key , value ) print ( \" \\n After: \\n \" ) od [ 'c' ] = 5 for key , value in od . items (): print ( key , value ) Before: a 1 b 2 c 3 d 4 After: a 1 b 2 c 5 d 4","title":"Key value Change"},{"location":"DS/pyOrdereddict/#deletion-and-re-inserting","text":"Deleting and re-inserting the same key will push it to the back as OrderedDict, however, maintains the order of insertion. # A Python program to demonstrate working of deletion # re-insertion in OrderedDict from collections import OrderedDict print ( \"Before deleting: \\n \" ) od = OrderedDict () od [ 'a' ] = 1 od [ 'b' ] = 2 od [ 'c' ] = 3 od [ 'd' ] = 4 for key , value in od . items (): print ( key , value ) print ( \" \\n After deleting: \\n \" ) od . pop ( 'c' ) for key , value in od . items (): print ( key , value ) print ( \" \\n After re-inserting: \\n \" ) od [ 'c' ] = 3 for key , value in od . items (): print ( key , value ) Before deleting: a 1 b 2 c 3 d 4 After deleting: a 1 b 2 d 4 After re-inserting: a 1 b 2 d 4 c 3 from collections import OrderedDict def main (): # list of sport teams with wins and losses sportTeams = [( \"Royals\" , ( 18 , 12 )), ( \"Rockets\" , ( 24 , 6 )), ( \"Cardinals\" , ( 20 , 10 )), ( \"Dragons\" , ( 22 , 8 )), ( \"Kings\" , ( 15 , 15 )), ( \"Chargers\" , ( 20 , 10 )), ( \"Jets\" , ( 16 , 14 )), ( \"Warriors\" , ( 25 , 5 ))] # sort the teams by number of wins sortedTeams = sorted ( sportTeams , key = lambda t : t [ 1 ][ 0 ], reverse = True ) # create an ordered dictionary of the teams teams = OrderedDict ( sortedTeams ) print ( teams ) # Use popitem to remove the top item tm , wl = teams . popitem ( False ) print ( \"Top team: \" , tm , wl ) # What are next the top 4 teams? for i , team in enumerate ( teams , start = 1 ): print ( i , team ) if i == 4 : break # test for equality a = OrderedDict ({ \"a\" : 1 , \"b\" : 2 , \"c\" : 3 }) b = OrderedDict ({ \"a\" : 1 , \"c\" : 3 , \"b\" : 2 }) print ( \"Equality test: \" , a == b ) if __name__ == \"__main__\" : main () OrderedDict([('Warriors', (25, 5)), ('Rockets', (24, 6)), ('Dragons', (22, 8)), ('Cardinals', (20, 10)), ('Chargers', (20, 10)), ('Royals', (18, 12)), ('Jets', (16, 14)), ('Kings', (15, 15))]) Top team: Warriors (25, 5) 1 Rockets 2 Dragons 3 Cardinals 4 Chargers Equality test: False","title":"Deletion and Re-Inserting"},{"location":"DS/pyQueue/","text":"A queue is similar to a stack, but defines a different way to add and remove elements. The elements are inserted from one end, called the rear, and deleted from the other end, called the front. This behavior is called FIFO (First in First Out). Terminology The process of adding new elements into the queue is called enqueue. The process of removal of an element from the queue is called dequeue. Applications Queues are used whenever we need to manage objects in order starting with the first one in. Scenarios include printing documents on a printer, call center systems answering people on hold, and so on. Queue in Python can be implemented by the following ways: list collections.deque queue.Queue Using List Python lists are the easiest way to implement a queue functionality. List is a Python\u2019s built-in data structure that can be used as a queue. Instead of enqueue() and dequeue(), append() and pop() function is used. However, lists are quite slow for this purpose because inserting or deleting an element at the beginning requires shifting all of the other elements by one, requiring O(n) time. # Python program to # demonstrate queue implementation # using list # Initializing a queue queue = [] # Adding elements to the queue queue . append ( 'a' ) queue . append ( 'b' ) queue . append ( 'c' ) print ( \"Initial queue\" ) print ( queue ) # Removing elements from the queue print ( \" \\n Elements dequeued from queue\" ) print ( queue . pop ( 0 )) print ( queue . pop ( 0 )) print ( queue . pop ( 0 )) print ( \" \\n Queue after removing elements\" ) print ( queue ) # Uncommenting print(queue.pop(0)) # will raise and IndexError # as the queue is now empty Initial queue ['a', 'b', 'c'] Elements dequeued from queue a b c Queue after removing elements [] Using collections.deque Queue in Python can be implemented using deque class from the collections module. Deque is preferred over list in the cases where we need quicker append and pop operations from both the ends of container, as deque provides an O(1) time complexity for append and pop operations as compared to list which provides O(n) time complexity. Instead of enqueue and deque, append() and popleft() functions are used. # Python program to # demonstrate queue implementation # using collections.dequeue from collections import deque # Initializing a queue q = deque () # Adding elements to a queue q . append ( 'a' ) q . append ( 'b' ) q . append ( 'c' ) print ( \"Initial queue\" ) print ( q ) # Removing elements from a queue print ( \" \\n Elements dequeued from the queue\" ) print ( q . popleft ()) print ( q . popleft ()) print ( q . popleft ()) print ( \" \\n Queue after removing elements\" ) print ( q ) # Uncommenting q.popleft() # will raise an IndexError # as queue is now empty Initial queue deque(['a', 'b', 'c']) Elements dequeued from the queue a b c Queue after removing elements deque([]) Using queue.Queue Queue is built-in module of Python which is used to implement a queue. queue.Queue(maxsize) initializes a variable to a maximum size of maxsize. A maxsize of zero \u20180\u2019 means a infinite queue. This Queue follows FIFO rule. There are various functions available in this module: maxsize \u2013 Number of items allowed in the queue. empty() \u2013 Return True if the queue is empty, False otherwise. full() \u2013 Return True if there are maxsize items in the queue. If the queue was initialized with maxsize=0 (the default), then full() never returns True. get() \u2013 Remove and return an item from the queue. If queue is empty, wait until an item is available. get_nowait() \u2013 Return an item if one is immediately available, else raise QueueEmpty. put(item) \u2013 Put an item into the queue. If the queue is full, wait until a free slot is available before adding the item. put_nowait(item) \u2013 Put an item into the queue without blocking. If no free slot is immediately available, raise QueueFull. qsize() \u2013 Return the number of items in the queue. # Python program to # demonstrate implementation of # queue using queue module from queue import Queue # Initializing a queue q = Queue ( maxsize = 3 ) # qsize() give the maxsize # of the Queue print ( q . qsize ()) # Adding of element to queue q . put ( 'a' ) q . put ( 'b' ) q . put ( 'c' ) # Return Boolean for Full # Queue print ( \" \\n Full: \" , q . full ()) # Removing element from queue print ( \" \\n Elements dequeued from the queue\" ) print ( q . get ()) print ( q . get ()) print ( q . get ()) # Return Boolean for Empty # Queue print ( \" \\n Empty: \" , q . empty ()) q . put ( 1 ) print ( \" \\n Empty: \" , q . empty ()) print ( \"Full: \" , q . full ()) # This would result into Infinite # Loop as the Queue is empty. # print(q.get()) 0 Full: True Elements dequeued from the queue a b c Empty: True Empty: False Full: False","title":"Queue"},{"location":"DS/pyQueue/#using-list","text":"Python lists are the easiest way to implement a queue functionality. List is a Python\u2019s built-in data structure that can be used as a queue. Instead of enqueue() and dequeue(), append() and pop() function is used. However, lists are quite slow for this purpose because inserting or deleting an element at the beginning requires shifting all of the other elements by one, requiring O(n) time. # Python program to # demonstrate queue implementation # using list # Initializing a queue queue = [] # Adding elements to the queue queue . append ( 'a' ) queue . append ( 'b' ) queue . append ( 'c' ) print ( \"Initial queue\" ) print ( queue ) # Removing elements from the queue print ( \" \\n Elements dequeued from queue\" ) print ( queue . pop ( 0 )) print ( queue . pop ( 0 )) print ( queue . pop ( 0 )) print ( \" \\n Queue after removing elements\" ) print ( queue ) # Uncommenting print(queue.pop(0)) # will raise and IndexError # as the queue is now empty Initial queue ['a', 'b', 'c'] Elements dequeued from queue a b c Queue after removing elements []","title":"Using List"},{"location":"DS/pyQueue/#using-collectionsdeque","text":"Queue in Python can be implemented using deque class from the collections module. Deque is preferred over list in the cases where we need quicker append and pop operations from both the ends of container, as deque provides an O(1) time complexity for append and pop operations as compared to list which provides O(n) time complexity. Instead of enqueue and deque, append() and popleft() functions are used. # Python program to # demonstrate queue implementation # using collections.dequeue from collections import deque # Initializing a queue q = deque () # Adding elements to a queue q . append ( 'a' ) q . append ( 'b' ) q . append ( 'c' ) print ( \"Initial queue\" ) print ( q ) # Removing elements from a queue print ( \" \\n Elements dequeued from the queue\" ) print ( q . popleft ()) print ( q . popleft ()) print ( q . popleft ()) print ( \" \\n Queue after removing elements\" ) print ( q ) # Uncommenting q.popleft() # will raise an IndexError # as queue is now empty Initial queue deque(['a', 'b', 'c']) Elements dequeued from the queue a b c Queue after removing elements deque([])","title":"Using collections.deque"},{"location":"DS/pyQueue/#using-queuequeue","text":"Queue is built-in module of Python which is used to implement a queue. queue.Queue(maxsize) initializes a variable to a maximum size of maxsize. A maxsize of zero \u20180\u2019 means a infinite queue. This Queue follows FIFO rule. There are various functions available in this module: maxsize \u2013 Number of items allowed in the queue. empty() \u2013 Return True if the queue is empty, False otherwise. full() \u2013 Return True if there are maxsize items in the queue. If the queue was initialized with maxsize=0 (the default), then full() never returns True. get() \u2013 Remove and return an item from the queue. If queue is empty, wait until an item is available. get_nowait() \u2013 Return an item if one is immediately available, else raise QueueEmpty. put(item) \u2013 Put an item into the queue. If the queue is full, wait until a free slot is available before adding the item. put_nowait(item) \u2013 Put an item into the queue without blocking. If no free slot is immediately available, raise QueueFull. qsize() \u2013 Return the number of items in the queue. # Python program to # demonstrate implementation of # queue using queue module from queue import Queue # Initializing a queue q = Queue ( maxsize = 3 ) # qsize() give the maxsize # of the Queue print ( q . qsize ()) # Adding of element to queue q . put ( 'a' ) q . put ( 'b' ) q . put ( 'c' ) # Return Boolean for Full # Queue print ( \" \\n Full: \" , q . full ()) # Removing element from queue print ( \" \\n Elements dequeued from the queue\" ) print ( q . get ()) print ( q . get ()) print ( q . get ()) # Return Boolean for Empty # Queue print ( \" \\n Empty: \" , q . empty ()) q . put ( 1 ) print ( \" \\n Empty: \" , q . empty ()) print ( \"Full: \" , q . full ()) # This would result into Infinite # Loop as the Queue is empty. # print(q.get()) 0 Full: True Elements dequeued from the queue a b c Empty: True Empty: False Full: False","title":"Using queue.Queue"},{"location":"DS/pySet/","text":"Sets are data structures, similar to lists or dictionaries. They are created using curly braces, or the set function. They share some functionality with lists, such as the use of in to check whether they contain a particular item. To create an empty set, you must use set(), as {} creates an empty dictionary. One small syntax nuisance: {1, 2, 3} is a set, but {} is an empty dictionary. empty_set = set () num_set = { 1 , 2 , 3 , 4 , 5 } word_set = set ([ \"spam\" , \"eggs\" , \"ham\" ]) print ( 3 in num_set ) print ( \"spam\" not in word_set ) True False Sets differ from lists in several ways, but share several list operations such as len. They are unordered, which means that they can't be indexed. They cannot contain duplicate elements. Due to the way they're stored, it's faster to check whether an item is part of a set, rather than part of a list. Instead of using append to add to a set, use add. The method remove removes a specific element from a set; pop removes an arbitrary element. Basic uses of sets include membership testing and the elimination of duplicate entries. nums = { 1 , 1 , 2 , 3 , 1 , 2 , 1 , 1 } print ( nums ) nums . add ( - 7 ) nums . remove ( 3 ) print ( nums ) {1, 2, 3} {1, 2, -7} Set Operations Sets can be combined using mathematical operations. The union operator | combines two sets to form a new one containing items in either. OR The intersection operator & gets items only in both. AND The difference operator - gets items in the first set but not in the second. The symmetric difference operator ^ gets items in either set, but not both. XOR The comparison operators check for subset and superset relationships. >= and <= f = { 3 , 2 , 9 , 4 , 5 , 6 } s = { 7 , 8 , 9 , 1 , 4 , 6 , 6 , 8 } print ( f | s ) #Union print ( f & s ) #Intersection print ( f - s ) # Difference print ( f ^ s ) #Sym Difference print ( f >= s ) #f superset of s {1, 2, 3, 4, 5, 6, 7, 8, 9} {9, 4, 6} {2, 3, 5} {1, 2, 3, 5, 7, 8} False Sets provide methods as well as operators. The argument you pass to a method can be any iterable, not just a set. And accept more than one argument f . issuperset ([ 1 , 2 , 3 ]) False f . union ([ 1 , 2 , 3 ], ( 3 , 4 , 5 ), { 5 , 6 , 7 }, { 7 : 'a' , 8 : 'b' }) {1, 2, 3, 4, 5, 6, 7, 8, 9} Union # Python3 program for union() function set1 = { 2 , 4 , 5 , 6 } set2 = { 4 , 6 , 7 , 8 } set3 = { 7 , 8 , 9 , 10 } # union of two sets print ( \"set1 U set2 : \" , set1 . union ( set2 )) # union of three sets print ( \"set1 U set2 U set3 :\" , set1 . union ( set2 , set3 )) set1 U set2 : {2, 4, 5, 6, 7, 8} set1 U set2 U set3 : {2, 4, 5, 6, 7, 8, 9, 10} Intersection # Python3 program for intersection() function set1 = { 2 , 4 , 5 , 6 } set2 = { 4 , 6 , 7 , 8 } set3 = { 4 , 6 , 8 } # union of two sets print ( \"set1 intersection set2 : \" , set1 . intersection ( set2 )) # union of three sets print ( \"set1 intersection set2 intersection set3 :\" , set1 . intersection ( set2 , set3 )) set1 intersection set2 : {4, 6} set1 intersection set2 intersection set3 : {4, 6} # Python3 program for intersection() function set1 = { 2 , 4 , 5 , 6 } set2 = { 4 , 6 , 7 , 8 } set3 = { 1 , 0 , 12 } print ( set1 & set2 ) print ( set1 & set3 ) print ( set1 & set2 & set3 ) {4, 6} set() set() Difference # Python code to get the difference between two sets # using difference() between set A and set B # Driver Code A = { 10 , 20 , 30 , 40 , 80 } B = { 100 , 30 , 80 , 40 , 60 } print ( A . difference ( B )) print ( B . difference ( A )) {10, 20} {100, 60}","title":"Set"},{"location":"DS/pySet/#set-operations","text":"Sets can be combined using mathematical operations. The union operator | combines two sets to form a new one containing items in either. OR The intersection operator & gets items only in both. AND The difference operator - gets items in the first set but not in the second. The symmetric difference operator ^ gets items in either set, but not both. XOR The comparison operators check for subset and superset relationships. >= and <= f = { 3 , 2 , 9 , 4 , 5 , 6 } s = { 7 , 8 , 9 , 1 , 4 , 6 , 6 , 8 } print ( f | s ) #Union print ( f & s ) #Intersection print ( f - s ) # Difference print ( f ^ s ) #Sym Difference print ( f >= s ) #f superset of s {1, 2, 3, 4, 5, 6, 7, 8, 9} {9, 4, 6} {2, 3, 5} {1, 2, 3, 5, 7, 8} False Sets provide methods as well as operators. The argument you pass to a method can be any iterable, not just a set. And accept more than one argument f . issuperset ([ 1 , 2 , 3 ]) False f . union ([ 1 , 2 , 3 ], ( 3 , 4 , 5 ), { 5 , 6 , 7 }, { 7 : 'a' , 8 : 'b' }) {1, 2, 3, 4, 5, 6, 7, 8, 9}","title":"Set Operations"},{"location":"DS/pySet/#union","text":"# Python3 program for union() function set1 = { 2 , 4 , 5 , 6 } set2 = { 4 , 6 , 7 , 8 } set3 = { 7 , 8 , 9 , 10 } # union of two sets print ( \"set1 U set2 : \" , set1 . union ( set2 )) # union of three sets print ( \"set1 U set2 U set3 :\" , set1 . union ( set2 , set3 )) set1 U set2 : {2, 4, 5, 6, 7, 8} set1 U set2 U set3 : {2, 4, 5, 6, 7, 8, 9, 10}","title":"Union"},{"location":"DS/pySet/#intersection","text":"# Python3 program for intersection() function set1 = { 2 , 4 , 5 , 6 } set2 = { 4 , 6 , 7 , 8 } set3 = { 4 , 6 , 8 } # union of two sets print ( \"set1 intersection set2 : \" , set1 . intersection ( set2 )) # union of three sets print ( \"set1 intersection set2 intersection set3 :\" , set1 . intersection ( set2 , set3 )) set1 intersection set2 : {4, 6} set1 intersection set2 intersection set3 : {4, 6} # Python3 program for intersection() function set1 = { 2 , 4 , 5 , 6 } set2 = { 4 , 6 , 7 , 8 } set3 = { 1 , 0 , 12 } print ( set1 & set2 ) print ( set1 & set3 ) print ( set1 & set2 & set3 ) {4, 6} set() set()","title":"Intersection"},{"location":"DS/pySet/#difference","text":"# Python code to get the difference between two sets # using difference() between set A and set B # Driver Code A = { 10 , 20 , 30 , 40 , 80 } B = { 100 , 30 , 80 , 40 , 60 } print ( A . difference ( B )) print ( B . difference ( A )) {10, 20} {100, 60}","title":"Difference"},{"location":"DS/pyStack/","text":"A stack is a simple data structure that adds and removes elements in a particular order. Every time an element is added, it goes on the \"top\" of the stack. Only an element at the top of the stack can be removed, just like a stack of plates. This behavior is called LIFO (Last In, First Out). Terminology Adding a new element onto the stack is called push. Removing an element from the stack is called pop. Applications Stacks can be used to create undo-redo functionalities, parsing expressions (infix to postfix/prefix conversion), and much more. There are various ways from which a stack can be implemented in Python. This article covers the implementation of a stack using data structures and modules from the Python library. Stack in Python can be implemented using the following ways: list Collections.deque queue.LifoQueue singly linked list using List Python\u2019s built-in data structure list can be used as a stack. Instead of push(), append() is used to add elements to the top of the stack while pop() removes the element in LIFO order. Unfortunately, the list has a few shortcomings. The biggest issue is that it can run into speed issues as it grows. The items in the list are stored next to each other in memory, if the stack grows bigger than the block of memory that currently holds it, then Python needs to do some memory allocations. This can lead to some append() calls taking much longer than other ones. # Python program to # demonstrate stack implementation # using list stack = [] # append() function to push # element in the stack stack . append ( 'a' ) stack . append ( 'b' ) stack . append ( 'c' ) print ( 'Initial stack' ) print ( stack ) # pop() function to pop # element from stack in # LIFO order print ( ' \\n Elements popped from stack:' ) print ( stack . pop ()) print ( stack . pop ()) print ( stack . pop ()) print ( ' \\n Stack after elements are popped:' ) print ( stack ) # uncommenting print(stack.pop()) # will cause an IndexError # as the stack is now empty Initial stack ['a', 'b', 'c'] Elements popped from stack: c b a Stack after elements are popped: [] using deque Python stack can be implemented using the deque class from the collections module. Deque is preferred over the list in the cases where we need quicker append and pop operations from both the ends of the container, as deque provides an O(1) time complexity for append and pop operations as compared to list which provides O(n) time complexity. The same methods on deque as seen in the list are used, append() and pop(). # Python program to # demonstrate stack implementation # using collections.deque from collections import deque stack = deque () # append() function to push # element in the stack stack . append ( 'a' ) stack . append ( 'b' ) stack . append ( 'c' ) print ( 'Initial stack:' ) print ( stack ) # pop() function to pop # element from stack in # LIFO order print ( ' \\n Elements popped from stack:' ) print ( stack . pop ()) print ( stack . pop ()) print ( stack . pop ()) print ( ' \\n Stack after elements are popped:' ) print ( stack ) # uncommenting print(stack.pop()) # will cause an IndexError # as the stack is now empty Initial stack: deque(['a', 'b', 'c']) Elements popped from stack: c b a Stack after elements are popped: deque([]) using queue module Queue module also has a LIFO Queue, which is basically a Stack. Data is inserted into Queue using the put() function and get() takes data out from the Queue. There are various functions available in this module: maxsize \u2013 Number of items allowed in the queue. empty() \u2013 Return True if the queue is empty, False otherwise. full() \u2013 Return True if there are maxsize items in the queue. If the queue was initialized with maxsize=0 (the default), then full() never returns True. get() \u2013 Remove and return an item from the queue. If the queue is empty, wait until an item is available. get_nowait() \u2013 Return an item if one is immediately available, else raise QueueEmpty. put(item) \u2013 Put an item into the queue. If the queue is full, wait until a free slot is available before adding the item. put_nowait(item) \u2013 Put an item into the queue without blocking. qsize() \u2013 Return the number of items in the queue. If no free slot is immediately available, raise QueueFull. # Python program to # demonstrate stack implementation # using queue module from queue import LifoQueue # Initializing a stack stack = LifoQueue ( maxsize = 3 ) # qsize() show the number of elements # in the stack print ( stack . qsize ()) # put() function to push # element in the stack stack . put ( 'a' ) stack . put ( 'b' ) stack . put ( 'c' ) print ( \"Full: \" , stack . full ()) print ( \"Size: \" , stack . qsize ()) # get() function to pop # element from stack in # LIFO order print ( ' \\n Elements popped from the stack' ) print ( stack . get ()) print ( stack . get ()) print ( stack . get ()) print ( \" \\n Empty: \" , stack . empty ()) 0 Full: True Size: 3 Elements popped from the stack c b a Empty: True using singly linked list The linked list has two methods addHead(item) and removeHead() that run in constant time. These two methods are suitable to implement a stack. getSize()\u2013 Get the number of items in the stack. isEmpty() \u2013 Return True if the stack is empty, False otherwise. peek() \u2013 Return the top item in the stack. If the stack is empty, raise an exception. push(value) \u2013 Push a value into the head of the stack. pop() \u2013 Remove and return a value in the head of the stack. If the stack is empty, raise an exception. # Python program to demonstrate # stack implementation using a linked list. # node class class Node : def __init__ ( self , value ): self . value = value self . next = None class Stack : # Initializing a stack. # Use a dummy node, which is # easier for handling edge cases. def __init__ ( self ): self . head = Node ( \"head\" ) self . size = 0 # String representation of the stack def __str__ ( self ): cur = self . head . next out = \"\" while cur : out += str ( cur . value ) + \"->\" cur = cur . next return out [: - 3 ] # Get the current size of the stack def getSize ( self ): return self . size # Check if the stack is empty def isEmpty ( self ): return self . size == 0 # Get the top item of the stack def peek ( self ): # Sanitary check to see if we # are peeking an empty stack. if self . isEmpty (): raise Exception ( \"Peeking from an empty stack\" ) return self . head . next . value # Push a value into the stack. def push ( self , value ): node = Node ( value ) node . next = self . head . next self . head . next = node self . size += 1 # Remove a value from the stack and return. def pop ( self ): if self . isEmpty (): raise Exception ( \"Popping from an empty stack\" ) remove = self . head . next self . head . next = self . head . next . next self . size -= 1 return remove . value # Driver Code if __name__ == \"__main__\" : stack = Stack () for i in range ( 1 , 11 ): stack . push ( i ) print ( f \"Stack: { stack } \" ) for _ in range ( 1 , 6 ): remove = stack . pop () print ( f \"Pop: { remove } \" ) print ( f \"Stack: { stack } \" ) Stack: 10->9->8->7->6->5->4->3->2-> Pop: 10 Pop: 9 Pop: 8 Pop: 7 Pop: 6 Stack: 5->4->3->2->","title":"Stacks"},{"location":"DS/pyStack/#using-list","text":"Python\u2019s built-in data structure list can be used as a stack. Instead of push(), append() is used to add elements to the top of the stack while pop() removes the element in LIFO order. Unfortunately, the list has a few shortcomings. The biggest issue is that it can run into speed issues as it grows. The items in the list are stored next to each other in memory, if the stack grows bigger than the block of memory that currently holds it, then Python needs to do some memory allocations. This can lead to some append() calls taking much longer than other ones. # Python program to # demonstrate stack implementation # using list stack = [] # append() function to push # element in the stack stack . append ( 'a' ) stack . append ( 'b' ) stack . append ( 'c' ) print ( 'Initial stack' ) print ( stack ) # pop() function to pop # element from stack in # LIFO order print ( ' \\n Elements popped from stack:' ) print ( stack . pop ()) print ( stack . pop ()) print ( stack . pop ()) print ( ' \\n Stack after elements are popped:' ) print ( stack ) # uncommenting print(stack.pop()) # will cause an IndexError # as the stack is now empty Initial stack ['a', 'b', 'c'] Elements popped from stack: c b a Stack after elements are popped: []","title":"using List"},{"location":"DS/pyStack/#using-deque","text":"Python stack can be implemented using the deque class from the collections module. Deque is preferred over the list in the cases where we need quicker append and pop operations from both the ends of the container, as deque provides an O(1) time complexity for append and pop operations as compared to list which provides O(n) time complexity. The same methods on deque as seen in the list are used, append() and pop(). # Python program to # demonstrate stack implementation # using collections.deque from collections import deque stack = deque () # append() function to push # element in the stack stack . append ( 'a' ) stack . append ( 'b' ) stack . append ( 'c' ) print ( 'Initial stack:' ) print ( stack ) # pop() function to pop # element from stack in # LIFO order print ( ' \\n Elements popped from stack:' ) print ( stack . pop ()) print ( stack . pop ()) print ( stack . pop ()) print ( ' \\n Stack after elements are popped:' ) print ( stack ) # uncommenting print(stack.pop()) # will cause an IndexError # as the stack is now empty Initial stack: deque(['a', 'b', 'c']) Elements popped from stack: c b a Stack after elements are popped: deque([])","title":"using deque"},{"location":"DS/pyStack/#using-queue-module","text":"Queue module also has a LIFO Queue, which is basically a Stack. Data is inserted into Queue using the put() function and get() takes data out from the Queue. There are various functions available in this module: maxsize \u2013 Number of items allowed in the queue. empty() \u2013 Return True if the queue is empty, False otherwise. full() \u2013 Return True if there are maxsize items in the queue. If the queue was initialized with maxsize=0 (the default), then full() never returns True. get() \u2013 Remove and return an item from the queue. If the queue is empty, wait until an item is available. get_nowait() \u2013 Return an item if one is immediately available, else raise QueueEmpty. put(item) \u2013 Put an item into the queue. If the queue is full, wait until a free slot is available before adding the item. put_nowait(item) \u2013 Put an item into the queue without blocking. qsize() \u2013 Return the number of items in the queue. If no free slot is immediately available, raise QueueFull. # Python program to # demonstrate stack implementation # using queue module from queue import LifoQueue # Initializing a stack stack = LifoQueue ( maxsize = 3 ) # qsize() show the number of elements # in the stack print ( stack . qsize ()) # put() function to push # element in the stack stack . put ( 'a' ) stack . put ( 'b' ) stack . put ( 'c' ) print ( \"Full: \" , stack . full ()) print ( \"Size: \" , stack . qsize ()) # get() function to pop # element from stack in # LIFO order print ( ' \\n Elements popped from the stack' ) print ( stack . get ()) print ( stack . get ()) print ( stack . get ()) print ( \" \\n Empty: \" , stack . empty ()) 0 Full: True Size: 3 Elements popped from the stack c b a Empty: True","title":"using queue module"},{"location":"DS/pyStack/#using-singly-linked-list","text":"The linked list has two methods addHead(item) and removeHead() that run in constant time. These two methods are suitable to implement a stack. getSize()\u2013 Get the number of items in the stack. isEmpty() \u2013 Return True if the stack is empty, False otherwise. peek() \u2013 Return the top item in the stack. If the stack is empty, raise an exception. push(value) \u2013 Push a value into the head of the stack. pop() \u2013 Remove and return a value in the head of the stack. If the stack is empty, raise an exception. # Python program to demonstrate # stack implementation using a linked list. # node class class Node : def __init__ ( self , value ): self . value = value self . next = None class Stack : # Initializing a stack. # Use a dummy node, which is # easier for handling edge cases. def __init__ ( self ): self . head = Node ( \"head\" ) self . size = 0 # String representation of the stack def __str__ ( self ): cur = self . head . next out = \"\" while cur : out += str ( cur . value ) + \"->\" cur = cur . next return out [: - 3 ] # Get the current size of the stack def getSize ( self ): return self . size # Check if the stack is empty def isEmpty ( self ): return self . size == 0 # Get the top item of the stack def peek ( self ): # Sanitary check to see if we # are peeking an empty stack. if self . isEmpty (): raise Exception ( \"Peeking from an empty stack\" ) return self . head . next . value # Push a value into the stack. def push ( self , value ): node = Node ( value ) node . next = self . head . next self . head . next = node self . size += 1 # Remove a value from the stack and return. def pop ( self ): if self . isEmpty (): raise Exception ( \"Popping from an empty stack\" ) remove = self . head . next self . head . next = self . head . next . next self . size -= 1 return remove . value # Driver Code if __name__ == \"__main__\" : stack = Stack () for i in range ( 1 , 11 ): stack . push ( i ) print ( f \"Stack: { stack } \" ) for _ in range ( 1 , 6 ): remove = stack . pop () print ( f \"Pop: { remove } \" ) print ( f \"Stack: { stack } \" ) Stack: 10->9->8->7->6->5->4->3->2-> Pop: 10 Pop: 9 Pop: 8 Pop: 7 Pop: 6 Stack: 5->4->3->2->","title":"using singly linked list"},{"location":"DS/pyTrees/","text":"Unlike Arrays, Linked Lists, Stack and Queues, which are linear data structures, trees are hierarchical data structures. Tree Vocabulary: Root The topmost node is called root of the tree. Children The elements that are directly under an element are called its children. Parent The element directly above something is called its parent. For example, \u2018a\u2019 is a child of \u2018f\u2019, and \u2018f\u2019 is the parent of \u2018a\u2019. Leaf Finally, elements with no children are called leaves. Main applications of trees include: Manipulate hierarchical data. Make information easy to search (see tree traversal). Manipulate sorted lists of data. As a workflow for compositing digital images for visual effects. Router algorithms Form of a multi-stage decision-making # Python program to introduce Binary Tree # A class that represents an individual node in a # Binary Tree class Node : def __init__ ( self , key ): self . left = None self . right = None self . val = key # create root root = Node ( 1 ) ''' following is the tree after above statement 1 / \\ None None''' root . left = Node ( 2 ); root . right = Node ( 3 ); ''' 2 and 3 become left and right children of 1 1 / \\ 2 3 / \\ / \\ None None None None''' root . left . left = Node ( 4 ); '''4 becomes left child of 2 1 / \\ 2 3 / \\ / \\ 4 None None None / \\ None None'''","title":"Trees"},{"location":"DS/pyTrie/","text":"Trie is a tree-like data structure made up of nodes. Nodes can be used to store data. Each node may have none, one or more children. When used to store a vocabulary, each node is used to store a character, and consequently each \"branch\" of the trie represents a unique word. The following figure shows a trie with five words (was, wax, what, word, work) stored in it. There are two major operations that can be performed on a trie, namely: Inserting a word into the Trie Searching for words using a prefix Inserting Words into the Trie In order to insert a new word into the trie, we need to first check whether any prefix of the word is already in the trie. Therefore, we will start traverse the trie from the root node, and follow the algorithm below: Set the current node to be the root node Set the current character as the first character of the input word Check if the current character is a child of the current node If yes, set the current node to be this child node, set the current character to the next character in the input word, and perform this step again If no, it means from this character onwards, we will need to create new nodes and insert them into the trie Below is an illustration of what will happen when we want to add the word won into the trie above. Following the steps in the algorithm mentioned above, we will arrive at the node o under w, at which point we discover that n is not a child of o, and therefore we create a new node for the character n, and insert it under o. Searching in the Trie A common application scenario of the trie data structure is to search for words with a certain prefix, just like the auto-complete or query suggestion function in a search bar. When given a prefix, we can traverse the trie to check if any word in the trie starts with that prefix. If the prefix is found in the trie, we can then use depth-first traversal to retrieve all the words with that prefix. For example, given the trie illustrated above, which contains the words was, wax, what, word, work and won, let's see what will happen if we want to search for words with the prefix wa: Starting from the root node, we are able to find the node w and a From the node a, we can go on to traverse the trie to retrieve all words starting with the prefix wa When we arrive at the node s, we check whether it is the end of a word (yes), and the word was was output Similarity, when we arrive at the node x, the word wax is output Implementation class TrieNode : \"\"\"A node in the trie structure\"\"\" def __init__ ( self , char ): # the character stored in this node self . char = char # whether this can be the end of a word self . is_end = False # a counter indicating how many times a word is inserted # (if this node's is_end is True) self . counter = 0 # a dictionary of child nodes # keys are characters, values are nodes self . children = {} In this implementation, we want to store also the number of times a word has been inserted into the trie. This allows us to support additional features, such as ranking the words by their popularity. class Trie ( object ): \"\"\"The trie object\"\"\" def __init__ ( self ): \"\"\" The trie has at least the root node. The root node does not store any character \"\"\" self . root = TrieNode ( \"\" ) def insert ( self , word ): \"\"\"Insert a word into the trie\"\"\" node = self . root # Loop through each character in the word # Check if there is no child containing the character, create a new child for the current node for char in word : if char in node . children : node = node . children [ char ] else : # If a character is not found, # create a new node in the trie new_node = TrieNode ( char ) node . children [ char ] = new_node node = new_node # Mark the end of a word node . is_end = True # Increment the counter to indicate that we see this word once more node . counter += 1 def dfs ( self , node , prefix ): \"\"\"Depth-first traversal of the trie Args: - node: the node to start with - prefix: the current prefix, for tracing a word while traversing the trie \"\"\" if node . is_end : self . output . append (( prefix + node . char , node . counter )) for child in node . children . values (): self . dfs ( child , prefix + node . char ) def query ( self , x ): \"\"\"Given an input (a prefix), retrieve all words stored in the trie with that prefix, sort the words by the number of times they have been inserted \"\"\" # Use a variable within the class to keep all possible outputs # As there can be more than one word with such prefix self . output = [] node = self . root # Check if the prefix is in the trie for char in x : if char in node . children : node = node . children [ char ] else : # cannot found the prefix, return empty list return [] # Traverse the trie to get all candidates self . dfs ( node , x [: - 1 ]) # Sort the results in reverse order and return return sorted ( self . output , key = lambda x : x [ 1 ], reverse = True ) Below is an example of how this Trie class can be used: t = Trie () t . insert ( \"was\" ) t . insert ( \"word\" ) t . insert ( \"war\" ) t . insert ( \"what\" ) t . insert ( \"where\" ) t . query ( \"wh\" ) [('what', 1), ('where', 1)] using DefaultDict _end = '_end_' def make_trie ( * words ): root = dict () for word in words : current_dict = root for letter in word : current_dict = current_dict . setdefault ( letter , {}) current_dict [ _end ] = _end return root make_trie ( 'foo' , 'bar' , 'baz' , 'barz' ) {'b': {'a': {'r': {'_end_': '_end_', 'z': {'_end_': '_end_'}}, 'z': {'_end_': '_end_'}}}, 'f': {'o': {'o': {'_end_': '_end_'}}}} setdefault() simply looks up a key in the dictionary (here, letter or _end). If the key is present, it returns the associated value; if not, it assigns a default value to that key and returns the value ({} or _end). (It's like a version of get that also updates the dictionary.) def in_trie ( trie , word ): current_dict = trie for letter in word : if letter not in current_dict : return False current_dict = current_dict [ letter ] return _end in current_dict in_trie ( make_trie ( 'foo' , 'bar' , 'baz' , 'barz' ), 'baz' ) True in_trie ( make_trie ( 'foo' , 'bar' , 'baz' , 'barz' ), 'barzz' ) False","title":"Trie"},{"location":"DS/pyTrie/#inserting-words-into-the-trie","text":"In order to insert a new word into the trie, we need to first check whether any prefix of the word is already in the trie. Therefore, we will start traverse the trie from the root node, and follow the algorithm below: Set the current node to be the root node Set the current character as the first character of the input word Check if the current character is a child of the current node If yes, set the current node to be this child node, set the current character to the next character in the input word, and perform this step again If no, it means from this character onwards, we will need to create new nodes and insert them into the trie Below is an illustration of what will happen when we want to add the word won into the trie above. Following the steps in the algorithm mentioned above, we will arrive at the node o under w, at which point we discover that n is not a child of o, and therefore we create a new node for the character n, and insert it under o.","title":"Inserting Words into the Trie"},{"location":"DS/pyTrie/#searching-in-the-trie","text":"A common application scenario of the trie data structure is to search for words with a certain prefix, just like the auto-complete or query suggestion function in a search bar. When given a prefix, we can traverse the trie to check if any word in the trie starts with that prefix. If the prefix is found in the trie, we can then use depth-first traversal to retrieve all the words with that prefix. For example, given the trie illustrated above, which contains the words was, wax, what, word, work and won, let's see what will happen if we want to search for words with the prefix wa: Starting from the root node, we are able to find the node w and a From the node a, we can go on to traverse the trie to retrieve all words starting with the prefix wa When we arrive at the node s, we check whether it is the end of a word (yes), and the word was was output Similarity, when we arrive at the node x, the word wax is output","title":"Searching in the Trie"},{"location":"DS/pyTrie/#implementation","text":"class TrieNode : \"\"\"A node in the trie structure\"\"\" def __init__ ( self , char ): # the character stored in this node self . char = char # whether this can be the end of a word self . is_end = False # a counter indicating how many times a word is inserted # (if this node's is_end is True) self . counter = 0 # a dictionary of child nodes # keys are characters, values are nodes self . children = {} In this implementation, we want to store also the number of times a word has been inserted into the trie. This allows us to support additional features, such as ranking the words by their popularity. class Trie ( object ): \"\"\"The trie object\"\"\" def __init__ ( self ): \"\"\" The trie has at least the root node. The root node does not store any character \"\"\" self . root = TrieNode ( \"\" ) def insert ( self , word ): \"\"\"Insert a word into the trie\"\"\" node = self . root # Loop through each character in the word # Check if there is no child containing the character, create a new child for the current node for char in word : if char in node . children : node = node . children [ char ] else : # If a character is not found, # create a new node in the trie new_node = TrieNode ( char ) node . children [ char ] = new_node node = new_node # Mark the end of a word node . is_end = True # Increment the counter to indicate that we see this word once more node . counter += 1 def dfs ( self , node , prefix ): \"\"\"Depth-first traversal of the trie Args: - node: the node to start with - prefix: the current prefix, for tracing a word while traversing the trie \"\"\" if node . is_end : self . output . append (( prefix + node . char , node . counter )) for child in node . children . values (): self . dfs ( child , prefix + node . char ) def query ( self , x ): \"\"\"Given an input (a prefix), retrieve all words stored in the trie with that prefix, sort the words by the number of times they have been inserted \"\"\" # Use a variable within the class to keep all possible outputs # As there can be more than one word with such prefix self . output = [] node = self . root # Check if the prefix is in the trie for char in x : if char in node . children : node = node . children [ char ] else : # cannot found the prefix, return empty list return [] # Traverse the trie to get all candidates self . dfs ( node , x [: - 1 ]) # Sort the results in reverse order and return return sorted ( self . output , key = lambda x : x [ 1 ], reverse = True ) Below is an example of how this Trie class can be used: t = Trie () t . insert ( \"was\" ) t . insert ( \"word\" ) t . insert ( \"war\" ) t . insert ( \"what\" ) t . insert ( \"where\" ) t . query ( \"wh\" ) [('what', 1), ('where', 1)]","title":"Implementation"},{"location":"DS/pyTrie/#using-defaultdict","text":"_end = '_end_' def make_trie ( * words ): root = dict () for word in words : current_dict = root for letter in word : current_dict = current_dict . setdefault ( letter , {}) current_dict [ _end ] = _end return root make_trie ( 'foo' , 'bar' , 'baz' , 'barz' ) {'b': {'a': {'r': {'_end_': '_end_', 'z': {'_end_': '_end_'}}, 'z': {'_end_': '_end_'}}}, 'f': {'o': {'o': {'_end_': '_end_'}}}} setdefault() simply looks up a key in the dictionary (here, letter or _end). If the key is present, it returns the associated value; if not, it assigns a default value to that key and returns the value ({} or _end). (It's like a version of get that also updates the dictionary.) def in_trie ( trie , word ): current_dict = trie for letter in word : if letter not in current_dict : return False current_dict = current_dict [ letter ] return _end in current_dict in_trie ( make_trie ( 'foo' , 'bar' , 'baz' , 'barz' ), 'baz' ) True in_trie ( make_trie ( 'foo' , 'bar' , 'baz' , 'barz' ), 'barzz' ) False","title":"using DefaultDict"},{"location":"DS/pyTuple/","text":"A tuple is a Python collection that is extremely similar to a list, with some subtle differences. For starters, tuples are indicated using parentheses instead of square brackets. Like lists and dictionaries, tuples can be nested within each other. x = 1 y = 2 coordinates = ( x , y ) The variable coordinates above is a tuple containing the variables x and y. This example was chosen to also demonstrate a difference between the typical usage of tuples versus lists. Whereas lists are frequently used to contain objects whose values are similar in some sense, tuples are frequently used to contain attributes of a coherent unit. For instance, as above, it makes sense to treat the coordinates of a point as a single unit. As another example, consider the following tuple and list concerning dates: year1 = 2011 month1 = \"May\" day1 = 18 date1 = ( month1 , day1 , year1 ) year2 = 2017 month2 = \"June\" day2 = 13 date2 = ( month2 , day2 , year2 ) years_list = [ year1 , year2 ] Notice above that we have collected the attributes of a single date into one tuple: those pieces of information all describe a single \"unit\". By contrast, in the years list we have collected the different years. In the code-snippet: the values in the list have a commonality (they are both years), but they do not describe the same unit. The distinction drawn between tuples and lists is one that many Python programmers recognize in practice, but not one that is strictly enforced (i.e., you won't get any errors if you break this convention!). Another subtle way in which tuples and lists differ involves what is called mutability of Python variables. Mutability refers to the fact that a mutable object can be changed after it is created, and an immutable object can\u2019t. Tuples are not mutable, lists are mutable. Tuples are very similar to lists, except that they are immutable (they cannot be changed). Trying to reassign a value in a tuple causes a TypeError. Objects of built-in types like (int, float, bool, str, tuple, frozenset, unicode) are immutable. Objects of built-in types like (list, set, dict, byte array) are mutable. Custom classes are generally mutable. To simulate immutability in a class, one should override attribute setting and deletion to raise exceptions. To read further. words = ( \"spam\" , \"eggs\" , \"sausages\" ,) print ( words [ 0 ]) words [ 1 ] = \"cheese\" spam --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-85-a8af92c7d33f> in <module>() 1 words = (\"spam\", \"eggs\", \"sausages\",) 2 print(words[0]) ----> 3 words[1]=\"cheese\" TypeError: 'tuple' object does not support item assignment Tuples can be created without the parentheses, by just separating the values with commas. An empty tuple is created using an empty parenthesis pair. Tuples are faster than lists, but they cannot be changed. Slicing can also be done on tuples. my_tp = () tp = \"one\" , \"two\" , \"three\" print ( tp [ 0 ]) print ( len ( my_tp )) print ( tp [ 1 :]) one 0 ('two', 'three') Tuple unpacking Tuple unpacking allows you to assign each item in an iterable (often a tuple) to a variable. This can be also used to swap variables by doing a, b = b, a , since b, a on the right hand side forms the tuple (b, a) which is then unpacked. A variable that is prefaced with an asterisk (*) takes all values from the iterable that are left over from the other variables. numbers = ( 1 , 2 , 3 ) a , b , c = numbers #unpacking print ( a ) print ( b ) print ( c ) 1 2 3 a , b , * c , d = ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ) print ( a ) print ( b ) print ( c ) print ( d ) 1 2 [3, 4, 5, 6, 7, 8] 9","title":"Tuple"},{"location":"DS/pyTuple/#tuple-unpacking","text":"Tuple unpacking allows you to assign each item in an iterable (often a tuple) to a variable. This can be also used to swap variables by doing a, b = b, a , since b, a on the right hand side forms the tuple (b, a) which is then unpacked. A variable that is prefaced with an asterisk (*) takes all values from the iterable that are left over from the other variables. numbers = ( 1 , 2 , 3 ) a , b , c = numbers #unpacking print ( a ) print ( b ) print ( c ) 1 2 3 a , b , * c , d = ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ) print ( a ) print ( b ) print ( c ) print ( d ) 1 2 [3, 4, 5, 6, 7, 8] 9","title":"Tuple unpacking"},{"location":"DS/pyUserDefGen/","text":"Linear data structures Lists Stacks Queue and Deque LinkedLists Non-linear data structures Mapping data structures Set Counter Hashtables and Hashmaps Tree data structures Trees (BST) Priority Queue Trie Graph data structures Dictionary of dictionaries Adjacent matrix","title":"General"},{"location":"DS/pyUserDefGen/#linear-data-structures","text":"Lists Stacks Queue and Deque LinkedLists","title":"Linear data structures"},{"location":"DS/pyUserDefGen/#non-linear-data-structures","text":"","title":"Non-linear data structures"},{"location":"DS/pyUserDefGen/#mapping-data-structures","text":"Set Counter Hashtables and Hashmaps","title":"Mapping data structures"},{"location":"DS/pyUserDefGen/#tree-data-structures","text":"Trees (BST) Priority Queue Trie","title":"Tree data structures"},{"location":"DS/pyUserDefGen/#graph-data-structures","text":"Dictionary of dictionaries Adjacent matrix","title":"Graph data structures"},{"location":"DS/pydeque/","text":"The collection Module in Python provides different types of containers. A Container is an object that is used to store different objects and provide a way to access the contained objects and iterate over them. Some of the built-in containers are Tuple, List, Dictionary, etc. import collections Deque (Doubly Ended Queue) in Python is implemented using the module \u201ccollections\u201c. Deque is preferred over list in the cases where we need quicker append and pop operations from both the ends of container, as deque provides an O(1) time complexity for append and pop operations as compared to list which provides O(n) time complexity. With Python lists, we can add and remove elements from the end of the list in constant time, but adding and removing from the beginning takes linear time. That\u2019s because Python lists are implemented using arrays that grow dynamically. With linked lists, we can add and remove elements from the beginning of the list in constant time, but adding and removing from the end takes linear time. With either of these implementations, it is easy to make a stack, that is, a collection where the first element we add is the last element we remove. A stack is also called a \u201cfirst-in, last-out\u201d queue, abbreviated FILO. But it is not easy to implement a \u201cfirst-in, first-out\u201d queue, that is, a collection where the first element we add is the first element we remove. Fortunately, there are ways to implement lists that can add and remove elements from both ends in constant time. A collection that has this property is called a double-ended queue, abbreviated \u201cdeque\u201d and pronounced like \u201cdeck\u201d. One way to implement a deque is a doubly-linked list, also known as a \u201chead-tail linked list\u201d. Each node in a doubly-linked list has a reference to the previous node in the list as well as the next element, which I will call left and right. Constructor dq = collections . deque ( range ( 10 )) dq deque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) append() & popleft() for i in range ( 10 , 15 ): dq . append ( i ) v = dq . popleft () print ( \"Inserted :\" , i , \"- popped: \" , v , \"-result:\" , dq ) Inserted : 10 - popped: 0 -result: deque([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) Inserted : 11 - popped: 1 -result: deque([2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) Inserted : 12 - popped: 2 -result: deque([3, 4, 5, 6, 7, 8, 9, 10, 11, 12]) Inserted : 13 - popped: 3 -result: deque([4, 5, 6, 7, 8, 9, 10, 11, 12, 13]) Inserted : 14 - popped: 4 -result: deque([5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) appendleft() & pop() for i in reversed ( range ( 0 , 5 )): dq . appendleft ( i ) v = dq . pop () print ( \"Inserted :\" , i , \"- popped: \" , v , \"-result:\" , dq ) Inserted : 4 - popped: 14 -result: deque([4, 5, 6, 7, 8, 9, 10, 11, 12, 13]) Inserted : 3 - popped: 13 -result: deque([3, 4, 5, 6, 7, 8, 9, 10, 11, 12]) Inserted : 2 - popped: 12 -result: deque([2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) Inserted : 1 - popped: 11 -result: deque([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) Inserted : 0 - popped: 10 -result: deque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) index(ele, beg, end) This function returns the first index of the value mentioned in arguments, starting searching from beg till end index. insert(i, a) This function inserts the value mentioned in arguments(a) at index(i) specified in arguments. remove() This function removes the first occurrence of value mentioned in arguments. count() This function counts the number of occurrences of value mentioned in arguments. # Python code to demonstrate working of # insert(), index(), remove(), count() # importing \"collections\" for deque operations import collections # initializing deque de = collections . deque ([ 1 , 2 , 3 , 3 , 4 , 2 , 4 ]) # using index() to print the first occurrence of 4 print ( \"The number 4 first occurs at a position : \" ) print ( de . index ( 4 , 2 , 5 )) # using insert() to insert the value 3 at 5th position de . insert ( 4 , 3 ) # printing modified deque print ( \"The deque after inserting 3 at 5th position is : \" ) print ( de ) # using count() to count the occurrences of 3 print ( \"The count of 3 in deque is : \" ) print ( de . count ( 3 )) # using remove() to remove the first occurrence of 3 de . remove ( 3 ) # printing modified deque print ( \"The deque after deleting first occurrence of 3 is : \" ) print ( de ) The number 4 first occurs at a position : 4 The deque after inserting 3 at 5th position is : deque([1, 2, 3, 3, 3, 4, 2, 4]) The count of 3 in deque is : 3 The deque after deleting first occurrence of 3 is : deque([1, 2, 3, 3, 4, 2, 4]) extend(iterable) This function is used to add multiple values at the right end of deque. The argument passed is an iterable. extendleft(iterable) This function is used to add multiple values at the left end of deque. The argument passed is an iterable. Order is reversed as a result of left appends. reverse() This function is used to reverse order of deque elements. rotate() This function rotates the deque by the number specified in arguments. If the number specified is negative, rotation occurs to left. Else rotation is to right. # Python code to demonstrate working of # extend(), extendleft(), rotate(), reverse() # importing \"collections\" for deque operations import collections # initializing deque de = collections . deque ([ 1 , 2 , 3 ,]) # using extend() to add numbers to right end # adds 4,5,6 to right end de . extend ([ 4 , 5 , 6 ]) # printing modified deque print ( \"The deque after extending deque at end is : \" ) print ( de ) # using extendleft() to add numbers to left end # adds 7,8,9 to right end de . extendleft ([ 7 , 8 , 9 ]) # printing modified deque print ( \"The deque after extending deque at beginning is : \" ) print ( de ) # using rotate() to rotate the deque # rotates by 3 to left de . rotate ( - 3 ) # printing modified deque print ( \"The deque after rotating deque is : \" ) print ( de ) # using reverse() to reverse the deque de . reverse () # printing modified deque print ( \"The deque after reversing deque is : \" ) print ( de ) The deque after extending deque at end is : deque([1, 2, 3, 4, 5, 6]) The deque after extending deque at beginning is : deque([9, 8, 7, 1, 2, 3, 4, 5, 6]) The deque after rotating deque is : deque([1, 2, 3, 4, 5, 6, 9, 8, 7]) The deque after reversing deque is : deque([7, 8, 9, 6, 5, 4, 3, 2, 1])","title":"Deque"},{"location":"DS/pydeque/#constructor","text":"dq = collections . deque ( range ( 10 )) dq deque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])","title":"Constructor"},{"location":"DS/pydeque/#append-popleft","text":"for i in range ( 10 , 15 ): dq . append ( i ) v = dq . popleft () print ( \"Inserted :\" , i , \"- popped: \" , v , \"-result:\" , dq ) Inserted : 10 - popped: 0 -result: deque([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) Inserted : 11 - popped: 1 -result: deque([2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) Inserted : 12 - popped: 2 -result: deque([3, 4, 5, 6, 7, 8, 9, 10, 11, 12]) Inserted : 13 - popped: 3 -result: deque([4, 5, 6, 7, 8, 9, 10, 11, 12, 13]) Inserted : 14 - popped: 4 -result: deque([5, 6, 7, 8, 9, 10, 11, 12, 13, 14])","title":"append() &amp; popleft()"},{"location":"DS/pydeque/#appendleft-pop","text":"for i in reversed ( range ( 0 , 5 )): dq . appendleft ( i ) v = dq . pop () print ( \"Inserted :\" , i , \"- popped: \" , v , \"-result:\" , dq ) Inserted : 4 - popped: 14 -result: deque([4, 5, 6, 7, 8, 9, 10, 11, 12, 13]) Inserted : 3 - popped: 13 -result: deque([3, 4, 5, 6, 7, 8, 9, 10, 11, 12]) Inserted : 2 - popped: 12 -result: deque([2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) Inserted : 1 - popped: 11 -result: deque([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) Inserted : 0 - popped: 10 -result: deque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])","title":"appendleft() &amp; pop()"},{"location":"DS/pydeque/#indexele-beg-end","text":"This function returns the first index of the value mentioned in arguments, starting searching from beg till end index.","title":"index(ele, beg, end)"},{"location":"DS/pydeque/#inserti-a","text":"This function inserts the value mentioned in arguments(a) at index(i) specified in arguments.","title":"insert(i, a)"},{"location":"DS/pydeque/#remove","text":"This function removes the first occurrence of value mentioned in arguments.","title":"remove()"},{"location":"DS/pydeque/#count","text":"This function counts the number of occurrences of value mentioned in arguments. # Python code to demonstrate working of # insert(), index(), remove(), count() # importing \"collections\" for deque operations import collections # initializing deque de = collections . deque ([ 1 , 2 , 3 , 3 , 4 , 2 , 4 ]) # using index() to print the first occurrence of 4 print ( \"The number 4 first occurs at a position : \" ) print ( de . index ( 4 , 2 , 5 )) # using insert() to insert the value 3 at 5th position de . insert ( 4 , 3 ) # printing modified deque print ( \"The deque after inserting 3 at 5th position is : \" ) print ( de ) # using count() to count the occurrences of 3 print ( \"The count of 3 in deque is : \" ) print ( de . count ( 3 )) # using remove() to remove the first occurrence of 3 de . remove ( 3 ) # printing modified deque print ( \"The deque after deleting first occurrence of 3 is : \" ) print ( de ) The number 4 first occurs at a position : 4 The deque after inserting 3 at 5th position is : deque([1, 2, 3, 3, 3, 4, 2, 4]) The count of 3 in deque is : 3 The deque after deleting first occurrence of 3 is : deque([1, 2, 3, 3, 4, 2, 4])","title":"count()"},{"location":"DS/pydeque/#extenditerable","text":"This function is used to add multiple values at the right end of deque. The argument passed is an iterable.","title":"extend(iterable)"},{"location":"DS/pydeque/#extendleftiterable","text":"This function is used to add multiple values at the left end of deque. The argument passed is an iterable. Order is reversed as a result of left appends.","title":"extendleft(iterable)"},{"location":"DS/pydeque/#reverse","text":"This function is used to reverse order of deque elements.","title":"reverse()"},{"location":"DS/pydeque/#rotate","text":"This function rotates the deque by the number specified in arguments. If the number specified is negative, rotation occurs to left. Else rotation is to right. # Python code to demonstrate working of # extend(), extendleft(), rotate(), reverse() # importing \"collections\" for deque operations import collections # initializing deque de = collections . deque ([ 1 , 2 , 3 ,]) # using extend() to add numbers to right end # adds 4,5,6 to right end de . extend ([ 4 , 5 , 6 ]) # printing modified deque print ( \"The deque after extending deque at end is : \" ) print ( de ) # using extendleft() to add numbers to left end # adds 7,8,9 to right end de . extendleft ([ 7 , 8 , 9 ]) # printing modified deque print ( \"The deque after extending deque at beginning is : \" ) print ( de ) # using rotate() to rotate the deque # rotates by 3 to left de . rotate ( - 3 ) # printing modified deque print ( \"The deque after rotating deque is : \" ) print ( de ) # using reverse() to reverse the deque de . reverse () # printing modified deque print ( \"The deque after reversing deque is : \" ) print ( de ) The deque after extending deque at end is : deque([1, 2, 3, 4, 5, 6]) The deque after extending deque at beginning is : deque([9, 8, 7, 1, 2, 3, 4, 5, 6]) The deque after rotating deque is : deque([1, 2, 3, 4, 5, 6, 9, 8, 7]) The deque after reversing deque is : deque([7, 8, 9, 6, 5, 4, 3, 2, 1])","title":"rotate()"},{"location":"Exe/pyCodPatterns/","text":"Window Sliding Usage: This algorithmic technique is used when we need to handle the input data in a specific window size. Window Sliding Technique is a computational technique which aims to reduce the use of nested loop and replace it with a single loop, thereby reducing the time complexity. DS involved: Array, String, Hashtable Sample Problems: - Longest substring with 'K' distinct chracters - Fruits into Baskets Example : Given an array of integers of size \u2018n\u2019, Our aim is to calculate the maximum sum of \u2018k\u2019 consecutive elements in the array. Input : arr[] = {100, 200, 300, 400}, k = 2 Output : 700 Input : arr[] = {1, 4, 2, 10, 23, 3, 1, 0, 20}, k = 4 Output : 39 We get maximum sum by adding subarray {4, 2, 10, 23} of size 4. Input : arr[] = {2, 3}, k = 3 Output : Invalid There is no subarray of size 3 as size of whole array is 2. Brute Force Approach We start with first index and sum till k-th element. We do it for all possible consecutive blocks or groups of k elements. This method requires nested for loop, the outer for loop starts with the starting element of the block of k elements and the inner or the nested loop will add up till the k-th element. Consider the below implementation : # code import sys # O(n * k) solution for finding # maximum sum of a subarray of size k INT_MIN = - sys . maxsize - 1 # Returns maximum sum in a # subarray of size k. def maxSum ( arr , n , k ): # Initialize result max_sum = INT_MIN # Consider all blocks # starting with i. for i in range ( n - k + 1 ): current_sum = 0 for j in range ( k ): current_sum = current_sum + arr [ i + j ] # Update result if required. max_sum = max ( current_sum , max_sum ) return max_sum # Driver code arr = [ 1 , 4 , 2 , 10 , 2 , 3 , 1 , 0 , 20 ] k = 4 n = len ( arr ) print ( maxSum ( arr , n , k )) 24 It can be observed from the above code that the time complexity is O(k*n) as it contains two nested loops. Sliding Window Technique The technique can be best understood with the window pane in bus, consider a window of length n and the pane which is fixed in it of length k. Consider, initially the pane is at extreme left i.e., at 0 units from the left. Now, co-relate the window with array arr[] of size n and pane with current_sum of size k elements. Now, if we apply force on the window such that it moves a unit distance ahead. The pane will cover next k consecutive elements. Applying sliding window technique : We compute the sum of first k elements out of n terms using a linear loop and store the sum in variable window_sum. Then we will graze linearly over the array till it reaches the end and simultaneously keep track of maximum sum. To get the current sum of block of k elements just subtract the first element from the previous block and add the last element of the current block . The below representation will make it clear how the window slides over the array. # O(n) solution for finding # maximum sum of a subarray of size k def maxSum ( arr , k ): # length of the array n = len ( arr ) # n must be greater than k if n < k : print ( \"Invalid\" ) return - 1 # Compute sum of first window of size k window_sum = sum ( arr [: k ]) # first sum available max_sum = window_sum # Compute the sums of remaining windows by # removing first element of previous # window and adding last element of # the current window. for i in range ( n - k ): window_sum = window_sum - arr [ i ] + arr [ i + k ] max_sum = max ( window_sum , max_sum ) return max_sum # Driver code arr = [ 1 , 4 , 2 , 10 , 2 , 3 , 1 , 0 , 20 ] k = 4 print ( maxSum ( arr , k )) 24 Islands (Matrix Traversal) Usage: This pattern describes all the efficient ways of traversing a matrix (or 2D array) DS involved: Matrix, Queue Sample Problems: - Number of Islands - Flood Fill - Cycle in a Matrix Two Pointers Usage: This technique uses two pointers to iterate input data. Generally, both pointers move in the opposite direction at a constant interval. DS Involved: Array, String, LinkedList Sample Problems: - Squaring a Sorted Array - Dutch National Flag Problem - Minimum Window Sort Fast and Slow Pointers Usage: Also known as Hare & Tortoise algorithm. This technique uses two pointers that traverse the input data at different speeds. DS Involved: Array, String, LinkedList Sample Problems: - Middle of the LinkedList - Happy Number - Cycle in a Ciscular Array Merge Intervals Usage: This technique is used to deal with overlapping intervals DS Involved: Array, heap Sample Problems: - Conflicting Appointements - Minimum Meeting Rooms A simple approach is to start from the first interval and compare it with all other intervals for overlapping, if it overlaps with any other interval, then remove the other interval from the list and merge the other into the first interval. Repeat the same steps for the remaining intervals after the first. This approach cannot be implemented in better than O(n^2) time. The idea to solve this problem is, first sort the intervals according to the starting time. Once we have the sorted intervals, we can combine all intervals in a linear traversal. The idea is, in sorted array of intervals, if interval[i] doesn\u2019t overlap with interval[i-1], then interval[i+1] cannot overlap with interval[i-1] because starting time of interval[i+1] must be greater than or equal to interval[i]. Sort the intervals based on the increasing order of starting time. Push the first interval into a stack. For each interval do the following: If the current interval does not overlap with the top of the stack then, push the current interval into the stack. If the current interval overlap with the top of the stack then, update the stack top with the ending time of the current interval. The end stack contains the merged intervals. def mergeIntervals ( intervals ): # Sort the array on the basis of start values of intervals. intervals . sort () stack = [] # insert first interval into stack stack . append ( intervals [ 0 ]) for i in intervals [ 1 :]: # Check for overlapping interval, # if interval overlap if stack [ - 1 ][ 0 ] <= i [ 0 ] <= stack [ - 1 ][ - 1 ]: stack [ - 1 ][ - 1 ] = max ( stack [ - 1 ][ - 1 ], i [ - 1 ]) else : stack . append ( i ) print ( \"The Merged Intervals are :\" , end = \" \" ) for i in range ( len ( stack )): print ( stack [ i ], end = \" \" ) arr = [[ 6 , 8 ], [ 1 , 9 ], [ 2 , 4 ], [ 4 , 7 ]] mergeIntervals ( arr ) The Merged Intervals are : [1, 9] Time complexity: O(N*log(N)) Auxiliary Space: O(N) The above solution requires O(n) extra space for the stack. We can avoid the use of extra space by doing merge operations in place. Below are detailed steps. Sort all intervals in increasing order of start time. Traverse sorted intervals starting from the first interval, Do the following for every interval. If the current interval is not the first interval and it overlaps with the previous interval, then merge it with the previous interval. Keep doing it while the interval overlaps with the previous one. Otherwise, Add the current interval to the output list of intervals. # Python program to merge overlapping Intervals in # O(n Log n) time and O(1) extra space def mergeIntervals ( arr ): # Sorting based on the increasing order # of the start intervals arr . sort ( key = lambda x : x [ 0 ]) # Stores index of last element # in output array (modified arr[]) index = 0 # Traverse all input Intervals starting from # second interval for i in range ( 1 , len ( arr )): # If this is not first Interval and overlaps # with the previous one, Merge previous and # current Intervals if ( arr [ index ][ 1 ] >= arr [ i ][ 0 ]): arr [ index ][ 1 ] = max ( arr [ index ][ 1 ], arr [ i ][ 1 ]) else : index = index + 1 arr [ index ] = arr [ i ] print ( \"The Merged Intervals are :\" , end = \" \" ) for i in range ( index + 1 ): print ( arr [ i ], end = \" \" ) # Driver code arr = [[ 6 , 8 ], [ 1 , 9 ], [ 2 , 4 ], [ 4 , 7 ]] mergeIntervals ( arr ) The Merged Intervals are : [1, 9] Time Complexity: O(N*log(N)) Auxiliary Space Complexity: O(1) Cyclic Sort Usage: Use this technique to solve array problems where the input data lies within a fixed range. DS involved: Array Sample Problems: - Find all Missing Numbers - Find all Duplicate Numbers - Find the first K Missing Positive Numbers In-place Reversal of a LinkedList Usage: This technique describes an efficient way to reverse the links between a set of nodes of a LinkedList. Often, the constraint is that we need to do this in-place, i.e., using the existing node objects and without using extra memory. DS involved: LinkedList Sample Problems: - Reverse every K-element Sub-list - Rotate a LinkedList Depth-First Search Usage: This technique is used to solve problems involving traversing trees or graphs in a depth-first search manner. DS involved: Tree, Graph, Matrix Sample Problems: - Path With Given Sequence - Count Paths for a Sum Breadth First Search Usage: This technique is used to solve problems involving traversing trees or graphs in a breadth-first search manner. DS involved: Tree, Graph, Matrix, Queue Sample Problems: - Binary Tree Level Order Traversal - Minimum Depth of a Binary Tree - Connect Level Order Siblings Two Heaps Usage: In many problems, we are given a set of elements that can be divided into two parts. We are interested in knowing the smallest element in one part and the biggest element in the other part. As the name suggests, this technique uses a Min-Heap to find the smallest element and a Max-Heap to find the biggest element. DS involved: Heap, Array Sample Problems: - Find the Median of a Number Stream - Next Interval Subsets Usage: Use this technique when the problem asks to deal with permutations or combinations of a set of elements DS involved: Queue, Array, String Sample Problems: - String Permutations by changing case - Unique Generalized Abbreviations Modified Binary Search Usage: Use this technique to search a sorted set of elements efficiently. DS involved: Array Sample Problems: - Ceiling of a Number - Bitonic Array Maximum Bitwise XOR Usage: This technique uses the XOR operator to manipulate bits to solve problems. DS involved: Array, Bits Sample Problems: - Two Single Numbers - Flip and Invert an Image Top 'K' Elements Usage: This technique is used to find top/smallest/frequently occurring \u2018K\u2019 elements in a set. DS involved: Array, Heap, Queue Sample Problems: - \u2018K\u2019 Closest Points to the Origin - Maximum Distinct Elements K-way Merge Usage: This technique helps us solve problems that involve a list of sorted arrays. DS involved: Array, Queue, Heap Sample Problems: - Kth Smallest Number in M Sorted Lists - Kth Smallest Number in a Sorted Matrix Topological Sort Usage: Use this technique to find a linear ordering of elements that have dependencies on each other. DS involved: Array, HashTable, Queue, Graph Sample Problems: - Tasks Scheduling - Alien Dictionary 0/1 Knapsack Usage: This technique is used to solve optimization problems. Use this technique to select elements that give maximum profit from a given set with a limitation on capacity and that each element can only be picked once. DS involved: Array, HashTable Sample Problems: - Equal Subset Sum Partition - Minimum Subset Sum Difference Fibonacci Numbers Usage: Use this technique to solve problems that follow the Fibonacci numbers sequence, i.e., every subsequent number is calculated from the last few numbers. DS involved: Array, HashTable Sample Problems: - Staircase - House Thief Palindromic Subsequence Usage: This technique is used to solve optimization problems related to palindromic sequences or strings. DS involved: Array, HashTable Sample Problems: - Longest Palindromic Subsequence - Minimum Deletions in a String to make it a Palindrome Longest Common Substring Usage: Use this technique to find the optimal part of a string/sequence or set of strings/sequences. DS involved: Array, HashTable Sample Problems: - Maximum Sum Increasing Subsequence - Edit Distance","title":"Coding Patterns"},{"location":"Exe/pyCodPatterns/#window-sliding","text":"Usage: This algorithmic technique is used when we need to handle the input data in a specific window size. Window Sliding Technique is a computational technique which aims to reduce the use of nested loop and replace it with a single loop, thereby reducing the time complexity. DS involved: Array, String, Hashtable Sample Problems: - Longest substring with 'K' distinct chracters - Fruits into Baskets Example : Given an array of integers of size \u2018n\u2019, Our aim is to calculate the maximum sum of \u2018k\u2019 consecutive elements in the array. Input : arr[] = {100, 200, 300, 400}, k = 2 Output : 700 Input : arr[] = {1, 4, 2, 10, 23, 3, 1, 0, 20}, k = 4 Output : 39 We get maximum sum by adding subarray {4, 2, 10, 23} of size 4. Input : arr[] = {2, 3}, k = 3 Output : Invalid There is no subarray of size 3 as size of whole array is 2. Brute Force Approach We start with first index and sum till k-th element. We do it for all possible consecutive blocks or groups of k elements. This method requires nested for loop, the outer for loop starts with the starting element of the block of k elements and the inner or the nested loop will add up till the k-th element. Consider the below implementation : # code import sys # O(n * k) solution for finding # maximum sum of a subarray of size k INT_MIN = - sys . maxsize - 1 # Returns maximum sum in a # subarray of size k. def maxSum ( arr , n , k ): # Initialize result max_sum = INT_MIN # Consider all blocks # starting with i. for i in range ( n - k + 1 ): current_sum = 0 for j in range ( k ): current_sum = current_sum + arr [ i + j ] # Update result if required. max_sum = max ( current_sum , max_sum ) return max_sum # Driver code arr = [ 1 , 4 , 2 , 10 , 2 , 3 , 1 , 0 , 20 ] k = 4 n = len ( arr ) print ( maxSum ( arr , n , k )) 24 It can be observed from the above code that the time complexity is O(k*n) as it contains two nested loops. Sliding Window Technique The technique can be best understood with the window pane in bus, consider a window of length n and the pane which is fixed in it of length k. Consider, initially the pane is at extreme left i.e., at 0 units from the left. Now, co-relate the window with array arr[] of size n and pane with current_sum of size k elements. Now, if we apply force on the window such that it moves a unit distance ahead. The pane will cover next k consecutive elements. Applying sliding window technique : We compute the sum of first k elements out of n terms using a linear loop and store the sum in variable window_sum. Then we will graze linearly over the array till it reaches the end and simultaneously keep track of maximum sum. To get the current sum of block of k elements just subtract the first element from the previous block and add the last element of the current block . The below representation will make it clear how the window slides over the array. # O(n) solution for finding # maximum sum of a subarray of size k def maxSum ( arr , k ): # length of the array n = len ( arr ) # n must be greater than k if n < k : print ( \"Invalid\" ) return - 1 # Compute sum of first window of size k window_sum = sum ( arr [: k ]) # first sum available max_sum = window_sum # Compute the sums of remaining windows by # removing first element of previous # window and adding last element of # the current window. for i in range ( n - k ): window_sum = window_sum - arr [ i ] + arr [ i + k ] max_sum = max ( window_sum , max_sum ) return max_sum # Driver code arr = [ 1 , 4 , 2 , 10 , 2 , 3 , 1 , 0 , 20 ] k = 4 print ( maxSum ( arr , k )) 24","title":"Window Sliding"},{"location":"Exe/pyCodPatterns/#islands-matrix-traversal","text":"Usage: This pattern describes all the efficient ways of traversing a matrix (or 2D array) DS involved: Matrix, Queue Sample Problems: - Number of Islands - Flood Fill - Cycle in a Matrix","title":"Islands (Matrix Traversal)"},{"location":"Exe/pyCodPatterns/#two-pointers","text":"Usage: This technique uses two pointers to iterate input data. Generally, both pointers move in the opposite direction at a constant interval. DS Involved: Array, String, LinkedList Sample Problems: - Squaring a Sorted Array - Dutch National Flag Problem - Minimum Window Sort","title":"Two Pointers"},{"location":"Exe/pyCodPatterns/#fast-and-slow-pointers","text":"Usage: Also known as Hare & Tortoise algorithm. This technique uses two pointers that traverse the input data at different speeds. DS Involved: Array, String, LinkedList Sample Problems: - Middle of the LinkedList - Happy Number - Cycle in a Ciscular Array","title":"Fast and Slow Pointers"},{"location":"Exe/pyCodPatterns/#merge-intervals","text":"Usage: This technique is used to deal with overlapping intervals DS Involved: Array, heap Sample Problems: - Conflicting Appointements - Minimum Meeting Rooms A simple approach is to start from the first interval and compare it with all other intervals for overlapping, if it overlaps with any other interval, then remove the other interval from the list and merge the other into the first interval. Repeat the same steps for the remaining intervals after the first. This approach cannot be implemented in better than O(n^2) time. The idea to solve this problem is, first sort the intervals according to the starting time. Once we have the sorted intervals, we can combine all intervals in a linear traversal. The idea is, in sorted array of intervals, if interval[i] doesn\u2019t overlap with interval[i-1], then interval[i+1] cannot overlap with interval[i-1] because starting time of interval[i+1] must be greater than or equal to interval[i]. Sort the intervals based on the increasing order of starting time. Push the first interval into a stack. For each interval do the following: If the current interval does not overlap with the top of the stack then, push the current interval into the stack. If the current interval overlap with the top of the stack then, update the stack top with the ending time of the current interval. The end stack contains the merged intervals. def mergeIntervals ( intervals ): # Sort the array on the basis of start values of intervals. intervals . sort () stack = [] # insert first interval into stack stack . append ( intervals [ 0 ]) for i in intervals [ 1 :]: # Check for overlapping interval, # if interval overlap if stack [ - 1 ][ 0 ] <= i [ 0 ] <= stack [ - 1 ][ - 1 ]: stack [ - 1 ][ - 1 ] = max ( stack [ - 1 ][ - 1 ], i [ - 1 ]) else : stack . append ( i ) print ( \"The Merged Intervals are :\" , end = \" \" ) for i in range ( len ( stack )): print ( stack [ i ], end = \" \" ) arr = [[ 6 , 8 ], [ 1 , 9 ], [ 2 , 4 ], [ 4 , 7 ]] mergeIntervals ( arr ) The Merged Intervals are : [1, 9] Time complexity: O(N*log(N)) Auxiliary Space: O(N) The above solution requires O(n) extra space for the stack. We can avoid the use of extra space by doing merge operations in place. Below are detailed steps. Sort all intervals in increasing order of start time. Traverse sorted intervals starting from the first interval, Do the following for every interval. If the current interval is not the first interval and it overlaps with the previous interval, then merge it with the previous interval. Keep doing it while the interval overlaps with the previous one. Otherwise, Add the current interval to the output list of intervals. # Python program to merge overlapping Intervals in # O(n Log n) time and O(1) extra space def mergeIntervals ( arr ): # Sorting based on the increasing order # of the start intervals arr . sort ( key = lambda x : x [ 0 ]) # Stores index of last element # in output array (modified arr[]) index = 0 # Traverse all input Intervals starting from # second interval for i in range ( 1 , len ( arr )): # If this is not first Interval and overlaps # with the previous one, Merge previous and # current Intervals if ( arr [ index ][ 1 ] >= arr [ i ][ 0 ]): arr [ index ][ 1 ] = max ( arr [ index ][ 1 ], arr [ i ][ 1 ]) else : index = index + 1 arr [ index ] = arr [ i ] print ( \"The Merged Intervals are :\" , end = \" \" ) for i in range ( index + 1 ): print ( arr [ i ], end = \" \" ) # Driver code arr = [[ 6 , 8 ], [ 1 , 9 ], [ 2 , 4 ], [ 4 , 7 ]] mergeIntervals ( arr ) The Merged Intervals are : [1, 9] Time Complexity: O(N*log(N)) Auxiliary Space Complexity: O(1)","title":"Merge Intervals"},{"location":"Exe/pyCodPatterns/#cyclic-sort","text":"Usage: Use this technique to solve array problems where the input data lies within a fixed range. DS involved: Array Sample Problems: - Find all Missing Numbers - Find all Duplicate Numbers - Find the first K Missing Positive Numbers","title":"Cyclic Sort"},{"location":"Exe/pyCodPatterns/#in-place-reversal-of-a-linkedlist","text":"Usage: This technique describes an efficient way to reverse the links between a set of nodes of a LinkedList. Often, the constraint is that we need to do this in-place, i.e., using the existing node objects and without using extra memory. DS involved: LinkedList Sample Problems: - Reverse every K-element Sub-list - Rotate a LinkedList","title":"In-place Reversal of a LinkedList"},{"location":"Exe/pyCodPatterns/#depth-first-search","text":"Usage: This technique is used to solve problems involving traversing trees or graphs in a depth-first search manner. DS involved: Tree, Graph, Matrix Sample Problems: - Path With Given Sequence - Count Paths for a Sum","title":"Depth-First Search"},{"location":"Exe/pyCodPatterns/#breadth-first-search","text":"Usage: This technique is used to solve problems involving traversing trees or graphs in a breadth-first search manner. DS involved: Tree, Graph, Matrix, Queue Sample Problems: - Binary Tree Level Order Traversal - Minimum Depth of a Binary Tree - Connect Level Order Siblings","title":"Breadth First Search"},{"location":"Exe/pyCodPatterns/#two-heaps","text":"Usage: In many problems, we are given a set of elements that can be divided into two parts. We are interested in knowing the smallest element in one part and the biggest element in the other part. As the name suggests, this technique uses a Min-Heap to find the smallest element and a Max-Heap to find the biggest element. DS involved: Heap, Array Sample Problems: - Find the Median of a Number Stream - Next Interval","title":"Two Heaps"},{"location":"Exe/pyCodPatterns/#subsets","text":"Usage: Use this technique when the problem asks to deal with permutations or combinations of a set of elements DS involved: Queue, Array, String Sample Problems: - String Permutations by changing case - Unique Generalized Abbreviations","title":"Subsets"},{"location":"Exe/pyCodPatterns/#modified-binary-search","text":"Usage: Use this technique to search a sorted set of elements efficiently. DS involved: Array Sample Problems: - Ceiling of a Number - Bitonic Array Maximum","title":"Modified Binary Search"},{"location":"Exe/pyCodPatterns/#bitwise-xor","text":"Usage: This technique uses the XOR operator to manipulate bits to solve problems. DS involved: Array, Bits Sample Problems: - Two Single Numbers - Flip and Invert an Image","title":"Bitwise XOR"},{"location":"Exe/pyCodPatterns/#top-k-elements","text":"Usage: This technique is used to find top/smallest/frequently occurring \u2018K\u2019 elements in a set. DS involved: Array, Heap, Queue Sample Problems: - \u2018K\u2019 Closest Points to the Origin - Maximum Distinct Elements","title":"Top 'K' Elements"},{"location":"Exe/pyCodPatterns/#k-way-merge","text":"Usage: This technique helps us solve problems that involve a list of sorted arrays. DS involved: Array, Queue, Heap Sample Problems: - Kth Smallest Number in M Sorted Lists - Kth Smallest Number in a Sorted Matrix","title":"K-way Merge"},{"location":"Exe/pyCodPatterns/#topological-sort","text":"Usage: Use this technique to find a linear ordering of elements that have dependencies on each other. DS involved: Array, HashTable, Queue, Graph Sample Problems: - Tasks Scheduling - Alien Dictionary","title":"Topological Sort"},{"location":"Exe/pyCodPatterns/#01-knapsack","text":"Usage: This technique is used to solve optimization problems. Use this technique to select elements that give maximum profit from a given set with a limitation on capacity and that each element can only be picked once. DS involved: Array, HashTable Sample Problems: - Equal Subset Sum Partition - Minimum Subset Sum Difference","title":"0/1 Knapsack"},{"location":"Exe/pyCodPatterns/#fibonacci-numbers","text":"Usage: Use this technique to solve problems that follow the Fibonacci numbers sequence, i.e., every subsequent number is calculated from the last few numbers. DS involved: Array, HashTable Sample Problems: - Staircase - House Thief","title":"Fibonacci Numbers"},{"location":"Exe/pyCodPatterns/#palindromic-subsequence","text":"Usage: This technique is used to solve optimization problems related to palindromic sequences or strings. DS involved: Array, HashTable Sample Problems: - Longest Palindromic Subsequence - Minimum Deletions in a String to make it a Palindrome","title":"Palindromic Subsequence"},{"location":"Exe/pyCodPatterns/#longest-common-substring","text":"Usage: Use this technique to find the optimal part of a string/sequence or set of strings/sequences. DS involved: Array, HashTable Sample Problems: - Maximum Sum Increasing Subsequence - Edit Distance","title":"Longest Common Substring"},{"location":"Exe/pyIntPractice_DS/","text":"Data Structures Arrays First Duplicate Asked by Google - 15 min - Easy Given an array a that contains only numbers in the range from 1 to a.length, find the first duplicate number for which the second occurrence has the minimal index. In other words, if there are more than 1 duplicated numbers, return the number for which the second occurrence has a smaller index than the second occurrence of the other number does. If there are no such elements, return -1. Example For a = [2, 1, 3, 5, 3, 2], the output should be solution(a) = 3. There are 2 duplicates: numbers 2 and 3. The second occurrence of 3 has a smaller index than the second occurrence of 2 does, so the answer is 3. For a = [2, 2], the output should be solution(a) = 2; For a = [2, 4, 3, 5, 1], the output should be solution(a) = -1. Solutions Solution 1 def solution ( a ): seen = set () for i in a : if i in seen : return i seen . add ( i ) return - 1 Solution 2 def solution ( a ): for i in a : a [ abs ( i ) - 1 ] *= - 1 if a [ abs ( i ) - 1 ] > 0 : return abs ( i ) return - 1 First Not Repeating Character Asked by Amazon - 15 min - Easy Given a string s consisting of small English letters, find and return the first instance of a non-repeating character in it. If there is no such character, return '_'. Example For s = \"abacabad\", the output should be solution(s) = 'c'. There are 2 non-repeating characters in the string: 'c' and 'd'. Return c since it appears in the string first. For s = \"abacabaabacaba\", the output should be solution(s) = '_'. There are no characters in this string that do not repeat. from collections import Counter def solution ( s ): st = Counter ( s ) print ( st . keys ()) for i in st . keys (): print ( i ) if st [ i ] == 1 : return i return '_' def solution ( s ): chk = [] for i in range ( len ( s )): if s [ i ] not in s [ i + 1 :] and s [ i ] not in chk : return s [ i ] chk . append ( s [ i ]) return '_' Rotate Image Asked by Amazon, Microsoft, and Apple - 15 min - Easy Note: Try to solve this task in-place (with O(1) additional memory), since this is what you'll be asked to do during an interview. You are given an n x n 2D matrix that represents an image. Rotate the image by 90 degrees (clockwise). Example For a = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] the output should be solution(a) = [[7, 4, 1], [8, 5, 2], [9, 6, 3]] Solutions def solution ( a ): a = list ( zip ( * a )) a = [ list ( l [:: - 1 ]) for l in a ] return a def solution ( a ): return list ( zip ( * reversed ( a ))) Sudoku 2 Asked by Apple and Uber - 30 min - Easy Sudoku is a number-placement puzzle. The objective is to fill a 9 \u00d7 9 grid with numbers in such a way that each column, each row, and each of the nine 3 \u00d7 3 sub-grids that compose the grid all contain all of the numbers from 1 to 9 one time. Implement an algorithm that will check whether the given grid of numbers represents a valid Sudoku puzzle according to the layout rules described above. Note that the puzzle represented by grid does not have to be solvable. Example For grid = [['.', '.', '.', '1', '4', '.', '.', '2', '.'], ['.', '.', '6', '.', '.', '.', '.', '.', '.'], ['.', '.', '.', '.', '.', '.', '.', '.', '.'], ['.', '.', '1', '.', '.', '.', '.', '.', '.'], ['.', '6', '7', '.', '.', '.', '.', '.', '9'], ['.', '.', '.', '.', '.', '.', '8', '1', '.'], ['.', '3', '.', '.', '.', '.', '.', '.', '6'], ['.', '.', '.', '.', '.', '7', '.', '.', '.'], ['.', '.', '.', '5', '.', '.', '.', '7', '.']] the output should be solution(grid) = true; For grid = [['.', '.', '.', '.', '2', '.', '.', '9', '.'], ['.', '.', '.', '.', '6', '.', '.', '.', '.'], ['7', '1', '.', '.', '7', '5', '.', '.', '.'], ['.', '7', '.', '.', '.', '.', '.', '.', '.'], ['.', '.', '.', '.', '8', '3', '.', '.', '.'], ['.', '.', '8', '.', '.', '7', '.', '6', '.'], ['.', '.', '.', '.', '.', '2', '.', '.', '.'], ['.', '1', '.', '2', '.', '.', '.', '.', '.'], ['.', '2', '.', '.', '3', '.', '.', '.', '.']] the output should be solution(grid) = false. The given grid is not correct because there are two 1s in the second column. Each column, each row, and each 3 \u00d7 3 subgrid can only contain the numbers 1 through 9 one time. Solution def line_is_valid ( row ): tmp_dict = {} valid = True if len ( row ) > 0 : #print(row) for r in row : if r in tmp_dict : tmp_dict [ r ] += 1 else : tmp_dict [ r ] = 1 if tmp_dict [ max ( tmp_dict , key = tmp_dict . get )] > 1 : valid = False return valid def grid_is_valid ( grid ): valid = True for i in range ( len ( grid )): row = [ int ( x ) for x in grid [ i ] if x != '.' ] #print(row) valid = line_is_valid ( row ) if not valid : break return valid def solution ( grid ): result = False # check rows: rows_valid = grid_is_valid ( grid ) # check columns: col_valid = grid_is_valid ( list ( zip ( * grid ))) # check 3x3 grids: grid3x3_valid = True for i in range ( 0 , len ( grid ), 3 ): if not grid3x3_valid : break sub_mat = grid [ i : i + 3 ] for j in range ( 0 , len ( grid [ 0 ]), 3 ): tmp_list = [ x [ j : j + 3 ] for x in sub_mat ] #print(tmp_list) tmp_list = [ tmp_list [ k ][ l ] for k in range ( len ( tmp_list )) for l in range ( len ( tmp_list [ k ]))] row = [ int ( x ) for x in tmp_list if x != '.' ] grid3x3_valid = line_is_valid ( row ) #print(grid3x3_valid) if not grid3x3_valid : break if rows_valid and col_valid and grid3x3_valid : result = True return result Is Crypt Solution Asked by Palantir Solutions - 15 min - Easy A cryptarithm is a mathematical puzzle for which the goal is to find the correspondence between letters and digits, such that the given arithmetic equation consisting of letters holds true when the letters are converted to digits. You have an array of strings crypt, the cryptarithm, and an an array containing the mapping of letters and digits, solution. The array crypt will contain three non-empty strings that follow the structure: [word1, word2, word3], which should be interpreted as the word1 + word2 = word3 cryptarithm. If crypt, when it is decoded by replacing all of the letters in the cryptarithm with digits using the mapping in solution, becomes a valid arithmetic equation containing no numbers with leading zeroes, the answer is true. If it does not become a valid arithmetic solution, the answer is false. Note that number 0 doesn't contain leading zeroes (while for example 00 or 0123 do). Example For crypt = [\"SEND\", \"MORE\", \"MONEY\"] and solution = [['O', '0'], ['M', '1'], ['Y', '2'], ['E', '5'], ['N', '6'], ['D', '7'], ['R', '8'], ['S', '9']] the output should be solution(crypt, solution) = true. When you decrypt \"SEND\", \"MORE\", and \"MONEY\" using the mapping given in crypt, you get 9567 + 1085 = 10652 which is correct and a valid arithmetic equation. For crypt = [\"TEN\", \"TWO\", \"ONE\"] and solution = [['O', '1'], ['T', '0'], ['W', '9'], ['E', '5'], ['N', '4']] the output should be solution(crypt, solution) = false. Even though 054 + 091 = 145, 054 and 091 both contain leading zeroes, meaning that this is not a valid solution. Solution def solution ( crypt , solution ): # a + b = c a = crypt [ 0 ] b = crypt [ 1 ] c = crypt [ 2 ] ch_dict = {} for s in solution : ch_dict [ s [ 0 ]] = int ( s [ 1 ]) hasLeadingZeros = False if ch_dict [ a [ 0 ]] == 0 or ch_dict [ b [ 0 ]] == 0 or ch_dict [ c [ 0 ]] == 0 : hasLeadingZeros = True n1 = '' . join ([ str ( ch_dict [ i ]) for i in a ]) n2 = '' . join ([ str ( ch_dict [ i ]) for i in b ]) n3 = '' . join ([ str ( ch_dict [ i ]) for i in c ]) if int ( n1 ) + int ( n2 ) == int ( n3 ): if hasLeadingZeros and int ( n3 ) == 0 and len ( n3 ) == len ( str ( int ( n3 ))): return True elif hasLeadingZeros : return False else : return True else : return False LinkedLists Remove K from List 15 min - Easy Note: Try to solve this task in O(n) time using O(1) additional space, where n is the number of elements in the list, since this is what you'll be asked to do during an interview. Given a singly linked list of integers l and an integer k, remove all elements from list l that have a value equal to k. Example For l = [3, 1, 2, 3, 4, 5] and k = 3, the output should be solution(l, k) = [1, 2, 4, 5]; For l = [1, 2, 3, 4, 5, 6, 7] and k = 10, the output should be solution(l, k) = [1, 2, 3, 4, 5, 6, 7]. Solution # Singly-linked lists are already defined with this interface: # class ListNode(object): # def __init__(self, x): # self.value = x # self.next = None # def solution ( l , k ): if l == None : return l while l != None and l . value == k : l = l . next n = l while n != None and n . next != None : if n . next . value == k : n . next = n . next . next else : n = n . next return l Is List Palindrome Asked by Amazon and Meta - 30 min - Easy Note: Try to solve this task in O(n) time using O(1) additional space, where n is the number of elements in l, since this is what you'll be asked to do during an interview. Given a singly linked list of integers, determine whether or not it's a palindrome. Note: in examples below and tests preview linked lists are presented as arrays just for simplicity of visualization: in real data you will be given a head node l of the linked list Example For l = [0, 1, 0], the output should be solution(l) = true; For l = [1, 2, 2, 3], the output should be solution(l) = false. Solutions # Definition for singly-linked list: # class ListNode(object): # def __init__(self, x): # self.value = x # self.next = None # def solution ( l ): a = [] while l != None : a . append ( l . value ) l = l . next return a == a [:: - 1 ] # Definition for singly-linked list: # class ListNode(object): # def __init__(self, x): # self.value = x # self.next = None # def solution ( l ): if not l or not l . next : return True s = 1 n = l while n . next : n = n . next s += 1 middle = s // 2 n = l for i in range ( middle ): n = n . next if s % 2 : n = n . next r = n # reverse n m = r . next for _ in range ( middle - 1 ): # flip n m . next , r , m = r , m , m . next for _ in range ( middle ): if r . value != l . value : return False r = r . next l = l . next return True Add Two Huge Numbers 30 min - Easy You're given 2 huge integers represented by linked lists. Each linked list element is a number from 0 to 9999 that represents a number with exactly 4 digits. The represented number might have leading zeros. Your task is to add up these huge integers and return the result in the same format. Example For a = [9876, 5432, 1999] and b = [1, 8001], the output should be solution(a, b) = [9876, 5434, 0]. Explanation: 987654321999 + 18001 = 987654340000. For a = [123, 4, 5] and b = [100, 100, 100], the output should be solution(a, b) = [223, 104, 105]. Explanation: 12300040005 + 10001000100 = 22301040105. # Definition for singly-linked list: # class ListNode(object): # def __init__(self, x): # self.value = x # self.next = None # def solution ( a , b ): a = reverse ( a ) b = reverse ( b ) carry = 0 result = None while a is not None or b is not None or carry > 0 : raw = (( a . value if a is not None else 0 ) + ( b . value if b is not None else 0 ) + carry ) node = ListNode ( raw % 10000 ) node . next = result result = node carry = raw // 10000 if a is not None : a = a . next if b is not None : b = b . next return result def reverse ( list ): current = list previous = None while current is not None : previous , current . next , current = current , previous , current . next return previous Merge Two LinkedLists 30 min - Medium Note: Your solution should have O(l1.length + l2.length) time complexity, since this is what you will be asked to accomplish in an interview. Given two singly linked lists sorted in non-decreasing order, your task is to merge them. In other words, return a singly linked list, also sorted in non-decreasing order, that contains the elements from both original lists. Example For l1 = [1, 2, 3] and l2 = [4, 5, 6], the output should be solution(l1, l2) = [1, 2, 3, 4, 5, 6]; For l1 = [1, 1, 2, 4] and l2 = [0, 3, 5], the output should be solution(l1, l2) = [0, 1, 1, 2, 3, 4, 5]. Solution # Singly-linked lists are already defined with this interface: # class ListNode(object): # def __init__(self, x): # self.value = x # self.next = None # # Create & Handle List operations class LinkedList : def __init__ ( self ): self . head = None # Method to display the list def printList ( self ): temp = self . head while temp : print ( temp . value , end = \" \" ) temp = temp . next # Method to add element to list def addToList ( self , newData ): newNode = ListNode ( newData ) if self . head is None : self . head = newNode return last = self . head while last . next : last = last . next last . next = newNode # Function to merge the lists # Takes two lists which are sorted # joins them to get a single sorted list def solution ( headA , headB ): # A dummy node to store the result dummyNode = ListNode ( 0 ) # Tail stores the last node tail = dummyNode while True : # If any of the list gets completely empty # directly join all the elements of the other list if headA is None : tail . next = headB break if headB is None : tail . next = headA break # Compare the data of the lists and whichever is smaller is # appended to the last of the merged list and the head is changed if headA . value <= headB . value : tail . next = headA headA = headA . next else : tail . next = headB headB = headB . next # Advance the tail tail = tail . next # Returns the head of the merged list return dummyNode . next Reverse Nodes in K Groups 45 min - Hard Note: Your solution should have O(n) time complexity, where n is the number of elements in l, and O(1) additional space complexity, since this is what you would be asked to accomplish in an interview. Given a linked list l, reverse its nodes k at a time and return the modified list. k is a positive integer that is less than or equal to the length of l. If the number of nodes in the linked list is not a multiple of k, then the nodes that are left out at the end should remain as-is. You may not alter the values in the nodes - only the nodes themselves can be changed. Example For l = [1, 2, 3, 4, 5] and k = 2, the output should be solution(l, k) = [2, 1, 4, 3, 5]; For l = [1, 2, 3, 4, 5] and k = 1, the output should be solution(l, k) = [1, 2, 3, 4, 5]; For l = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] and k = 3, the output should be solution(l, k) = [3, 2, 1, 6, 5, 4, 9, 8, 7, 10, 11]. Solution # Singly-linked lists are already defined with this interface: # class ListNode(object): # def __init__(self, x): # self.value = x # self.next = None # def solution ( l , k ): if not l : return l c = l n = k while c and n : c = c . next n -= 1 if n : return l curr = l prev = None next = None n = k while curr and n : next = curr . next curr . next = prev prev = curr curr = next n -= 1 if next : l . next = solution ( next , k ) return prev Rearrange Last N 40 min - Hard Note: Try to solve this task in O(list size) time using O(1) additional space, since this is what you'll be asked during an interview. Given a singly linked list of integers l and a non-negative integer n, move the last n list nodes to the beginning of the linked list. Example For l = [1, 2, 3, 4, 5] and n = 3, the output should be solution(l, n) = [3, 4, 5, 1, 2]; For l = [1, 2, 3, 4, 5, 6, 7] and n = 1, the output should be solution(l, n) = [7, 1, 2, 3, 4, 5, 6]. Solution # Singly-linked lists are already defined with this interface: # class ListNode(object): # def __init__(self, x): # self.value = x # self.next = None # def solution ( l , n ): if n == 0 : return l front , back = l , l for _ in range ( n ): front = front . next if not front : return l while front . next : front = front . next back = back . next out = back . next back . next = None front . next = l return out Hashtables Grouping Dishes Asked by Linkedin - 20 min - Easy You are given a list dishes, where each element consists of a list of strings beginning with the name of the dish, followed by all the ingredients used in preparing it. You want to group the dishes by ingredients, so that for each ingredient you'll be able to find all the dishes that contain it (if there are at least 2 such dishes). Return an array where each element is a list beginning with the ingredient name, followed by the names of all the dishes that contain this ingredient. The dishes inside each list should be sorted alphabetically, and the result array should be sorted alphabetically by the names of the ingredients. Example For dishes = [[ \"Salad\" , \"Tomato\" , \"Cucumber\" , \"Salad\" , \"Sauce\" ], [ \"Pizza\" , \"Tomato\" , \"Sausage\" , \"Sauce\" , \"Dough\" ], [ \"Quesadilla\" , \"Chicken\" , \"Cheese\" , \"Sauce\" ], [ \"Sandwich\" , \"Salad\" , \"Bread\" , \"Tomato\" , \"Cheese\" ]] the output should be solution ( dishes ) = [[ \"Cheese\" , \"Quesadilla\" , \"Sandwich\" ], [ \"Salad\" , \"Salad\" , \"Sandwich\" ], [ \"Sauce\" , \"Pizza\" , \"Quesadilla\" , \"Salad\" ], [ \"Tomato\" , \"Pizza\" , \"Salad\" , \"Sandwich\" ]] For dishes = [[ \"Pasta\" , \"Tomato Sauce\" , \"Onions\" , \"Garlic\" ], [ \"Chicken Curry\" , \"Chicken\" , \"Curry Sauce\" ], [ \"Fried Rice\" , \"Rice\" , \"Onions\" , \"Nuts\" ], [ \"Salad\" , \"Spinach\" , \"Nuts\" ], [ \"Sandwich\" , \"Cheese\" , \"Bread\" ], [ \"Quesadilla\" , \"Chicken\" , \"Cheese\" ]] the output should be solution ( dishes ) = [[ \"Cheese\" , \"Quesadilla\" , \"Sandwich\" ], [ \"Chicken\" , \"Chicken Curry\" , \"Quesadilla\" ], [ \"Nuts\" , \"Fried Rice\" , \"Salad\" ], [ \"Onions\" , \"Fried Rice\" , \"Pasta\" ]] Solution def solution ( dishes ): groups = {} for d , * v in dishes : for x in v : groups . setdefault ( x , []) . append ( d ) ans = [] for x in sorted ( groups ): if len ( groups [ x ]) >= 2 : ans . append ([ x ] + sorted ( groups [ x ])) return ans Are Following Patterns Asked by Google - 30 min - Easy Given an array strings, determine whether it follows the sequence given in the patterns array. In other words, there should be no i and j for which strings[i] = strings[j] and patterns[i] \u2260 patterns[j] or for which strings[i] \u2260 strings[j] and patterns[i] = patterns[j]. Example For strings = [\"cat\", \"dog\", \"dog\"] and patterns = [\"a\", \"b\", \"b\"], the output should be solution(strings, patterns) = true; For strings = [\"cat\", \"dog\", \"doggy\"] and patterns = [\"a\", \"b\", \"b\"], the output should be solution(strings, patterns) = false. Solution def solution ( strings , patterns ): return len ( set ( strings )) == len ( set ( patterns )) == len ( set ( zip ( strings , patterns ))) Contains Close Nuns Asked by Palantir - 30 min - Medium Given an array of integers nums and an integer k, determine whether there are two distinct indices i and j in the array where nums[i] = nums[j] and the absolute difference between i and j is less than or equal to k. Example For nums = [0, 1, 2, 3, 5, 2] and k = 3, the output should be solution(nums, k) = true. There are two 2s in nums, and the absolute difference between their positions is exactly 3. For nums = [0, 1, 2, 3, 5, 2] and k = 2, the output should be solution(nums, k) = false. The absolute difference between the positions of the two 2s is 3, which is more than k. Solution def solution ( nums , k ): dct = {} #key:number, value:index contains the last occurance index of a number if not nums or k == 0 : return False for index , num in enumerate ( nums ): try : if index - dct [ num ] <= k : return True except : pass dct [ num ] = index return False Possible Sums Asked by Google - 45 min - Hard You have a collection of coins, and you know the values of the coins and the quantity of each type of coin in it. You want to know how many distinct sums you can make from non-empty groupings of these coins. Example For coins = [10, 50, 100] and quantity = [1, 2, 1], the output should be solution(coins, quantity) = 9. Here are all the possible sums: 50 = 50; 10 + 50 = 60; 50 + 100 = 150; 10 + 50 + 100 = 160; 50 + 50 = 100; 10 + 50 + 50 = 110; 50 + 50 + 100 = 200; 10 + 50 + 50 + 100 = 210; 10 = 10; 100 = 100; 10 + 100 = 110. As you can see, there are 9 distinct sums that can be created from non-empty groupings of your coins. Solution def solution ( coins , quantity ): possible_sums = { 0 } for c , q in zip ( coins , quantity ): possible_sums = { x + c * i for x in possible_sums for i in range ( q + 1 )} return len ( possible_sums ) - 1 Swap Lex Order Asked by Meta - 45 min - Hard Given a string str and array of pairs that indicates which indices in the string can be swapped, return the lexicographically largest string that results from doing the allowed swaps. You can swap indices any number of times. Example For str = \"abdc\" and pairs = [[1, 4], [3, 4]], the output should be solution(str, pairs) = \"dbca\". By swapping the given indices, you get the strings: \"cbda\", \"cbad\", \"dbac\", \"dbca\". The lexicographically largest string in this list is \"dbca\". Solution def solution ( strn , pairs ): # if not strn or not pairs : return strn # check for connected node groupings which can be sorted individually grp = {} # set of all possible locations an index could end up for a , b in pairs : g = grp . get ( a ,{ a }) | grp . get ( b ,{ b }) for n in g : # reset all nodes in group grp [ n ] = g for n in grp : grp [ n ] = tuple ( sorted ( grp [ n ])) reord = {} for c in set ( grp . values ()) : word = sorted (( strn [ i - 1 ] for i in c ), reverse = True ) for i , l in zip ( c , word ) : reord [ i - 1 ] = l # string is 0 indexed return '' . join ( reord . get ( i , x ) for i , x in enumerate ( strn )) Trees: Basic Has Path With Given Sum 20 min - Easy Given a binary tree t and an integer s, determine whether there is a root to leaf path in t such that the sum of vertex values equals s. Example For t = { \"value\": 4, \"left\": { \"value\": 1, \"left\": { \"value\": -2, \"left\": null, \"right\": { \"value\": 3, \"left\": null, \"right\": null } }, \"right\": null }, \"right\": { \"value\": 3, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": { \"value\": 2, \"left\": { \"value\": -2, \"left\": null, \"right\": null }, \"right\": { \"value\": -3, \"left\": null, \"right\": null } } } } and s = 7, the output should be solution(t, s) = true. This is what this tree looks like: 4 / \\ 1 3 / / \\ -2 1 2 \\ / \\ 3 -2 -3 Path 4 -> 3 -> 2 -> -2 gives us 7, the required sum. For t = { \"value\": 4, \"left\": { \"value\": 1, \"left\": { \"value\": -2, \"left\": null, \"right\": { \"value\": 3, \"left\": null, \"right\": null } }, \"right\": null }, \"right\": { \"value\": 3, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": { \"value\": 2, \"left\": { \"value\": -4, \"left\": null, \"right\": null }, \"right\": { \"value\": -3, \"left\": null, \"right\": null } } } } and s = 7, the output should be solution(t, s) = false. This is what this tree looks like: 4 / \\ 1 3 / / \\ -2 1 2 \\ / \\ 3 -4 -3 There is no path from root to leaf with the given sum 7. Solution # # Definition for binary tree: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None def solution ( t , s ): if t is None : return s == 0 return solution ( t . left , s - t . value ) or solution ( t . right , s - t . value ) Is Tree Symmetric Asked by Linkedin and Microsoft - 30 min - Easy Given a binary tree t, determine whether it is symmetric around its center, i.e. each side mirrors the other. Example For t = { \"value\": 1, \"left\": { \"value\": 2, \"left\": { \"value\": 3, \"left\": null, \"right\": null }, \"right\": { \"value\": 4, \"left\": null, \"right\": null } }, \"right\": { \"value\": 2, \"left\": { \"value\": 4, \"left\": null, \"right\": null }, \"right\": { \"value\": 3, \"left\": null, \"right\": null } } } the output should be solution(t) = true. Here's what the tree in this example looks like: 1 / \\ 2 2 / \\ / \\ 3 4 4 3 As you can see, it is symmetric. For t = { \"value\": 1, \"left\": { \"value\": 2, \"left\": null, \"right\": { \"value\": 3, \"left\": null, \"right\": null } }, \"right\": { \"value\": 2, \"left\": null, \"right\": { \"value\": 3, \"left\": null, \"right\": null } } } the output should be solution(t) = false. Here's what the tree in this example looks like: 1 / \\ 2 2 \\ \\ 3 3 As you can see, it is not symmetric. Solutions # # Definition for binary tree: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None def solution ( t ): def mirrorEquals ( left , right ): if left is None or right is None : return left == None and right == None return left . value == right . value and \\ mirrorEquals ( left . left , right . right ) and \\ mirrorEquals ( left . right , right . left ) return mirrorEquals ( t , t ) def solution ( t ): def iTS ( le , ri ): if le is None and ri is None : return True elif le is None or ri is None : return False else : if le . value == ri . value : return iTS ( le . left , ri . right ) and iTS ( le . right , ri . left ) else : return False if t is None : return True else : return iTS ( t . left , t . right ) Find Profession 20 min - Easy Consider a special family of Engineers and Doctors. This family has the following rules: Everybody has two children. The first child of an Engineer is an Engineer and the second child is a Doctor. The first child of a Doctor is a Doctor and the second child is an Engineer. All generations of Doctors and Engineers start with an Engineer. We can represent the situation using this diagram: E / \\ E D / \\ / \\ E D D E / \\ / \\ / \\ / \\ E D D E D E E D Given the level and position of a person in the ancestor tree above, find the profession of the person. Note: in this tree first child is considered as left child, second - as right. Example For level = 3 and pos = 3, the output should be solution(level, pos) = \"Doctor\". Solution def solution ( level , pos ): \"\"\" Level 1: E Level 2: ED Level 3: EDDE Level 4: EDDEDEED Level 5: EDDEDEEDDEEDEDDE Level input is not necessary because first elements are the same The result is based on the count of 1's in binary representation of position-1 If position is even, then Engineer; Else Doctor \"\"\" bits = bin ( pos - 1 ) . count ( '1' ) if bits % 2 == 0 : return \"Engineer\" else : return \"Doctor\" Kth Smallest in BST 30 min - Medium Note: Your solution should have only one BST traversal and O(1) extra space complexity, since this is what you will be asked to accomplish in an interview. A tree is considered a binary search tree (BST) if for each of its nodes the following is true: The left subtree of a node contains only nodes with keys less than the node's key. The right subtree of a node contains only nodes with keys greater than the node's key. Both the left and the right subtrees must also be binary search trees. Given a binary search tree t, find the kth smallest element in it. Note that kth smallest element means kth element in increasing order. See examples for better understanding. Example For t = { \"value\": 3, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": { \"value\": 5, \"left\": { \"value\": 4, \"left\": null, \"right\": null }, \"right\": { \"value\": 6, \"left\": null, \"right\": null } } } and k = 4, the output should be solution(t, k) = 5. Here is what t looks like: 3 / \\ 1 5 / \\ 4 6 The values of t are [1, 3, 4, 5, 6], and the 4th smallest is 5. For t = { \"value\": 1, \"left\": { \"value\": -1, \"left\": { \"value\": -2, \"left\": null, \"right\": null }, \"right\": { \"value\": 0, \"left\": null, \"right\": null } }, \"right\": null } and k = 1, the output should be solution(t, k) = -2. Here is what t looks like: 1 / -1 / \\ -2 0 The values of t are [-2, -1, 0, 1], and the 1st smallest is -2. Solution # # Definition for binary tree: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None def solution ( root , K ): def dfs ( node ): if node : yield from dfs ( node . left ) yield node . value yield from dfs ( node . right ) f = dfs ( root ) for _ in range ( K ): ans = next ( f ) return ans # # Definition for binary tree: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None def solution ( t , k ): # Morris in-order traversal (O(1) memory usage) # https://www.youtube.com/watch?v=wGXB9OWhPTg explains this pretty well current = t while current : if not current . left : k -= 1 if k == 0 : return current . value current = current . right else : predecessor = find_predecessor ( current ) if not predecessor . right : predecessor . right = current current = current . left else : predecessor . right = None k -= 1 if k == 0 : return current . value current = current . right return None def find_predecessor ( t ): # Go left once, then right as much times as possible predecessor = t . left while predecessor . right and predecessor . right != t : predecessor = predecessor . right return predecessor Is Subtree 30 min - Medium Given two binary trees t1 and t2, determine whether the second tree is a subtree of the first tree. A subtree for vertex v in a binary tree t is a tree consisting of v and all its descendants in t. Determine whether or not there is a vertex v (possibly none) in tree t1 such that a subtree for vertex v (possibly empty) in t1 equals t2. Example For t1 = { \"value\": 5, \"left\": { \"value\": 10, \"left\": { \"value\": 4, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": { \"value\": 2, \"left\": null, \"right\": null } }, \"right\": { \"value\": 6, \"left\": null, \"right\": { \"value\": -1, \"left\": null, \"right\": null } } }, \"right\": { \"value\": 7, \"left\": null, \"right\": null } } and t2 = { \"value\": 10, \"left\": { \"value\": 4, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": { \"value\": 2, \"left\": null, \"right\": null } }, \"right\": { \"value\": 6, \"left\": null, \"right\": { \"value\": -1, \"left\": null, \"right\": null } } } the output should be solution(t1, t2) = true. This is what these trees look like: t1: t2: 5 10 / \\ / \\ 10 7 4 6 / \\ / \\ \\ 4 6 1 2 -1 / \\ \\ 1 2 -1 As you can see, t2 is a subtree of t1 (the vertex in t1 with value 10). For t1 = { \"value\": 5, \"left\": { \"value\": 10, \"left\": { \"value\": 4, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": { \"value\": 2, \"left\": null, \"right\": null } }, \"right\": { \"value\": 6, \"left\": { \"value\": -1, \"left\": null, \"right\": null }, \"right\": null } }, \"right\": { \"value\": 7, \"left\": null, \"right\": null } } and t2 = { \"value\": 10, \"left\": { \"value\": 4, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": { \"value\": 2, \"left\": null, \"right\": null } }, \"right\": { \"value\": 6, \"left\": null, \"right\": { \"value\": -1, \"left\": null, \"right\": null } } } the output should be solution(t1, t2) = false. This is what these trees look like: t1: t2: 5 10 / \\ / \\ 10 7 4 6 / \\ / \\ \\ 4 6 1 2 -1 / \\ / 1 2 -1 As you can see, there is no vertex v such that the subtree of t1 for vertex v equals t2. For t1 = { \"value\": 1, \"left\": { \"value\": 2, \"left\": null, \"right\": null }, \"right\": { \"value\": 2, \"left\": null, \"right\": null } } and t2 = { \"value\": 2, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": null } the output should be solution(t1, t2) = false. Solution # # Definition for binary tree: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None from functools import reduce def solution ( t1 , t2 ): if not t2 : return True if not t1 : return False return equals ( t1 , t2 ) or solution ( t1 . left , t2 ) or solution ( t1 . right , t2 ) def equals ( t1 , t2 ): if not t1 and not t2 : return True if not t1 or not t2 : return False if t1 . value == t2 . value : return equals ( t1 . left , t2 . left ) and equals ( t1 . right , t2 . right ) return False Restore Binary Tree 40 min - Hard Note: Your solution should have O(inorder.length) time complexity, since this is what you will be asked to accomplish in an interview. Let's define inorder and preorder traversals of a binary tree as follows: Inorder traversal first visits the left subtree, then the root, then its right subtree; Preorder traversal first visits the root, then its left subtree, then its right subtree. For example, if tree looks like this: 1 / \\ 2 3 / / \\ 4 5 6 then the traversals will be as follows: Inorder traversal: [4, 2, 1, 5, 3, 6] Preorder traversal: [1, 2, 4, 3, 5, 6] Given the inorder and preorder traversals of a binary tree t, but not t itself, restore t and return it. Example For inorder = [4, 2, 1, 5, 3, 6] and preorder = [1, 2, 4, 3, 5, 6], the output should be solution(inorder, preorder) = { \"value\": 1, \"left\": { \"value\": 2, \"left\": { \"value\": 4, \"left\": null, \"right\": null }, \"right\": null }, \"right\": { \"value\": 3, \"left\": { \"value\": 5, \"left\": null, \"right\": null }, \"right\": { \"value\": 6, \"left\": null, \"right\": null } } } For inorder = [2, 5] and preorder = [5, 2], the output should be solution(inorder, preorder) = { \"value\": 5, \"left\": { \"value\": 2, \"left\": null, \"right\": null }, \"right\": null } Solution # # Definition for binary tree: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None def solution ( inorder , preorder ): if not preorder : return None root = Tree ( preorder [ 0 ]) i = inorder . index ( preorder [ 0 ]) root . left = solution ( inorder [: i ], preorder [ 1 : i + 1 ]) root . right = solution ( inorder [ i + 1 :], preorder [ i + 1 :]) return root Find Substrings Asked by Uber - 40 min - Hard You have two arrays of strings, words and parts. Return an array that contains the strings from words, modified so that any occurrences of the substrings from parts are surrounded by square brackets [], following these guidelines: If several parts substrings occur in one string in words, choose the longest one. If there is still more than one such part, then choose the one that appears first in the string. Example For words = [\"Apple\", \"Melon\", \"Orange\", \"Watermelon\"] and parts = [\"a\", \"mel\", \"lon\", \"el\", \"An\"], the output should be solution(words, parts) = [\"Apple\", \"Me[lon]\", \"Or[a]nge\", \"Water[mel]on\"]. While \"Watermelon\" contains three substrings from the parts array, \"a\", \"mel\", and \"lon\", \"mel\" is the longest substring that appears first in the string. Solution class Trie ( object ): def __init__ ( self ): self . nxt = {} self . end = False def add ( self , word ): if not word : self . end = True else : self . nxt . setdefault ( word [ 0 ], Trie ()) . add ( word [ 1 :]) def solution ( words , parts ): trie = Trie () for x in parts : trie . add ( x ) for i , w in enumerate ( words ): pos = len ( w ) L = - 1 for j in range ( len ( w )): t = trie k = j while k < len ( w ) and w [ k ] in t . nxt : t = t . nxt [ w [ k ]] k += 1 if t . end and k - j > L : L = k - j pos = j if L > 0 : words [ i ] = \" %s [ %s ] %s \" % ( w [: pos ], w [ pos : pos + L ], w [ pos + L :]) return words Delete From BST 30 min - Medium A tree is considered a binary search tree (BST) if for each of its nodes the following is true: The left subtree of a node contains only nodes with keys less than the node's key. The right subtree of a node contains only nodes with keys greater than the node's key. Both the left and the right subtrees must also be binary search trees. Removing a value x from a BST t is done in the following way: If there is no x in t, nothing happens; Otherwise, let t' be a subtree of t such that t'.value = x. If t' has a left subtree, remove the rightmost node from it and put it at the root of t'; Otherwise, remove the root of t' and its right subtree becomes the new t's root. For example, removing 4 from the following tree has no effect because there is no such value in the tree: 5 / \\ 2 6 / \\ \\ 1 3 8 / 7 Removing 5 causes 3 (the rightmost node in left subtree) to move to the root: 3 / \\ 2 6 / \\ 1 8 / 7 And removing 6 after that creates the following tree: 3 / \\ 2 8 / / 1 7 You're given a binary search tree t and an array of numbers queries. Your task is to remove queries[0], queries[1], etc., from t, step by step, following the algorithm above. Return the resulting BST. Example For t = { \"value\": 5, \"left\": { \"value\": 2, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": { \"value\": 3, \"left\": null, \"right\": null } }, \"right\": { \"value\": 6, \"left\": null, \"right\": { \"value\": 8, \"left\": { \"value\": 7, \"left\": null, \"right\": null }, \"right\": null } } } and queries = [4, 5, 6], the output should be solution(t, queries) = { \"value\": 3, \"left\": { \"value\": 2, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": null }, \"right\": { \"value\": 8, \"left\": { \"value\": 7, \"left\": null, \"right\": null }, \"right\": null } } Solution # # Definition for binary tree: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None def solution ( t , queries ): def max_of_tree ( t ): if t is None : return ( None ) while t . right is not None : t = t . right return ( t . value ) def remove_right ( t ): if t . right is None : return ( t . left ) else : t . right = remove_right ( t . right ) return ( t ) def f1 ( t , q ): if t is None : return ( None ) if q == t . value : if t . left : t . value = max_of_tree ( t . left ) t . left = remove_right ( t . left ) else : t = t . right elif q < t . value : t . left = f1 ( t . left , q ) elif q > t . value : t . right = f1 ( t . right , q ) return ( t ) for q in queries : t = f1 ( t , q ) return ( t )","title":"Data Structures"},{"location":"Exe/pyIntPractice_DS/#data-structures","text":"","title":"Data Structures"},{"location":"Exe/pyIntPractice_DS/#arrays","text":"","title":"Arrays"},{"location":"Exe/pyIntPractice_DS/#first-duplicate","text":"Asked by Google - 15 min - Easy Given an array a that contains only numbers in the range from 1 to a.length, find the first duplicate number for which the second occurrence has the minimal index. In other words, if there are more than 1 duplicated numbers, return the number for which the second occurrence has a smaller index than the second occurrence of the other number does. If there are no such elements, return -1. Example For a = [2, 1, 3, 5, 3, 2], the output should be solution(a) = 3. There are 2 duplicates: numbers 2 and 3. The second occurrence of 3 has a smaller index than the second occurrence of 2 does, so the answer is 3. For a = [2, 2], the output should be solution(a) = 2; For a = [2, 4, 3, 5, 1], the output should be solution(a) = -1. Solutions Solution 1 def solution ( a ): seen = set () for i in a : if i in seen : return i seen . add ( i ) return - 1 Solution 2 def solution ( a ): for i in a : a [ abs ( i ) - 1 ] *= - 1 if a [ abs ( i ) - 1 ] > 0 : return abs ( i ) return - 1","title":"First Duplicate"},{"location":"Exe/pyIntPractice_DS/#first-not-repeating-character","text":"Asked by Amazon - 15 min - Easy Given a string s consisting of small English letters, find and return the first instance of a non-repeating character in it. If there is no such character, return '_'. Example For s = \"abacabad\", the output should be solution(s) = 'c'. There are 2 non-repeating characters in the string: 'c' and 'd'. Return c since it appears in the string first. For s = \"abacabaabacaba\", the output should be solution(s) = '_'. There are no characters in this string that do not repeat. from collections import Counter def solution ( s ): st = Counter ( s ) print ( st . keys ()) for i in st . keys (): print ( i ) if st [ i ] == 1 : return i return '_' def solution ( s ): chk = [] for i in range ( len ( s )): if s [ i ] not in s [ i + 1 :] and s [ i ] not in chk : return s [ i ] chk . append ( s [ i ]) return '_'","title":"First Not Repeating Character"},{"location":"Exe/pyIntPractice_DS/#rotate-image","text":"Asked by Amazon, Microsoft, and Apple - 15 min - Easy Note: Try to solve this task in-place (with O(1) additional memory), since this is what you'll be asked to do during an interview. You are given an n x n 2D matrix that represents an image. Rotate the image by 90 degrees (clockwise). Example For a = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] the output should be solution(a) = [[7, 4, 1], [8, 5, 2], [9, 6, 3]] Solutions def solution ( a ): a = list ( zip ( * a )) a = [ list ( l [:: - 1 ]) for l in a ] return a def solution ( a ): return list ( zip ( * reversed ( a )))","title":"Rotate Image"},{"location":"Exe/pyIntPractice_DS/#sudoku-2","text":"Asked by Apple and Uber - 30 min - Easy Sudoku is a number-placement puzzle. The objective is to fill a 9 \u00d7 9 grid with numbers in such a way that each column, each row, and each of the nine 3 \u00d7 3 sub-grids that compose the grid all contain all of the numbers from 1 to 9 one time. Implement an algorithm that will check whether the given grid of numbers represents a valid Sudoku puzzle according to the layout rules described above. Note that the puzzle represented by grid does not have to be solvable. Example For grid = [['.', '.', '.', '1', '4', '.', '.', '2', '.'], ['.', '.', '6', '.', '.', '.', '.', '.', '.'], ['.', '.', '.', '.', '.', '.', '.', '.', '.'], ['.', '.', '1', '.', '.', '.', '.', '.', '.'], ['.', '6', '7', '.', '.', '.', '.', '.', '9'], ['.', '.', '.', '.', '.', '.', '8', '1', '.'], ['.', '3', '.', '.', '.', '.', '.', '.', '6'], ['.', '.', '.', '.', '.', '7', '.', '.', '.'], ['.', '.', '.', '5', '.', '.', '.', '7', '.']] the output should be solution(grid) = true; For grid = [['.', '.', '.', '.', '2', '.', '.', '9', '.'], ['.', '.', '.', '.', '6', '.', '.', '.', '.'], ['7', '1', '.', '.', '7', '5', '.', '.', '.'], ['.', '7', '.', '.', '.', '.', '.', '.', '.'], ['.', '.', '.', '.', '8', '3', '.', '.', '.'], ['.', '.', '8', '.', '.', '7', '.', '6', '.'], ['.', '.', '.', '.', '.', '2', '.', '.', '.'], ['.', '1', '.', '2', '.', '.', '.', '.', '.'], ['.', '2', '.', '.', '3', '.', '.', '.', '.']] the output should be solution(grid) = false. The given grid is not correct because there are two 1s in the second column. Each column, each row, and each 3 \u00d7 3 subgrid can only contain the numbers 1 through 9 one time. Solution def line_is_valid ( row ): tmp_dict = {} valid = True if len ( row ) > 0 : #print(row) for r in row : if r in tmp_dict : tmp_dict [ r ] += 1 else : tmp_dict [ r ] = 1 if tmp_dict [ max ( tmp_dict , key = tmp_dict . get )] > 1 : valid = False return valid def grid_is_valid ( grid ): valid = True for i in range ( len ( grid )): row = [ int ( x ) for x in grid [ i ] if x != '.' ] #print(row) valid = line_is_valid ( row ) if not valid : break return valid def solution ( grid ): result = False # check rows: rows_valid = grid_is_valid ( grid ) # check columns: col_valid = grid_is_valid ( list ( zip ( * grid ))) # check 3x3 grids: grid3x3_valid = True for i in range ( 0 , len ( grid ), 3 ): if not grid3x3_valid : break sub_mat = grid [ i : i + 3 ] for j in range ( 0 , len ( grid [ 0 ]), 3 ): tmp_list = [ x [ j : j + 3 ] for x in sub_mat ] #print(tmp_list) tmp_list = [ tmp_list [ k ][ l ] for k in range ( len ( tmp_list )) for l in range ( len ( tmp_list [ k ]))] row = [ int ( x ) for x in tmp_list if x != '.' ] grid3x3_valid = line_is_valid ( row ) #print(grid3x3_valid) if not grid3x3_valid : break if rows_valid and col_valid and grid3x3_valid : result = True return result","title":"Sudoku 2"},{"location":"Exe/pyIntPractice_DS/#is-crypt-solution","text":"Asked by Palantir Solutions - 15 min - Easy A cryptarithm is a mathematical puzzle for which the goal is to find the correspondence between letters and digits, such that the given arithmetic equation consisting of letters holds true when the letters are converted to digits. You have an array of strings crypt, the cryptarithm, and an an array containing the mapping of letters and digits, solution. The array crypt will contain three non-empty strings that follow the structure: [word1, word2, word3], which should be interpreted as the word1 + word2 = word3 cryptarithm. If crypt, when it is decoded by replacing all of the letters in the cryptarithm with digits using the mapping in solution, becomes a valid arithmetic equation containing no numbers with leading zeroes, the answer is true. If it does not become a valid arithmetic solution, the answer is false. Note that number 0 doesn't contain leading zeroes (while for example 00 or 0123 do). Example For crypt = [\"SEND\", \"MORE\", \"MONEY\"] and solution = [['O', '0'], ['M', '1'], ['Y', '2'], ['E', '5'], ['N', '6'], ['D', '7'], ['R', '8'], ['S', '9']] the output should be solution(crypt, solution) = true. When you decrypt \"SEND\", \"MORE\", and \"MONEY\" using the mapping given in crypt, you get 9567 + 1085 = 10652 which is correct and a valid arithmetic equation. For crypt = [\"TEN\", \"TWO\", \"ONE\"] and solution = [['O', '1'], ['T', '0'], ['W', '9'], ['E', '5'], ['N', '4']] the output should be solution(crypt, solution) = false. Even though 054 + 091 = 145, 054 and 091 both contain leading zeroes, meaning that this is not a valid solution. Solution def solution ( crypt , solution ): # a + b = c a = crypt [ 0 ] b = crypt [ 1 ] c = crypt [ 2 ] ch_dict = {} for s in solution : ch_dict [ s [ 0 ]] = int ( s [ 1 ]) hasLeadingZeros = False if ch_dict [ a [ 0 ]] == 0 or ch_dict [ b [ 0 ]] == 0 or ch_dict [ c [ 0 ]] == 0 : hasLeadingZeros = True n1 = '' . join ([ str ( ch_dict [ i ]) for i in a ]) n2 = '' . join ([ str ( ch_dict [ i ]) for i in b ]) n3 = '' . join ([ str ( ch_dict [ i ]) for i in c ]) if int ( n1 ) + int ( n2 ) == int ( n3 ): if hasLeadingZeros and int ( n3 ) == 0 and len ( n3 ) == len ( str ( int ( n3 ))): return True elif hasLeadingZeros : return False else : return True else : return False","title":"Is Crypt Solution"},{"location":"Exe/pyIntPractice_DS/#linkedlists","text":"","title":"LinkedLists"},{"location":"Exe/pyIntPractice_DS/#remove-k-from-list","text":"15 min - Easy Note: Try to solve this task in O(n) time using O(1) additional space, where n is the number of elements in the list, since this is what you'll be asked to do during an interview. Given a singly linked list of integers l and an integer k, remove all elements from list l that have a value equal to k. Example For l = [3, 1, 2, 3, 4, 5] and k = 3, the output should be solution(l, k) = [1, 2, 4, 5]; For l = [1, 2, 3, 4, 5, 6, 7] and k = 10, the output should be solution(l, k) = [1, 2, 3, 4, 5, 6, 7]. Solution # Singly-linked lists are already defined with this interface: # class ListNode(object): # def __init__(self, x): # self.value = x # self.next = None # def solution ( l , k ): if l == None : return l while l != None and l . value == k : l = l . next n = l while n != None and n . next != None : if n . next . value == k : n . next = n . next . next else : n = n . next return l","title":"Remove K from List"},{"location":"Exe/pyIntPractice_DS/#is-list-palindrome","text":"Asked by Amazon and Meta - 30 min - Easy Note: Try to solve this task in O(n) time using O(1) additional space, where n is the number of elements in l, since this is what you'll be asked to do during an interview. Given a singly linked list of integers, determine whether or not it's a palindrome. Note: in examples below and tests preview linked lists are presented as arrays just for simplicity of visualization: in real data you will be given a head node l of the linked list Example For l = [0, 1, 0], the output should be solution(l) = true; For l = [1, 2, 2, 3], the output should be solution(l) = false. Solutions # Definition for singly-linked list: # class ListNode(object): # def __init__(self, x): # self.value = x # self.next = None # def solution ( l ): a = [] while l != None : a . append ( l . value ) l = l . next return a == a [:: - 1 ] # Definition for singly-linked list: # class ListNode(object): # def __init__(self, x): # self.value = x # self.next = None # def solution ( l ): if not l or not l . next : return True s = 1 n = l while n . next : n = n . next s += 1 middle = s // 2 n = l for i in range ( middle ): n = n . next if s % 2 : n = n . next r = n # reverse n m = r . next for _ in range ( middle - 1 ): # flip n m . next , r , m = r , m , m . next for _ in range ( middle ): if r . value != l . value : return False r = r . next l = l . next return True","title":"Is List Palindrome"},{"location":"Exe/pyIntPractice_DS/#add-two-huge-numbers","text":"30 min - Easy You're given 2 huge integers represented by linked lists. Each linked list element is a number from 0 to 9999 that represents a number with exactly 4 digits. The represented number might have leading zeros. Your task is to add up these huge integers and return the result in the same format. Example For a = [9876, 5432, 1999] and b = [1, 8001], the output should be solution(a, b) = [9876, 5434, 0]. Explanation: 987654321999 + 18001 = 987654340000. For a = [123, 4, 5] and b = [100, 100, 100], the output should be solution(a, b) = [223, 104, 105]. Explanation: 12300040005 + 10001000100 = 22301040105. # Definition for singly-linked list: # class ListNode(object): # def __init__(self, x): # self.value = x # self.next = None # def solution ( a , b ): a = reverse ( a ) b = reverse ( b ) carry = 0 result = None while a is not None or b is not None or carry > 0 : raw = (( a . value if a is not None else 0 ) + ( b . value if b is not None else 0 ) + carry ) node = ListNode ( raw % 10000 ) node . next = result result = node carry = raw // 10000 if a is not None : a = a . next if b is not None : b = b . next return result def reverse ( list ): current = list previous = None while current is not None : previous , current . next , current = current , previous , current . next return previous","title":"Add Two Huge Numbers"},{"location":"Exe/pyIntPractice_DS/#merge-two-linkedlists","text":"30 min - Medium Note: Your solution should have O(l1.length + l2.length) time complexity, since this is what you will be asked to accomplish in an interview. Given two singly linked lists sorted in non-decreasing order, your task is to merge them. In other words, return a singly linked list, also sorted in non-decreasing order, that contains the elements from both original lists. Example For l1 = [1, 2, 3] and l2 = [4, 5, 6], the output should be solution(l1, l2) = [1, 2, 3, 4, 5, 6]; For l1 = [1, 1, 2, 4] and l2 = [0, 3, 5], the output should be solution(l1, l2) = [0, 1, 1, 2, 3, 4, 5]. Solution # Singly-linked lists are already defined with this interface: # class ListNode(object): # def __init__(self, x): # self.value = x # self.next = None # # Create & Handle List operations class LinkedList : def __init__ ( self ): self . head = None # Method to display the list def printList ( self ): temp = self . head while temp : print ( temp . value , end = \" \" ) temp = temp . next # Method to add element to list def addToList ( self , newData ): newNode = ListNode ( newData ) if self . head is None : self . head = newNode return last = self . head while last . next : last = last . next last . next = newNode # Function to merge the lists # Takes two lists which are sorted # joins them to get a single sorted list def solution ( headA , headB ): # A dummy node to store the result dummyNode = ListNode ( 0 ) # Tail stores the last node tail = dummyNode while True : # If any of the list gets completely empty # directly join all the elements of the other list if headA is None : tail . next = headB break if headB is None : tail . next = headA break # Compare the data of the lists and whichever is smaller is # appended to the last of the merged list and the head is changed if headA . value <= headB . value : tail . next = headA headA = headA . next else : tail . next = headB headB = headB . next # Advance the tail tail = tail . next # Returns the head of the merged list return dummyNode . next","title":"Merge Two LinkedLists"},{"location":"Exe/pyIntPractice_DS/#reverse-nodes-in-k-groups","text":"45 min - Hard Note: Your solution should have O(n) time complexity, where n is the number of elements in l, and O(1) additional space complexity, since this is what you would be asked to accomplish in an interview. Given a linked list l, reverse its nodes k at a time and return the modified list. k is a positive integer that is less than or equal to the length of l. If the number of nodes in the linked list is not a multiple of k, then the nodes that are left out at the end should remain as-is. You may not alter the values in the nodes - only the nodes themselves can be changed. Example For l = [1, 2, 3, 4, 5] and k = 2, the output should be solution(l, k) = [2, 1, 4, 3, 5]; For l = [1, 2, 3, 4, 5] and k = 1, the output should be solution(l, k) = [1, 2, 3, 4, 5]; For l = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] and k = 3, the output should be solution(l, k) = [3, 2, 1, 6, 5, 4, 9, 8, 7, 10, 11]. Solution # Singly-linked lists are already defined with this interface: # class ListNode(object): # def __init__(self, x): # self.value = x # self.next = None # def solution ( l , k ): if not l : return l c = l n = k while c and n : c = c . next n -= 1 if n : return l curr = l prev = None next = None n = k while curr and n : next = curr . next curr . next = prev prev = curr curr = next n -= 1 if next : l . next = solution ( next , k ) return prev","title":"Reverse Nodes in K Groups"},{"location":"Exe/pyIntPractice_DS/#rearrange-last-n","text":"40 min - Hard Note: Try to solve this task in O(list size) time using O(1) additional space, since this is what you'll be asked during an interview. Given a singly linked list of integers l and a non-negative integer n, move the last n list nodes to the beginning of the linked list. Example For l = [1, 2, 3, 4, 5] and n = 3, the output should be solution(l, n) = [3, 4, 5, 1, 2]; For l = [1, 2, 3, 4, 5, 6, 7] and n = 1, the output should be solution(l, n) = [7, 1, 2, 3, 4, 5, 6]. Solution # Singly-linked lists are already defined with this interface: # class ListNode(object): # def __init__(self, x): # self.value = x # self.next = None # def solution ( l , n ): if n == 0 : return l front , back = l , l for _ in range ( n ): front = front . next if not front : return l while front . next : front = front . next back = back . next out = back . next back . next = None front . next = l return out","title":"Rearrange Last N"},{"location":"Exe/pyIntPractice_DS/#hashtables","text":"","title":"Hashtables"},{"location":"Exe/pyIntPractice_DS/#grouping-dishes","text":"Asked by Linkedin - 20 min - Easy You are given a list dishes, where each element consists of a list of strings beginning with the name of the dish, followed by all the ingredients used in preparing it. You want to group the dishes by ingredients, so that for each ingredient you'll be able to find all the dishes that contain it (if there are at least 2 such dishes). Return an array where each element is a list beginning with the ingredient name, followed by the names of all the dishes that contain this ingredient. The dishes inside each list should be sorted alphabetically, and the result array should be sorted alphabetically by the names of the ingredients. Example For dishes = [[ \"Salad\" , \"Tomato\" , \"Cucumber\" , \"Salad\" , \"Sauce\" ], [ \"Pizza\" , \"Tomato\" , \"Sausage\" , \"Sauce\" , \"Dough\" ], [ \"Quesadilla\" , \"Chicken\" , \"Cheese\" , \"Sauce\" ], [ \"Sandwich\" , \"Salad\" , \"Bread\" , \"Tomato\" , \"Cheese\" ]] the output should be solution ( dishes ) = [[ \"Cheese\" , \"Quesadilla\" , \"Sandwich\" ], [ \"Salad\" , \"Salad\" , \"Sandwich\" ], [ \"Sauce\" , \"Pizza\" , \"Quesadilla\" , \"Salad\" ], [ \"Tomato\" , \"Pizza\" , \"Salad\" , \"Sandwich\" ]] For dishes = [[ \"Pasta\" , \"Tomato Sauce\" , \"Onions\" , \"Garlic\" ], [ \"Chicken Curry\" , \"Chicken\" , \"Curry Sauce\" ], [ \"Fried Rice\" , \"Rice\" , \"Onions\" , \"Nuts\" ], [ \"Salad\" , \"Spinach\" , \"Nuts\" ], [ \"Sandwich\" , \"Cheese\" , \"Bread\" ], [ \"Quesadilla\" , \"Chicken\" , \"Cheese\" ]] the output should be solution ( dishes ) = [[ \"Cheese\" , \"Quesadilla\" , \"Sandwich\" ], [ \"Chicken\" , \"Chicken Curry\" , \"Quesadilla\" ], [ \"Nuts\" , \"Fried Rice\" , \"Salad\" ], [ \"Onions\" , \"Fried Rice\" , \"Pasta\" ]] Solution def solution ( dishes ): groups = {} for d , * v in dishes : for x in v : groups . setdefault ( x , []) . append ( d ) ans = [] for x in sorted ( groups ): if len ( groups [ x ]) >= 2 : ans . append ([ x ] + sorted ( groups [ x ])) return ans","title":"Grouping Dishes"},{"location":"Exe/pyIntPractice_DS/#are-following-patterns","text":"Asked by Google - 30 min - Easy Given an array strings, determine whether it follows the sequence given in the patterns array. In other words, there should be no i and j for which strings[i] = strings[j] and patterns[i] \u2260 patterns[j] or for which strings[i] \u2260 strings[j] and patterns[i] = patterns[j]. Example For strings = [\"cat\", \"dog\", \"dog\"] and patterns = [\"a\", \"b\", \"b\"], the output should be solution(strings, patterns) = true; For strings = [\"cat\", \"dog\", \"doggy\"] and patterns = [\"a\", \"b\", \"b\"], the output should be solution(strings, patterns) = false. Solution def solution ( strings , patterns ): return len ( set ( strings )) == len ( set ( patterns )) == len ( set ( zip ( strings , patterns )))","title":"Are Following Patterns"},{"location":"Exe/pyIntPractice_DS/#contains-close-nuns","text":"Asked by Palantir - 30 min - Medium Given an array of integers nums and an integer k, determine whether there are two distinct indices i and j in the array where nums[i] = nums[j] and the absolute difference between i and j is less than or equal to k. Example For nums = [0, 1, 2, 3, 5, 2] and k = 3, the output should be solution(nums, k) = true. There are two 2s in nums, and the absolute difference between their positions is exactly 3. For nums = [0, 1, 2, 3, 5, 2] and k = 2, the output should be solution(nums, k) = false. The absolute difference between the positions of the two 2s is 3, which is more than k. Solution def solution ( nums , k ): dct = {} #key:number, value:index contains the last occurance index of a number if not nums or k == 0 : return False for index , num in enumerate ( nums ): try : if index - dct [ num ] <= k : return True except : pass dct [ num ] = index return False","title":"Contains Close Nuns"},{"location":"Exe/pyIntPractice_DS/#possible-sums","text":"Asked by Google - 45 min - Hard You have a collection of coins, and you know the values of the coins and the quantity of each type of coin in it. You want to know how many distinct sums you can make from non-empty groupings of these coins. Example For coins = [10, 50, 100] and quantity = [1, 2, 1], the output should be solution(coins, quantity) = 9. Here are all the possible sums: 50 = 50; 10 + 50 = 60; 50 + 100 = 150; 10 + 50 + 100 = 160; 50 + 50 = 100; 10 + 50 + 50 = 110; 50 + 50 + 100 = 200; 10 + 50 + 50 + 100 = 210; 10 = 10; 100 = 100; 10 + 100 = 110. As you can see, there are 9 distinct sums that can be created from non-empty groupings of your coins. Solution def solution ( coins , quantity ): possible_sums = { 0 } for c , q in zip ( coins , quantity ): possible_sums = { x + c * i for x in possible_sums for i in range ( q + 1 )} return len ( possible_sums ) - 1","title":"Possible Sums"},{"location":"Exe/pyIntPractice_DS/#swap-lex-order","text":"Asked by Meta - 45 min - Hard Given a string str and array of pairs that indicates which indices in the string can be swapped, return the lexicographically largest string that results from doing the allowed swaps. You can swap indices any number of times. Example For str = \"abdc\" and pairs = [[1, 4], [3, 4]], the output should be solution(str, pairs) = \"dbca\". By swapping the given indices, you get the strings: \"cbda\", \"cbad\", \"dbac\", \"dbca\". The lexicographically largest string in this list is \"dbca\". Solution def solution ( strn , pairs ): # if not strn or not pairs : return strn # check for connected node groupings which can be sorted individually grp = {} # set of all possible locations an index could end up for a , b in pairs : g = grp . get ( a ,{ a }) | grp . get ( b ,{ b }) for n in g : # reset all nodes in group grp [ n ] = g for n in grp : grp [ n ] = tuple ( sorted ( grp [ n ])) reord = {} for c in set ( grp . values ()) : word = sorted (( strn [ i - 1 ] for i in c ), reverse = True ) for i , l in zip ( c , word ) : reord [ i - 1 ] = l # string is 0 indexed return '' . join ( reord . get ( i , x ) for i , x in enumerate ( strn ))","title":"Swap Lex Order"},{"location":"Exe/pyIntPractice_DS/#trees-basic","text":"","title":"Trees: Basic"},{"location":"Exe/pyIntPractice_DS/#has-path-with-given-sum","text":"20 min - Easy Given a binary tree t and an integer s, determine whether there is a root to leaf path in t such that the sum of vertex values equals s. Example For t = { \"value\": 4, \"left\": { \"value\": 1, \"left\": { \"value\": -2, \"left\": null, \"right\": { \"value\": 3, \"left\": null, \"right\": null } }, \"right\": null }, \"right\": { \"value\": 3, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": { \"value\": 2, \"left\": { \"value\": -2, \"left\": null, \"right\": null }, \"right\": { \"value\": -3, \"left\": null, \"right\": null } } } } and s = 7, the output should be solution(t, s) = true. This is what this tree looks like: 4 / \\ 1 3 / / \\ -2 1 2 \\ / \\ 3 -2 -3 Path 4 -> 3 -> 2 -> -2 gives us 7, the required sum. For t = { \"value\": 4, \"left\": { \"value\": 1, \"left\": { \"value\": -2, \"left\": null, \"right\": { \"value\": 3, \"left\": null, \"right\": null } }, \"right\": null }, \"right\": { \"value\": 3, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": { \"value\": 2, \"left\": { \"value\": -4, \"left\": null, \"right\": null }, \"right\": { \"value\": -3, \"left\": null, \"right\": null } } } } and s = 7, the output should be solution(t, s) = false. This is what this tree looks like: 4 / \\ 1 3 / / \\ -2 1 2 \\ / \\ 3 -4 -3 There is no path from root to leaf with the given sum 7. Solution # # Definition for binary tree: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None def solution ( t , s ): if t is None : return s == 0 return solution ( t . left , s - t . value ) or solution ( t . right , s - t . value )","title":"Has Path With Given Sum"},{"location":"Exe/pyIntPractice_DS/#is-tree-symmetric","text":"Asked by Linkedin and Microsoft - 30 min - Easy Given a binary tree t, determine whether it is symmetric around its center, i.e. each side mirrors the other. Example For t = { \"value\": 1, \"left\": { \"value\": 2, \"left\": { \"value\": 3, \"left\": null, \"right\": null }, \"right\": { \"value\": 4, \"left\": null, \"right\": null } }, \"right\": { \"value\": 2, \"left\": { \"value\": 4, \"left\": null, \"right\": null }, \"right\": { \"value\": 3, \"left\": null, \"right\": null } } } the output should be solution(t) = true. Here's what the tree in this example looks like: 1 / \\ 2 2 / \\ / \\ 3 4 4 3 As you can see, it is symmetric. For t = { \"value\": 1, \"left\": { \"value\": 2, \"left\": null, \"right\": { \"value\": 3, \"left\": null, \"right\": null } }, \"right\": { \"value\": 2, \"left\": null, \"right\": { \"value\": 3, \"left\": null, \"right\": null } } } the output should be solution(t) = false. Here's what the tree in this example looks like: 1 / \\ 2 2 \\ \\ 3 3 As you can see, it is not symmetric. Solutions # # Definition for binary tree: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None def solution ( t ): def mirrorEquals ( left , right ): if left is None or right is None : return left == None and right == None return left . value == right . value and \\ mirrorEquals ( left . left , right . right ) and \\ mirrorEquals ( left . right , right . left ) return mirrorEquals ( t , t ) def solution ( t ): def iTS ( le , ri ): if le is None and ri is None : return True elif le is None or ri is None : return False else : if le . value == ri . value : return iTS ( le . left , ri . right ) and iTS ( le . right , ri . left ) else : return False if t is None : return True else : return iTS ( t . left , t . right )","title":"Is Tree Symmetric"},{"location":"Exe/pyIntPractice_DS/#find-profession","text":"20 min - Easy Consider a special family of Engineers and Doctors. This family has the following rules: Everybody has two children. The first child of an Engineer is an Engineer and the second child is a Doctor. The first child of a Doctor is a Doctor and the second child is an Engineer. All generations of Doctors and Engineers start with an Engineer. We can represent the situation using this diagram: E / \\ E D / \\ / \\ E D D E / \\ / \\ / \\ / \\ E D D E D E E D Given the level and position of a person in the ancestor tree above, find the profession of the person. Note: in this tree first child is considered as left child, second - as right. Example For level = 3 and pos = 3, the output should be solution(level, pos) = \"Doctor\". Solution def solution ( level , pos ): \"\"\" Level 1: E Level 2: ED Level 3: EDDE Level 4: EDDEDEED Level 5: EDDEDEEDDEEDEDDE Level input is not necessary because first elements are the same The result is based on the count of 1's in binary representation of position-1 If position is even, then Engineer; Else Doctor \"\"\" bits = bin ( pos - 1 ) . count ( '1' ) if bits % 2 == 0 : return \"Engineer\" else : return \"Doctor\"","title":"Find Profession"},{"location":"Exe/pyIntPractice_DS/#kth-smallest-in-bst","text":"30 min - Medium Note: Your solution should have only one BST traversal and O(1) extra space complexity, since this is what you will be asked to accomplish in an interview. A tree is considered a binary search tree (BST) if for each of its nodes the following is true: The left subtree of a node contains only nodes with keys less than the node's key. The right subtree of a node contains only nodes with keys greater than the node's key. Both the left and the right subtrees must also be binary search trees. Given a binary search tree t, find the kth smallest element in it. Note that kth smallest element means kth element in increasing order. See examples for better understanding. Example For t = { \"value\": 3, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": { \"value\": 5, \"left\": { \"value\": 4, \"left\": null, \"right\": null }, \"right\": { \"value\": 6, \"left\": null, \"right\": null } } } and k = 4, the output should be solution(t, k) = 5. Here is what t looks like: 3 / \\ 1 5 / \\ 4 6 The values of t are [1, 3, 4, 5, 6], and the 4th smallest is 5. For t = { \"value\": 1, \"left\": { \"value\": -1, \"left\": { \"value\": -2, \"left\": null, \"right\": null }, \"right\": { \"value\": 0, \"left\": null, \"right\": null } }, \"right\": null } and k = 1, the output should be solution(t, k) = -2. Here is what t looks like: 1 / -1 / \\ -2 0 The values of t are [-2, -1, 0, 1], and the 1st smallest is -2. Solution # # Definition for binary tree: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None def solution ( root , K ): def dfs ( node ): if node : yield from dfs ( node . left ) yield node . value yield from dfs ( node . right ) f = dfs ( root ) for _ in range ( K ): ans = next ( f ) return ans # # Definition for binary tree: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None def solution ( t , k ): # Morris in-order traversal (O(1) memory usage) # https://www.youtube.com/watch?v=wGXB9OWhPTg explains this pretty well current = t while current : if not current . left : k -= 1 if k == 0 : return current . value current = current . right else : predecessor = find_predecessor ( current ) if not predecessor . right : predecessor . right = current current = current . left else : predecessor . right = None k -= 1 if k == 0 : return current . value current = current . right return None def find_predecessor ( t ): # Go left once, then right as much times as possible predecessor = t . left while predecessor . right and predecessor . right != t : predecessor = predecessor . right return predecessor","title":"Kth Smallest in BST"},{"location":"Exe/pyIntPractice_DS/#is-subtree","text":"30 min - Medium Given two binary trees t1 and t2, determine whether the second tree is a subtree of the first tree. A subtree for vertex v in a binary tree t is a tree consisting of v and all its descendants in t. Determine whether or not there is a vertex v (possibly none) in tree t1 such that a subtree for vertex v (possibly empty) in t1 equals t2. Example For t1 = { \"value\": 5, \"left\": { \"value\": 10, \"left\": { \"value\": 4, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": { \"value\": 2, \"left\": null, \"right\": null } }, \"right\": { \"value\": 6, \"left\": null, \"right\": { \"value\": -1, \"left\": null, \"right\": null } } }, \"right\": { \"value\": 7, \"left\": null, \"right\": null } } and t2 = { \"value\": 10, \"left\": { \"value\": 4, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": { \"value\": 2, \"left\": null, \"right\": null } }, \"right\": { \"value\": 6, \"left\": null, \"right\": { \"value\": -1, \"left\": null, \"right\": null } } } the output should be solution(t1, t2) = true. This is what these trees look like: t1: t2: 5 10 / \\ / \\ 10 7 4 6 / \\ / \\ \\ 4 6 1 2 -1 / \\ \\ 1 2 -1 As you can see, t2 is a subtree of t1 (the vertex in t1 with value 10). For t1 = { \"value\": 5, \"left\": { \"value\": 10, \"left\": { \"value\": 4, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": { \"value\": 2, \"left\": null, \"right\": null } }, \"right\": { \"value\": 6, \"left\": { \"value\": -1, \"left\": null, \"right\": null }, \"right\": null } }, \"right\": { \"value\": 7, \"left\": null, \"right\": null } } and t2 = { \"value\": 10, \"left\": { \"value\": 4, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": { \"value\": 2, \"left\": null, \"right\": null } }, \"right\": { \"value\": 6, \"left\": null, \"right\": { \"value\": -1, \"left\": null, \"right\": null } } } the output should be solution(t1, t2) = false. This is what these trees look like: t1: t2: 5 10 / \\ / \\ 10 7 4 6 / \\ / \\ \\ 4 6 1 2 -1 / \\ / 1 2 -1 As you can see, there is no vertex v such that the subtree of t1 for vertex v equals t2. For t1 = { \"value\": 1, \"left\": { \"value\": 2, \"left\": null, \"right\": null }, \"right\": { \"value\": 2, \"left\": null, \"right\": null } } and t2 = { \"value\": 2, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": null } the output should be solution(t1, t2) = false. Solution # # Definition for binary tree: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None from functools import reduce def solution ( t1 , t2 ): if not t2 : return True if not t1 : return False return equals ( t1 , t2 ) or solution ( t1 . left , t2 ) or solution ( t1 . right , t2 ) def equals ( t1 , t2 ): if not t1 and not t2 : return True if not t1 or not t2 : return False if t1 . value == t2 . value : return equals ( t1 . left , t2 . left ) and equals ( t1 . right , t2 . right ) return False","title":"Is Subtree"},{"location":"Exe/pyIntPractice_DS/#restore-binary-tree","text":"40 min - Hard Note: Your solution should have O(inorder.length) time complexity, since this is what you will be asked to accomplish in an interview. Let's define inorder and preorder traversals of a binary tree as follows: Inorder traversal first visits the left subtree, then the root, then its right subtree; Preorder traversal first visits the root, then its left subtree, then its right subtree. For example, if tree looks like this: 1 / \\ 2 3 / / \\ 4 5 6 then the traversals will be as follows: Inorder traversal: [4, 2, 1, 5, 3, 6] Preorder traversal: [1, 2, 4, 3, 5, 6] Given the inorder and preorder traversals of a binary tree t, but not t itself, restore t and return it. Example For inorder = [4, 2, 1, 5, 3, 6] and preorder = [1, 2, 4, 3, 5, 6], the output should be solution(inorder, preorder) = { \"value\": 1, \"left\": { \"value\": 2, \"left\": { \"value\": 4, \"left\": null, \"right\": null }, \"right\": null }, \"right\": { \"value\": 3, \"left\": { \"value\": 5, \"left\": null, \"right\": null }, \"right\": { \"value\": 6, \"left\": null, \"right\": null } } } For inorder = [2, 5] and preorder = [5, 2], the output should be solution(inorder, preorder) = { \"value\": 5, \"left\": { \"value\": 2, \"left\": null, \"right\": null }, \"right\": null } Solution # # Definition for binary tree: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None def solution ( inorder , preorder ): if not preorder : return None root = Tree ( preorder [ 0 ]) i = inorder . index ( preorder [ 0 ]) root . left = solution ( inorder [: i ], preorder [ 1 : i + 1 ]) root . right = solution ( inorder [ i + 1 :], preorder [ i + 1 :]) return root","title":"Restore Binary Tree"},{"location":"Exe/pyIntPractice_DS/#find-substrings","text":"Asked by Uber - 40 min - Hard You have two arrays of strings, words and parts. Return an array that contains the strings from words, modified so that any occurrences of the substrings from parts are surrounded by square brackets [], following these guidelines: If several parts substrings occur in one string in words, choose the longest one. If there is still more than one such part, then choose the one that appears first in the string. Example For words = [\"Apple\", \"Melon\", \"Orange\", \"Watermelon\"] and parts = [\"a\", \"mel\", \"lon\", \"el\", \"An\"], the output should be solution(words, parts) = [\"Apple\", \"Me[lon]\", \"Or[a]nge\", \"Water[mel]on\"]. While \"Watermelon\" contains three substrings from the parts array, \"a\", \"mel\", and \"lon\", \"mel\" is the longest substring that appears first in the string. Solution class Trie ( object ): def __init__ ( self ): self . nxt = {} self . end = False def add ( self , word ): if not word : self . end = True else : self . nxt . setdefault ( word [ 0 ], Trie ()) . add ( word [ 1 :]) def solution ( words , parts ): trie = Trie () for x in parts : trie . add ( x ) for i , w in enumerate ( words ): pos = len ( w ) L = - 1 for j in range ( len ( w )): t = trie k = j while k < len ( w ) and w [ k ] in t . nxt : t = t . nxt [ w [ k ]] k += 1 if t . end and k - j > L : L = k - j pos = j if L > 0 : words [ i ] = \" %s [ %s ] %s \" % ( w [: pos ], w [ pos : pos + L ], w [ pos + L :]) return words","title":"Find Substrings"},{"location":"Exe/pyIntPractice_DS/#delete-from-bst","text":"30 min - Medium A tree is considered a binary search tree (BST) if for each of its nodes the following is true: The left subtree of a node contains only nodes with keys less than the node's key. The right subtree of a node contains only nodes with keys greater than the node's key. Both the left and the right subtrees must also be binary search trees. Removing a value x from a BST t is done in the following way: If there is no x in t, nothing happens; Otherwise, let t' be a subtree of t such that t'.value = x. If t' has a left subtree, remove the rightmost node from it and put it at the root of t'; Otherwise, remove the root of t' and its right subtree becomes the new t's root. For example, removing 4 from the following tree has no effect because there is no such value in the tree: 5 / \\ 2 6 / \\ \\ 1 3 8 / 7 Removing 5 causes 3 (the rightmost node in left subtree) to move to the root: 3 / \\ 2 6 / \\ 1 8 / 7 And removing 6 after that creates the following tree: 3 / \\ 2 8 / / 1 7 You're given a binary search tree t and an array of numbers queries. Your task is to remove queries[0], queries[1], etc., from t, step by step, following the algorithm above. Return the resulting BST. Example For t = { \"value\": 5, \"left\": { \"value\": 2, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": { \"value\": 3, \"left\": null, \"right\": null } }, \"right\": { \"value\": 6, \"left\": null, \"right\": { \"value\": 8, \"left\": { \"value\": 7, \"left\": null, \"right\": null }, \"right\": null } } } and queries = [4, 5, 6], the output should be solution(t, queries) = { \"value\": 3, \"left\": { \"value\": 2, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": null }, \"right\": { \"value\": 8, \"left\": { \"value\": 7, \"left\": null, \"right\": null }, \"right\": null } } Solution # # Definition for binary tree: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None def solution ( t , queries ): def max_of_tree ( t ): if t is None : return ( None ) while t . right is not None : t = t . right return ( t . value ) def remove_right ( t ): if t . right is None : return ( t . left ) else : t . right = remove_right ( t . right ) return ( t ) def f1 ( t , q ): if t is None : return ( None ) if q == t . value : if t . left : t . value = max_of_tree ( t . left ) t . left = remove_right ( t . left ) else : t = t . right elif q < t . value : t . left = f1 ( t . left , q ) elif q > t . value : t . right = f1 ( t . right , q ) return ( t ) for q in queries : t = f1 ( t , q ) return ( t )","title":"Delete From BST"},{"location":"Exe/pyIntPractice_O/","text":"Other Topics Dynamic Programming: Basic Climbing Stairs Asked by Apple and Adobe - 15 min - Easy You are climbing a staircase that has n steps. You can take the steps either 1 or 2 at a time. Calculate how many distinct ways you can climb to the top of the staircase. Example For n = 1, the output should be solution(n) = 1; For n = 2, the output should be solution(n) = 2. You can either climb 2 steps at once or climb 1 step two times. Solution def solution ( n ): a , b = 1 , 0 for _ in range ( n ): a , b = a + b , a return a House Robber Asked by Linkedin - 20 min - Medium You are planning to rob houses on a specific street, and you know that every house on the street has a certain amount of money hidden. The only thing stopping you from robbing all of them in one night is that adjacent houses on the street have a connected security system. The system will automatically trigger an alarm if two adjacent houses are broken into on the same night. Given a list of non-negative integers nums representing the amount of money hidden in each house, determine the maximum amount of money you can rob in one night without triggering an alarm. Example For nums = [1, 1, 1], the output should be solution(nums) = 2. The optimal way to get the most money in one night is to rob the first and the third houses for a total of 2. Solutions def solution ( nums ): prev_max = 0 prev_prev_max = 0 for n in nums : prev_prev_max , prev_max = prev_max , max ( prev_prev_max + n , prev_max ) return prev_max from functools import lru_cache def solution ( nums ): return house ( tuple ( nums )) @lru_cache ( maxsize = 1000 ) def house ( nums ): if len ( nums ) == 0 : return 0 return max ( house ( nums [ 1 :]), house ( nums [ 2 :]) + nums [ 0 ]) Compose Ranges Asked by Google - 15 min - Easy Given a sorted integer array that does not contain any duplicates, return a summary of the number ranges it contains. Example For nums = [-1, 0, 1, 2, 6, 7, 9], the output should be solution(nums) = [\"-1->2\", \"6->7\", \"9\"]. Solution def solution ( nums ): ranges = [] while nums : start = end = nums . pop ( 0 ) while nums and nums [ 0 ] - end == 1 : end = nums . pop ( 0 ) ranges . append ( str ( start ) + ( '' , '->' + str ( end ))[ start != end ]) return ranges Map Decoding Asked by Microsoft, Uber and Meta - 30 min - Hard A top secret message containing uppercase letters from 'A' to 'Z' has been encoded as numbers using the following mapping: 'A' -> 1 'B' -> 2 ... 'Z' -> 26 You are an FBI agent and you need to determine the total number of ways that the message can be decoded. Since the answer could be very large, take it modulo 109 + 7. Example For message = \"123\", the output should be solution(message) = 3. \"123\" can be decoded as \"ABC\" (1 2 3), \"LC\" (12 3) or \"AW\" (1 23), so the total number of ways is 3. Solution def solution ( msg ): a , b = 1 , 0 M = 10 ** 9 + 7 for i in range ( len ( msg ) - 1 , - 1 , - 1 ): if msg [ i ] == \"0\" : a , b = 0 , a else : a , b = ( a + ( i + 2 <= len ( msg ) and msg [ i : i + 2 ] <= \"26\" ) * b ) % M , a return a Filling Blocks Asked by Uber - 30 min - Hard You have a block with the dimensions 4 \u00d7 n. Find the number of different ways you can fill this block with smaller blocks that have the dimensions 1 \u00d7 2. You are allowed to rotate the smaller blocks. Example For n = 1, the output should be solution(n) = 1. There is only one possible way to arrange the smaller 1 \u00d7 2 blocks inside the 4 \u00d7 1 block. For n = 4, the output should be solution(n) = 36. Here are the 36 possible configuration of smaller blocks inside the 4 \u00d7 4 block: Solution def solution ( n ): a = [ 1 , 1 , 5 , 11 ] for i in range ( 4 , n + 1 ): a . append ( a [ i - 1 ] + 5 * a [ i - 2 ] + a [ i - 3 ] - a [ i - 4 ]) return a [ n ] Common Techniques : Basic Contains Duplicates Asked by Palantir - 15 min - Easy Given an array of integers, write a function that determines whether the array contains any duplicates. Your function should return true if any element appears at least twice in the array, and it should return false if every element is distinct. Example For a = [1, 2, 3, 1], the output should be solution(a) = true. There are two 1s in the given array. For a = [3, 1], the output should be solution(a) = false. The given array contains no duplicates. Solution def solution ( a ): return len ( set ( a )) != len ( a ) Sum Of Two Asked by Google - 20 min - Easy You have two integer arrays, a and b, and an integer target value v. Determine whether there is a pair of numbers, where one number is taken from a and the other from b, that can be added together to get a sum of v. Return true if such a pair exists, otherwise return false. Example For a = [1, 2, 3], b = [10, 20, 30, 40], and v = 42, the output should be solution(a, b, v) = true. Solution def solution ( a , b , v ): #No need to iterate a huge list, if the other list is empty if not a or not b : return False #kill duplicates b = set ( b ) #iterate through list a to look if the wanted difference is in b for x in a : if ( v - x ) in b : return True return False Sum In Range Asked by Palantir - 30 min - Medium You have an array of integers nums and an array queries, where queries[i] is a pair of indices (0-based). Find the sum of the elements in nums from the indices at queries[i][0] to queries[i][1] (inclusive) for each query, then add all of the sums for all the queries together. Return that number modulo 109 + 7. Example For nums = [3, 0, -2, 6, -3, 2] and queries = [[0, 2], [2, 5], [0, 5]], the output should be solution(nums, queries) = 10. The array of results for queries is [1, 3, 6], so the answer is 1 + 3 + 6 = 10 Solution from itertools import accumulate def solution ( n , q ): a , res = tuple ( accumulate ([ 0 ] + n )), 0 for i , j in q : res += a [ j + 1 ] - a [ i ] return res % 1000000007 Array Max Consecutive Sum 2 Asked by Amazon, LinkedIn, Microsoft, and Samsung - 30 min - Easy Given an array of integers, find the maximum possible sum you can get from one of its contiguous subarrays. The subarray from which this sum comes must contain at least 1 element. Example For inputArray = [-2, 2, 5, -11, 6], the output should be solution(inputArray) = 7. The contiguous subarray that gives the maximum possible sum is [2, 5], with a sum of 7. Solution def solution ( inputArray ): maxsum = inputArray [ 0 ] l = len ( inputArray ) cumsum = inputArray [ 0 ] for i in range ( 1 , l ): cumsum += inputArray [ i ] if inputArray [ i ] > cumsum : cumsum = inputArray [ i ] maxsum = max ( maxsum , cumsum ) return maxsum Find Longest Subarray By Sum Asked by Palantir and Meta - 30 min - Medium You have an unsorted array arr of non-negative integers and a number s. Find a longest contiguous subarray in arr that has a sum equal to s. Return two integers that represent its inclusive bounds. If there are several possible answers, return the one with the smallest left bound. If there are no answers, return [-1]. Your answer should be 1-based, meaning that the first position of the array is 1 instead of 0. Example For s = 12 and arr = [1, 2, 3, 7, 5], the output should be solution(s, arr) = [2, 4]. The sum of elements from the 2nd position to the 4th position (1-based) is equal to 12: 2 + 3 + 7. For s = 15 and arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], the output should be solution(s, arr) = [1, 5]. The sum of elements from the 1st position to the 5th position (1-based) is equal to 15: 1 + 2 + 3 + 4 + 5. For s = 15 and arr = [1, 2, 3, 4, 5, 0, 0, 0, 6, 7, 8, 9, 10], the output should be solution(s, arr) = [1, 8]. The sum of elements from the 1st position to the 8th position (1-based) is equal to 15: 1 + 2 + 3 + 4 + 5 + 0 + 0 + 0. Solution def solution ( s , a ): total = j = 0 res = ( 0 , - 1 ) for i , v in enumerate ( a ): total += v while j <= i and total > s : total -= a [ j ] j += 1 if ( total == s ) and ( res [ 1 ] - res [ 0 ] < i - j ): res = ( j + 1 , i + 1 ) return res if res [ 0 ] else [ - 1 ] Product Except Self Asked by Amazon, Linkedin, Meta, Microsoft, and Apple - 30 min - Hard You have an array nums. We determine two functions to perform on nums. In both cases, n is the length of nums: fi(nums) = nums[0] \u00b7 nums[1] \u00b7 ... \u00b7 nums[i - 1] \u00b7 nums[i + 1] \u00b7 ... \u00b7 nums[n - 1]. (In other words, fi(nums) is the product of all array elements except the ithf.) g(nums) = f0(nums) + f1(nums) + ... + fn-1(nums). Using these two functions, calculate all values of f modulo the given m. Take these new values and add them together to get g. You should return the value of g modulo the given m. Example For nums = [1, 2, 3, 4] and m = 12, the output should be solution(nums, m) = 2. The array of the values of f is: [24, 12, 8, 6]. If we take all the elements modulo m, we get: [0, 0, 8, 6]. The sum of those values is 8 + 6 = 14, making the answer 14 % 12 = 2. Solution def solution ( nums , m ): s , p = 0 , 1 for num in nums : s , p = ( p + s * num ) % m , p * num % m return s","title":"Other Topics"},{"location":"Exe/pyIntPractice_O/#other-topics","text":"","title":"Other Topics"},{"location":"Exe/pyIntPractice_O/#dynamic-programming-basic","text":"","title":"Dynamic Programming: Basic"},{"location":"Exe/pyIntPractice_O/#climbing-stairs","text":"Asked by Apple and Adobe - 15 min - Easy You are climbing a staircase that has n steps. You can take the steps either 1 or 2 at a time. Calculate how many distinct ways you can climb to the top of the staircase. Example For n = 1, the output should be solution(n) = 1; For n = 2, the output should be solution(n) = 2. You can either climb 2 steps at once or climb 1 step two times. Solution def solution ( n ): a , b = 1 , 0 for _ in range ( n ): a , b = a + b , a return a","title":"Climbing Stairs"},{"location":"Exe/pyIntPractice_O/#house-robber","text":"Asked by Linkedin - 20 min - Medium You are planning to rob houses on a specific street, and you know that every house on the street has a certain amount of money hidden. The only thing stopping you from robbing all of them in one night is that adjacent houses on the street have a connected security system. The system will automatically trigger an alarm if two adjacent houses are broken into on the same night. Given a list of non-negative integers nums representing the amount of money hidden in each house, determine the maximum amount of money you can rob in one night without triggering an alarm. Example For nums = [1, 1, 1], the output should be solution(nums) = 2. The optimal way to get the most money in one night is to rob the first and the third houses for a total of 2. Solutions def solution ( nums ): prev_max = 0 prev_prev_max = 0 for n in nums : prev_prev_max , prev_max = prev_max , max ( prev_prev_max + n , prev_max ) return prev_max from functools import lru_cache def solution ( nums ): return house ( tuple ( nums )) @lru_cache ( maxsize = 1000 ) def house ( nums ): if len ( nums ) == 0 : return 0 return max ( house ( nums [ 1 :]), house ( nums [ 2 :]) + nums [ 0 ])","title":"House Robber"},{"location":"Exe/pyIntPractice_O/#compose-ranges","text":"Asked by Google - 15 min - Easy Given a sorted integer array that does not contain any duplicates, return a summary of the number ranges it contains. Example For nums = [-1, 0, 1, 2, 6, 7, 9], the output should be solution(nums) = [\"-1->2\", \"6->7\", \"9\"]. Solution def solution ( nums ): ranges = [] while nums : start = end = nums . pop ( 0 ) while nums and nums [ 0 ] - end == 1 : end = nums . pop ( 0 ) ranges . append ( str ( start ) + ( '' , '->' + str ( end ))[ start != end ]) return ranges","title":"Compose Ranges"},{"location":"Exe/pyIntPractice_O/#map-decoding","text":"Asked by Microsoft, Uber and Meta - 30 min - Hard A top secret message containing uppercase letters from 'A' to 'Z' has been encoded as numbers using the following mapping: 'A' -> 1 'B' -> 2 ... 'Z' -> 26 You are an FBI agent and you need to determine the total number of ways that the message can be decoded. Since the answer could be very large, take it modulo 109 + 7. Example For message = \"123\", the output should be solution(message) = 3. \"123\" can be decoded as \"ABC\" (1 2 3), \"LC\" (12 3) or \"AW\" (1 23), so the total number of ways is 3. Solution def solution ( msg ): a , b = 1 , 0 M = 10 ** 9 + 7 for i in range ( len ( msg ) - 1 , - 1 , - 1 ): if msg [ i ] == \"0\" : a , b = 0 , a else : a , b = ( a + ( i + 2 <= len ( msg ) and msg [ i : i + 2 ] <= \"26\" ) * b ) % M , a return a","title":"Map Decoding"},{"location":"Exe/pyIntPractice_O/#filling-blocks","text":"Asked by Uber - 30 min - Hard You have a block with the dimensions 4 \u00d7 n. Find the number of different ways you can fill this block with smaller blocks that have the dimensions 1 \u00d7 2. You are allowed to rotate the smaller blocks. Example For n = 1, the output should be solution(n) = 1. There is only one possible way to arrange the smaller 1 \u00d7 2 blocks inside the 4 \u00d7 1 block. For n = 4, the output should be solution(n) = 36. Here are the 36 possible configuration of smaller blocks inside the 4 \u00d7 4 block: Solution def solution ( n ): a = [ 1 , 1 , 5 , 11 ] for i in range ( 4 , n + 1 ): a . append ( a [ i - 1 ] + 5 * a [ i - 2 ] + a [ i - 3 ] - a [ i - 4 ]) return a [ n ]","title":"Filling Blocks"},{"location":"Exe/pyIntPractice_O/#common-techniques-basic","text":"","title":"Common Techniques : Basic"},{"location":"Exe/pyIntPractice_O/#contains-duplicates","text":"Asked by Palantir - 15 min - Easy Given an array of integers, write a function that determines whether the array contains any duplicates. Your function should return true if any element appears at least twice in the array, and it should return false if every element is distinct. Example For a = [1, 2, 3, 1], the output should be solution(a) = true. There are two 1s in the given array. For a = [3, 1], the output should be solution(a) = false. The given array contains no duplicates. Solution def solution ( a ): return len ( set ( a )) != len ( a )","title":"Contains Duplicates"},{"location":"Exe/pyIntPractice_O/#sum-of-two","text":"Asked by Google - 20 min - Easy You have two integer arrays, a and b, and an integer target value v. Determine whether there is a pair of numbers, where one number is taken from a and the other from b, that can be added together to get a sum of v. Return true if such a pair exists, otherwise return false. Example For a = [1, 2, 3], b = [10, 20, 30, 40], and v = 42, the output should be solution(a, b, v) = true. Solution def solution ( a , b , v ): #No need to iterate a huge list, if the other list is empty if not a or not b : return False #kill duplicates b = set ( b ) #iterate through list a to look if the wanted difference is in b for x in a : if ( v - x ) in b : return True return False","title":"Sum Of Two"},{"location":"Exe/pyIntPractice_O/#sum-in-range","text":"Asked by Palantir - 30 min - Medium You have an array of integers nums and an array queries, where queries[i] is a pair of indices (0-based). Find the sum of the elements in nums from the indices at queries[i][0] to queries[i][1] (inclusive) for each query, then add all of the sums for all the queries together. Return that number modulo 109 + 7. Example For nums = [3, 0, -2, 6, -3, 2] and queries = [[0, 2], [2, 5], [0, 5]], the output should be solution(nums, queries) = 10. The array of results for queries is [1, 3, 6], so the answer is 1 + 3 + 6 = 10 Solution from itertools import accumulate def solution ( n , q ): a , res = tuple ( accumulate ([ 0 ] + n )), 0 for i , j in q : res += a [ j + 1 ] - a [ i ] return res % 1000000007","title":"Sum In Range"},{"location":"Exe/pyIntPractice_O/#array-max-consecutive-sum-2","text":"Asked by Amazon, LinkedIn, Microsoft, and Samsung - 30 min - Easy Given an array of integers, find the maximum possible sum you can get from one of its contiguous subarrays. The subarray from which this sum comes must contain at least 1 element. Example For inputArray = [-2, 2, 5, -11, 6], the output should be solution(inputArray) = 7. The contiguous subarray that gives the maximum possible sum is [2, 5], with a sum of 7. Solution def solution ( inputArray ): maxsum = inputArray [ 0 ] l = len ( inputArray ) cumsum = inputArray [ 0 ] for i in range ( 1 , l ): cumsum += inputArray [ i ] if inputArray [ i ] > cumsum : cumsum = inputArray [ i ] maxsum = max ( maxsum , cumsum ) return maxsum","title":"Array Max Consecutive Sum 2"},{"location":"Exe/pyIntPractice_O/#find-longest-subarray-by-sum","text":"Asked by Palantir and Meta - 30 min - Medium You have an unsorted array arr of non-negative integers and a number s. Find a longest contiguous subarray in arr that has a sum equal to s. Return two integers that represent its inclusive bounds. If there are several possible answers, return the one with the smallest left bound. If there are no answers, return [-1]. Your answer should be 1-based, meaning that the first position of the array is 1 instead of 0. Example For s = 12 and arr = [1, 2, 3, 7, 5], the output should be solution(s, arr) = [2, 4]. The sum of elements from the 2nd position to the 4th position (1-based) is equal to 12: 2 + 3 + 7. For s = 15 and arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], the output should be solution(s, arr) = [1, 5]. The sum of elements from the 1st position to the 5th position (1-based) is equal to 15: 1 + 2 + 3 + 4 + 5. For s = 15 and arr = [1, 2, 3, 4, 5, 0, 0, 0, 6, 7, 8, 9, 10], the output should be solution(s, arr) = [1, 8]. The sum of elements from the 1st position to the 8th position (1-based) is equal to 15: 1 + 2 + 3 + 4 + 5 + 0 + 0 + 0. Solution def solution ( s , a ): total = j = 0 res = ( 0 , - 1 ) for i , v in enumerate ( a ): total += v while j <= i and total > s : total -= a [ j ] j += 1 if ( total == s ) and ( res [ 1 ] - res [ 0 ] < i - j ): res = ( j + 1 , i + 1 ) return res if res [ 0 ] else [ - 1 ]","title":"Find Longest Subarray By Sum"},{"location":"Exe/pyIntPractice_O/#product-except-self","text":"Asked by Amazon, Linkedin, Meta, Microsoft, and Apple - 30 min - Hard You have an array nums. We determine two functions to perform on nums. In both cases, n is the length of nums: fi(nums) = nums[0] \u00b7 nums[1] \u00b7 ... \u00b7 nums[i - 1] \u00b7 nums[i + 1] \u00b7 ... \u00b7 nums[n - 1]. (In other words, fi(nums) is the product of all array elements except the ithf.) g(nums) = f0(nums) + f1(nums) + ... + fn-1(nums). Using these two functions, calculate all values of f modulo the given m. Take these new values and add them together to get g. You should return the value of g modulo the given m. Example For nums = [1, 2, 3, 4] and m = 12, the output should be solution(nums, m) = 2. The array of the values of f is: [24, 12, 8, 6]. If we take all the elements modulo m, we get: [0, 0, 8, 6]. The sum of those values is 8 + 6 = 14, making the answer 14 % 12 = 2. Solution def solution ( nums , m ): s , p = 0 , 1 for num in nums : s , p = ( p + s * num ) % m , p * num % m return s","title":"Product Except Self"},{"location":"Exe/pyIntPractice_SS/","text":"Sorting & Searching DFS & BFS Traverse Tree 30 min - Easy Note: Try to solve this task without using recursion, since this is what you'll be asked to do during an interview. Given a binary tree of integers t, return its node values in the following format: The first element should be the value of the tree root; The next elements should be the values of the nodes at height 1 (i.e. the root children), ordered from the leftmost to the rightmost one; The elements after that should be the values of the nodes at height 2 (i.e. the children of the nodes at height 1) ordered in the same way; Etc. Example For t = { \"value\": 1, \"left\": { \"value\": 2, \"left\": null, \"right\": { \"value\": 3, \"left\": null, \"right\": null } }, \"right\": { \"value\": 4, \"left\": { \"value\": 5, \"left\": null, \"right\": null }, \"right\": null } } the output should be solution(t) = [1, 2, 4, 3, 5]. This t looks like this: 1 / \\ 2 4 \\ / 3 5 Solution # # Binary trees are already defined with this interface: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None def solution ( t ): if t is None : return [] res = [] q = [ t ] while q : v = q . pop ( 0 ) res += [ v . value ] if v . left : q += [ v . left ] if v . right : q += [ v . right ] return res Largest Values In Tree Rows 30 min - Easy You have a binary tree t. Your task is to find the largest value in each row of this tree. In a tree, a row is a set of nodes that have equal depth. For example, a row with depth 0 is a tree root, a row with depth 1 is composed of the root's children, etc. Return an array in which the first element is the largest value in the row with depth 0, the second element is the largest value in the row with depth 1, the third element is the largest element in the row with depth 2, etc. Example For t = { \"value\": -1, \"left\": { \"value\": 5, \"left\": null, \"right\": null }, \"right\": { \"value\": 7, \"left\": null, \"right\": { \"value\": 1, \"left\": null, \"right\": null } } } the output should be solution(t) = [-1, 7, 1]. The tree in the example looks like this: -1 / \\ 5 7 \\ 1 In the row with depth 0, there is only one vertex - the root with value -1; In the row with depth 1, there are two vertices with values 5 and 7, so the largest value here is 7; In the row with depth 2, there is only one vertex with value 1. Solution # Definition for binary tree: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None import math def solution ( t ): if t is None : return [] stack = [ t ] result = [] while len ( stack ) > 0 : result . append ( max ( tree . value for tree in stack )) next_row = [ tree . left for tree in stack if tree . left ] + [ tree . right for tree in stack if tree . right ] stack = next_row return result Digit Tree Sum 30 min - Medium We're going to store numbers in a tree. Each node in this tree will store a single digit (from 0 to 9), and each path from root to leaf encodes a non-negative integer. Given a binary tree t, find the sum of all the numbers encoded in it. Example For t = { \"value\": 1, \"left\": { \"value\": 0, \"left\": { \"value\": 3, \"left\": null, \"right\": null }, \"right\": { \"value\": 1, \"left\": null, \"right\": null } }, \"right\": { \"value\": 4, \"left\": null, \"right\": null } } the output should be solution(t) = 218. There are 3 numbers encoded in this tree: Path 1->0->3 encodes 103 Path 1->0->1 encodes 101 Path 1->4 encodes 14 and their sum is 103 + 101 + 14 = 218. t = { \"value\": 0, \"left\": { \"value\": 9, \"left\": null, \"right\": null }, \"right\": { \"value\": 9, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": { \"value\": 3, \"left\": null, \"right\": null } } } the output should be solution(t) = 193. Because 09 + 091 + 093 = 193 Solution # Definition for binary tree: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None def solution ( t ): if not t : return 0 stack = [( t , 0 )] sum = 0 while stack : cur , v = stack . pop () if cur . left or cur . right : if cur . left : stack . append (( cur . left , cur . value + v * 10 )) if cur . right : stack . append (( cur . right , cur . value + v * 10 )) else : sum += cur . value + v * 10 return sum Longest Path Asked by Google - 45 min - Hard Suppose we represent our file system as a string. For example, the string \"user\\n\\tpictures\\n\\tdocuments\\n\\t\\tnotes.txt\" represents: user pictures documents notes.txt The directory user contains an empty sub-directory pictures and a sub-directory documents containing a file notes.txt. The string \"user\\n\\tpictures\\n\\t\\tphoto.png\\n\\t\\tcamera\\n\\tdocuments\\n\\t\\tlectures\\n\\t\\t\\tnotes.txt\" represents: user pictures photo.png camera documents lectures notes.txt The directory user contains two sub-directories pictures and documents. pictures contains a file photo.png and an empty second-level sub-directory camera. documents contains a second-level sub-directory lectures containing a file notes.txt. We want to find the longest (as determined by the number of characters) absolute path to a file within our system. For example, in the second example above, the longest absolute path is \"user/documents/lectures/notes.txt\", and its length is 33 (not including the double quotes). Given a string representing the file system in this format, return the length of the longest absolute path to a file in the abstracted file system. If there is not a file in the file system, return 0. Notes: Due to system limitations, test cases use form feeds ('\\f', ASCII code 12) instead of newline characters. File names do not contain spaces at the beginning. Example For fileSystem = \"user\\f\\tpictures\\f\\tdocuments\\f\\t\\tnotes.txt\", the output should be solution(fileSystem) = 24. The longest path is \"user/documents/notes.txt\", and it consists of 24 characters. Solution def solution ( fileSystem ): maxlen = 0 pathlen = { 0 : 0 } for line in fileSystem . splitlines (): name = line . lstrip ( ' \\t ' ) depth = len ( line ) - len ( name ) if '.' in name : maxlen = max ( maxlen , pathlen [ depth ] + len ( name )) else : pathlen [ depth + 1 ] = pathlen [ depth ] + len ( name ) + 1 return maxlen Graph Distances Asked by Adobe and Samsung - 45 min - Hard You have a strongly connected directed graph that has positive weights in the adjacency matrix g. The graph is represented as a square matrix, where g[i][j] is the weight of the edge (i, j), or -1 if there is no such edge. Given g and the index of a start vertex s, find the minimal distances between the start vertex s and each of the vertices of the graph. Example For g = [[-1, 3, 2], [2, -1, 0], [-1, 0, -1]] and s = 0, the output should be solution(g, s) = [0, 2, 2]. The distance from the start vertex 0 to itself is 0. The distance from the start vertex 0 to vertex 1 is 2 + 0 = 2. The distance from the start vertex 0 to vertex 2 is 2. Solution from collections import deque def solution ( g , s ): dists = [ - 1 for _ in g ] dists [ s ] = 0 q = deque ( graphExpand ( g , s )) while q : i , val = q . popleft () if dists [ i ] == - 1 or val < dists [ i ]: dists [ i ] = val q . extend ( graphExpand ( g , i , val )) return dists def graphExpand ( g , i , dist = 0 ): return [( j , dist + v ) for j , v in enumerate ( g [ i ]) if v != - 1 ] Backtracking Climbing Staircase Asked by Adobe - 30 min - Medium You need to climb a staircase that has n steps, and you decide to get some extra exercise by jumping up the steps. You can cover at most k steps in a single jump. Return all the possible sequences of jumps that you could take to climb the staircase, sorted. Example For n = 4 and k = 2, the output should be solution(n, k) = [[1, 1, 1, 1], [1, 1, 2], [1, 2, 1], [2, 1, 1], [2, 2]] There are 4 steps in the staircase, and you can jump up 2 or fewer steps at a time. There are 5 potential sequences in which you jump up the stairs either 2 or 1 at a time. Solution def solution ( n , k ): return climb ( n , k , []) def climb ( n , k , jumps ): if n == 0 : return [ jumps ] out = [] for i in range ( 1 , k + 1 ): if i > n : continue temp = jumps + [ i ] out += climb ( n - i , k , temp ) return out N Queens Asked by Amazon and Twitter - 30 min - Hard In chess, queens can move any number of squares vertically, horizontally, or diagonally. The n-queens puzzle is the problem of placing n queens on an n \u00d7 n chessboard so that no two queens can attack each other. Given an integer n, print all possible distinct solutions to the n-queens puzzle. Each solution contains distinct board configurations of the placement of the n queens, where the solutions are arrays that contain permutations of [1, 2, 3, .. n]. The number in the ith position of the results array indicates that the ith column queen is placed in the row with that number. In your solution, the board configurations should be returned in lexicographical order. Example For n = 1, the output should be solution(n) = [[1]]; For n = 4, the output should be solution(n) = [[2, 4, 1, 3], [3, 1, 4, 2]] This diagram of the second permutation, [3, 1, 4, 2], will help you visualize its configuration: The element in the 1st position of the array, 3, indicates that the queen for column 1 is placed in row 3. Since the element in the 2nd position of the array is 1, the queen for column 2 is placed in row 1. The element in the 3rd position of the array is 4, meaning that the queen for column 3 is placed in row 4, and the element in the 4th position of the array is 2, meaning that the queen for column 4 is placed in row 2. Solution def solution ( n , state = [], col = 1 ): if col > n : return [ state ] res = [] for i in range ( 1 , n + 1 ): if invalid ( state , i ): continue for sol in solution ( n , state + [ i ], col + 1 ): res += [ sol ] return res def invalid ( s , r2 ): if not s : return False if r2 in s : return True c2 = len ( s ) + 1 return any ( abs ( c1 - c2 ) == abs ( r1 - r2 ) for c1 , r1 in enumerate ( s , 1 )) Sum Subsets Asked by Palantir - 30 min - Hard Given a sorted array of integers arr and an integer num, find all possible unique subsets of arr that add up to num. Both the array of subsets and the subsets themselves should be sorted in lexicographical order. Example For arr = [1, 2, 3, 4, 5] and num = 5, the output should be solution(arr, num) = [[1, 4], [2, 3], [5]]. Solution def solution ( arr , num ): result = set () def addSumSubsets ( arr_i , target , subset ): if target == 0 : result . add ( subset ) elif arr_i >= len ( arr ) or target < 0 : return else : n = arr [ arr_i ] addSumSubsets ( arr_i + 1 , target - n , subset + ( n ,)) addSumSubsets ( arr_i + 1 , target , subset ) addSumSubsets ( 0 , num , ()) return sorted ( list ( result )) Word Boggle 35 min - Medium Boggle is a popular word game in which players attempt to find words in sequences of adjacent letters on a rectangular board. Given a two-dimensional array board that represents the character cells of the Boggle board and an array of unique strings words, find all the possible words from words that can be formed on the board. Note that in Boggle when you're finding a word, you can move from a cell to any of its 8 neighbors, but you can't use the same cell twice in one word. Example For board = [ ['R', 'L', 'D'], ['U', 'O', 'E'], ['C', 'S', 'O'] ] and words = [\"CODE\", \"SOLO\", \"RULES\", \"COOL\"], the output should be solution(board, words) = [\"CODE\", \"RULES\"] Solution def solution ( board , words ): r = [] for word in words : if canBoggle ( board , word ): r . append ( word ) return sorted ( r ) def canBoggle ( board , word , used = []): if len ( word ) == 0 : return True for i in range ( len ( board )): for j in range ( len ( board [ 0 ])): if ( i , j ) not in used and board [ i ][ j ] == word [ 0 ]: if len ( used ) == 0 or ( abs ( used [ - 1 ][ 0 ] - i ) <= 1 and abs ( used [ - 1 ][ 1 ] - j ) <= 1 ): if canBoggle ( board , word [ 1 :], used + [( i , j )]): return True return False Combination Sum Asked by Amazon, Adobe and Microsoft - 40 min - Hard Given an array of integers a and an integer sum, find all of the unique combinations in a that add up to sum. The same number from a can be used an unlimited number of times in a combination. Elements in a combination (a1 a2 \u2026 ak) must be sorted in non-descending order, while the combinations themselves must be sorted in ascending order. If there are no possible combinations that add up to sum, the output should be the string \"Empty\". Example For a = [2, 3, 5, 9] and sum = 9, the output should be solution(a, sum) = \"(2 2 2 3)(2 2 5)(3 3 3)(9)\". Solution def solution ( a , s ): arr = sorted ( set ( a )) ans = list ( comb_recur ([], arr , s )) ans . sort () if len ( ans ) == 0 : return 'Empty' else : return '( {} )' . format ( ')(' . join ( ' ' . join ( map ( str , row )) for row in ans )) def comb_recur ( pref , arr , s ): for i , val in enumerate ( arr ): if val == s : yield pref + [ val ] elif val < s : yield from comb_recur ( pref + [ val ], arr [ i :], s - val ) elif val > s : break","title":"Sorting and Searching"},{"location":"Exe/pyIntPractice_SS/#sorting-searching","text":"","title":"Sorting &amp; Searching"},{"location":"Exe/pyIntPractice_SS/#dfs-bfs","text":"","title":"DFS &amp; BFS"},{"location":"Exe/pyIntPractice_SS/#traverse-tree","text":"30 min - Easy Note: Try to solve this task without using recursion, since this is what you'll be asked to do during an interview. Given a binary tree of integers t, return its node values in the following format: The first element should be the value of the tree root; The next elements should be the values of the nodes at height 1 (i.e. the root children), ordered from the leftmost to the rightmost one; The elements after that should be the values of the nodes at height 2 (i.e. the children of the nodes at height 1) ordered in the same way; Etc. Example For t = { \"value\": 1, \"left\": { \"value\": 2, \"left\": null, \"right\": { \"value\": 3, \"left\": null, \"right\": null } }, \"right\": { \"value\": 4, \"left\": { \"value\": 5, \"left\": null, \"right\": null }, \"right\": null } } the output should be solution(t) = [1, 2, 4, 3, 5]. This t looks like this: 1 / \\ 2 4 \\ / 3 5 Solution # # Binary trees are already defined with this interface: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None def solution ( t ): if t is None : return [] res = [] q = [ t ] while q : v = q . pop ( 0 ) res += [ v . value ] if v . left : q += [ v . left ] if v . right : q += [ v . right ] return res","title":"Traverse Tree"},{"location":"Exe/pyIntPractice_SS/#largest-values-in-tree-rows","text":"30 min - Easy You have a binary tree t. Your task is to find the largest value in each row of this tree. In a tree, a row is a set of nodes that have equal depth. For example, a row with depth 0 is a tree root, a row with depth 1 is composed of the root's children, etc. Return an array in which the first element is the largest value in the row with depth 0, the second element is the largest value in the row with depth 1, the third element is the largest element in the row with depth 2, etc. Example For t = { \"value\": -1, \"left\": { \"value\": 5, \"left\": null, \"right\": null }, \"right\": { \"value\": 7, \"left\": null, \"right\": { \"value\": 1, \"left\": null, \"right\": null } } } the output should be solution(t) = [-1, 7, 1]. The tree in the example looks like this: -1 / \\ 5 7 \\ 1 In the row with depth 0, there is only one vertex - the root with value -1; In the row with depth 1, there are two vertices with values 5 and 7, so the largest value here is 7; In the row with depth 2, there is only one vertex with value 1. Solution # Definition for binary tree: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None import math def solution ( t ): if t is None : return [] stack = [ t ] result = [] while len ( stack ) > 0 : result . append ( max ( tree . value for tree in stack )) next_row = [ tree . left for tree in stack if tree . left ] + [ tree . right for tree in stack if tree . right ] stack = next_row return result","title":"Largest Values In Tree Rows"},{"location":"Exe/pyIntPractice_SS/#digit-tree-sum","text":"30 min - Medium We're going to store numbers in a tree. Each node in this tree will store a single digit (from 0 to 9), and each path from root to leaf encodes a non-negative integer. Given a binary tree t, find the sum of all the numbers encoded in it. Example For t = { \"value\": 1, \"left\": { \"value\": 0, \"left\": { \"value\": 3, \"left\": null, \"right\": null }, \"right\": { \"value\": 1, \"left\": null, \"right\": null } }, \"right\": { \"value\": 4, \"left\": null, \"right\": null } } the output should be solution(t) = 218. There are 3 numbers encoded in this tree: Path 1->0->3 encodes 103 Path 1->0->1 encodes 101 Path 1->4 encodes 14 and their sum is 103 + 101 + 14 = 218. t = { \"value\": 0, \"left\": { \"value\": 9, \"left\": null, \"right\": null }, \"right\": { \"value\": 9, \"left\": { \"value\": 1, \"left\": null, \"right\": null }, \"right\": { \"value\": 3, \"left\": null, \"right\": null } } } the output should be solution(t) = 193. Because 09 + 091 + 093 = 193 Solution # Definition for binary tree: # class Tree(object): # def __init__(self, x): # self.value = x # self.left = None # self.right = None def solution ( t ): if not t : return 0 stack = [( t , 0 )] sum = 0 while stack : cur , v = stack . pop () if cur . left or cur . right : if cur . left : stack . append (( cur . left , cur . value + v * 10 )) if cur . right : stack . append (( cur . right , cur . value + v * 10 )) else : sum += cur . value + v * 10 return sum","title":"Digit Tree Sum"},{"location":"Exe/pyIntPractice_SS/#longest-path","text":"Asked by Google - 45 min - Hard Suppose we represent our file system as a string. For example, the string \"user\\n\\tpictures\\n\\tdocuments\\n\\t\\tnotes.txt\" represents: user pictures documents notes.txt The directory user contains an empty sub-directory pictures and a sub-directory documents containing a file notes.txt. The string \"user\\n\\tpictures\\n\\t\\tphoto.png\\n\\t\\tcamera\\n\\tdocuments\\n\\t\\tlectures\\n\\t\\t\\tnotes.txt\" represents: user pictures photo.png camera documents lectures notes.txt The directory user contains two sub-directories pictures and documents. pictures contains a file photo.png and an empty second-level sub-directory camera. documents contains a second-level sub-directory lectures containing a file notes.txt. We want to find the longest (as determined by the number of characters) absolute path to a file within our system. For example, in the second example above, the longest absolute path is \"user/documents/lectures/notes.txt\", and its length is 33 (not including the double quotes). Given a string representing the file system in this format, return the length of the longest absolute path to a file in the abstracted file system. If there is not a file in the file system, return 0. Notes: Due to system limitations, test cases use form feeds ('\\f', ASCII code 12) instead of newline characters. File names do not contain spaces at the beginning. Example For fileSystem = \"user\\f\\tpictures\\f\\tdocuments\\f\\t\\tnotes.txt\", the output should be solution(fileSystem) = 24. The longest path is \"user/documents/notes.txt\", and it consists of 24 characters. Solution def solution ( fileSystem ): maxlen = 0 pathlen = { 0 : 0 } for line in fileSystem . splitlines (): name = line . lstrip ( ' \\t ' ) depth = len ( line ) - len ( name ) if '.' in name : maxlen = max ( maxlen , pathlen [ depth ] + len ( name )) else : pathlen [ depth + 1 ] = pathlen [ depth ] + len ( name ) + 1 return maxlen","title":"Longest Path"},{"location":"Exe/pyIntPractice_SS/#graph-distances","text":"Asked by Adobe and Samsung - 45 min - Hard You have a strongly connected directed graph that has positive weights in the adjacency matrix g. The graph is represented as a square matrix, where g[i][j] is the weight of the edge (i, j), or -1 if there is no such edge. Given g and the index of a start vertex s, find the minimal distances between the start vertex s and each of the vertices of the graph. Example For g = [[-1, 3, 2], [2, -1, 0], [-1, 0, -1]] and s = 0, the output should be solution(g, s) = [0, 2, 2]. The distance from the start vertex 0 to itself is 0. The distance from the start vertex 0 to vertex 1 is 2 + 0 = 2. The distance from the start vertex 0 to vertex 2 is 2. Solution from collections import deque def solution ( g , s ): dists = [ - 1 for _ in g ] dists [ s ] = 0 q = deque ( graphExpand ( g , s )) while q : i , val = q . popleft () if dists [ i ] == - 1 or val < dists [ i ]: dists [ i ] = val q . extend ( graphExpand ( g , i , val )) return dists def graphExpand ( g , i , dist = 0 ): return [( j , dist + v ) for j , v in enumerate ( g [ i ]) if v != - 1 ]","title":"Graph Distances"},{"location":"Exe/pyIntPractice_SS/#backtracking","text":"","title":"Backtracking"},{"location":"Exe/pyIntPractice_SS/#climbing-staircase","text":"Asked by Adobe - 30 min - Medium You need to climb a staircase that has n steps, and you decide to get some extra exercise by jumping up the steps. You can cover at most k steps in a single jump. Return all the possible sequences of jumps that you could take to climb the staircase, sorted. Example For n = 4 and k = 2, the output should be solution(n, k) = [[1, 1, 1, 1], [1, 1, 2], [1, 2, 1], [2, 1, 1], [2, 2]] There are 4 steps in the staircase, and you can jump up 2 or fewer steps at a time. There are 5 potential sequences in which you jump up the stairs either 2 or 1 at a time. Solution def solution ( n , k ): return climb ( n , k , []) def climb ( n , k , jumps ): if n == 0 : return [ jumps ] out = [] for i in range ( 1 , k + 1 ): if i > n : continue temp = jumps + [ i ] out += climb ( n - i , k , temp ) return out","title":"Climbing Staircase"},{"location":"Exe/pyIntPractice_SS/#n-queens","text":"Asked by Amazon and Twitter - 30 min - Hard In chess, queens can move any number of squares vertically, horizontally, or diagonally. The n-queens puzzle is the problem of placing n queens on an n \u00d7 n chessboard so that no two queens can attack each other. Given an integer n, print all possible distinct solutions to the n-queens puzzle. Each solution contains distinct board configurations of the placement of the n queens, where the solutions are arrays that contain permutations of [1, 2, 3, .. n]. The number in the ith position of the results array indicates that the ith column queen is placed in the row with that number. In your solution, the board configurations should be returned in lexicographical order. Example For n = 1, the output should be solution(n) = [[1]]; For n = 4, the output should be solution(n) = [[2, 4, 1, 3], [3, 1, 4, 2]] This diagram of the second permutation, [3, 1, 4, 2], will help you visualize its configuration: The element in the 1st position of the array, 3, indicates that the queen for column 1 is placed in row 3. Since the element in the 2nd position of the array is 1, the queen for column 2 is placed in row 1. The element in the 3rd position of the array is 4, meaning that the queen for column 3 is placed in row 4, and the element in the 4th position of the array is 2, meaning that the queen for column 4 is placed in row 2. Solution def solution ( n , state = [], col = 1 ): if col > n : return [ state ] res = [] for i in range ( 1 , n + 1 ): if invalid ( state , i ): continue for sol in solution ( n , state + [ i ], col + 1 ): res += [ sol ] return res def invalid ( s , r2 ): if not s : return False if r2 in s : return True c2 = len ( s ) + 1 return any ( abs ( c1 - c2 ) == abs ( r1 - r2 ) for c1 , r1 in enumerate ( s , 1 ))","title":"N Queens"},{"location":"Exe/pyIntPractice_SS/#sum-subsets","text":"Asked by Palantir - 30 min - Hard Given a sorted array of integers arr and an integer num, find all possible unique subsets of arr that add up to num. Both the array of subsets and the subsets themselves should be sorted in lexicographical order. Example For arr = [1, 2, 3, 4, 5] and num = 5, the output should be solution(arr, num) = [[1, 4], [2, 3], [5]]. Solution def solution ( arr , num ): result = set () def addSumSubsets ( arr_i , target , subset ): if target == 0 : result . add ( subset ) elif arr_i >= len ( arr ) or target < 0 : return else : n = arr [ arr_i ] addSumSubsets ( arr_i + 1 , target - n , subset + ( n ,)) addSumSubsets ( arr_i + 1 , target , subset ) addSumSubsets ( 0 , num , ()) return sorted ( list ( result ))","title":"Sum Subsets"},{"location":"Exe/pyIntPractice_SS/#word-boggle","text":"35 min - Medium Boggle is a popular word game in which players attempt to find words in sequences of adjacent letters on a rectangular board. Given a two-dimensional array board that represents the character cells of the Boggle board and an array of unique strings words, find all the possible words from words that can be formed on the board. Note that in Boggle when you're finding a word, you can move from a cell to any of its 8 neighbors, but you can't use the same cell twice in one word. Example For board = [ ['R', 'L', 'D'], ['U', 'O', 'E'], ['C', 'S', 'O'] ] and words = [\"CODE\", \"SOLO\", \"RULES\", \"COOL\"], the output should be solution(board, words) = [\"CODE\", \"RULES\"] Solution def solution ( board , words ): r = [] for word in words : if canBoggle ( board , word ): r . append ( word ) return sorted ( r ) def canBoggle ( board , word , used = []): if len ( word ) == 0 : return True for i in range ( len ( board )): for j in range ( len ( board [ 0 ])): if ( i , j ) not in used and board [ i ][ j ] == word [ 0 ]: if len ( used ) == 0 or ( abs ( used [ - 1 ][ 0 ] - i ) <= 1 and abs ( used [ - 1 ][ 1 ] - j ) <= 1 ): if canBoggle ( board , word [ 1 :], used + [( i , j )]): return True return False","title":"Word Boggle"},{"location":"Exe/pyIntPractice_SS/#combination-sum","text":"Asked by Amazon, Adobe and Microsoft - 40 min - Hard Given an array of integers a and an integer sum, find all of the unique combinations in a that add up to sum. The same number from a can be used an unlimited number of times in a combination. Elements in a combination (a1 a2 \u2026 ak) must be sorted in non-descending order, while the combinations themselves must be sorted in ascending order. If there are no possible combinations that add up to sum, the output should be the string \"Empty\". Example For a = [2, 3, 5, 9] and sum = 9, the output should be solution(a, sum) = \"(2 2 2 3)(2 2 5)(3 3 3)(9)\". Solution def solution ( a , s ): arr = sorted ( set ( a )) ans = list ( comb_recur ([], arr , s )) ans . sort () if len ( ans ) == 0 : return 'Empty' else : return '( {} )' . format ( ')(' . join ( ' ' . join ( map ( str , row )) for row in ans )) def comb_recur ( pref , arr , s ): for i , val in enumerate ( arr ): if val == s : yield pref + [ val ] elif val < s : yield from comb_recur ( pref + [ val ], arr [ i :], s - val ) elif val > s : break","title":"Combination Sum"},{"location":"Exe/pyStrategies/","text":"Not solving coding problems but practicing to map problems onto problems that you have already solved. 1 - If input is sorted If the given input is sorted (array, list, or matrix), we will use a variation of Binary Search or a Two Pointers strategy. Sample Problem for Binary Search Bitonic array maximum Problem Statement: Find the maximum value in a given Bitonic array. An array is considered bitonic if it is monotonically increasing and then monotonically decreasing. Monotonically increasing or decreasing means that for any index i in the array, arr[i] != arr[i+1]. Example: Input: [1, 3, 8, 12, 4, 2], Output: 12 Solution A bitonic array is a sorted array; the only difference is that its first part is sorted in ascending order, and the second part is sorted in descending order. We can use a variation of Binary Search to solve this problem. Remember that in Binary Search, we have start, end, and middle indices, and in each step, we reduce our search space by moving start or end. Since no two consecutive numbers are the same (as the array is monotonically increasing or decreasing), whenever we calculate the middle index for Binary Search, we can compare the numbers pointed out by the index middle and middle+1 to find if we are in the ascending or the descending part. So: If arr[middle] > arr[middle + 1], we are in the second (descending) part of the bitonic array. Therefore, our required number could either be pointed out by middle or will be before middle. This means we will do end = middle. If arr[middle] <= arr[middle + 1], we are in the first (ascending) part of the bitonic array. Therefore, the required number will be after middle. This means we do start = middle + 1. We can break when start == end. Due to the above two points, both start and end will point at the maximum number of the Bitonic array. class MaxInBitonicArray { public static int findMax ( int [] arr ) { int start = 0 , end = arr . length - 1 ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; if ( arr [ mid ] > arr [ mid + 1 ] ) { end = mid ; } else { start = mid + 1 ; } } // at the end of the while loop, 'start == end' return arr [ start ] ; } public static void main ( String [] args ) { System . out . println ( MaxInBitonicArray . findMax ( new int [] { 1 , 3 , 8 , 12 , 4 , 2 })); System . out . println ( MaxInBitonicArray . findMax ( new int [] { 3 , 8 , 3 , 1 })); System . out . println ( MaxInBitonicArray . findMax ( new int [] { 1 , 3 , 8 , 12 })); System . out . println ( MaxInBitonicArray . findMax ( new int [] { 10 , 9 , 8 })); } } Sample Problem for Two Pointers Pair with the target sum Problem Statement: Given an array of sorted numbers and a target sum, find a pair in the array whose sum is equal to the given target. Write a function to return the indices of the two numbers (i.e., the pair) such that they add up to the given target. Example: Input: [1, 2, 3, 4, 6], target = 6, Output: [1, 3] (The numbers at index 1 and 3 add up to 6: 2+4=6) Solution Since the given array is sorted, a brute-force solution could be to iterate through the array, taking one number at a time and searching for the second number through Binary Search. The time complexity of this algorithm will be O(N*logN). Can we do better than this? We can follow the Two Pointers approach. We will start with one pointer pointing to the beginning of the array and another pointing at the end. At every step, we will see if the numbers pointed by the two pointers add up to the target sum. If they do, we\u2019ve found our pair. Otherwise, we\u2019ll do one of two things: If the sum of the two numbers pointed by the two pointers is greater than the target sum, we need a pair with a smaller sum. So, to try more pairs, we can decrement the end-pointer. If the sum of the two numbers pointed by the two pointers is smaller than the target sum, this means that we need a pair with a larger sum. So, to try more pairs, we can increment the start-pointer. Here is the visual representation of this algorithm for the example mentioned above: class PairWithTargetSum { public static int [] search ( int [] arr , int targetSum ) { int left = 0 , right = arr . length - 1 ; while ( left < right ) { // comparing the sum of two numbers to the 'targetSum' can cause integer overflow // so, we will try to find a target difference instead int targetDiff = targetSum - arr [ left ] ; if ( targetDiff == arr [ right ] ) return new int [] { left , right }; // found the pair if ( targetDiff > arr [ right ] ) left ++ ; // we need a pair with a bigger sum else right -- ; // we need a pair with a smaller sum } return new int [] { - 1 , - 1 }; } public static void main ( String [] args ) { int [] result = PairWithTargetSum . search ( new int [] { 1 , 2 , 3 , 4 , 6 }, 6 ); System . out . println ( \"Pair with target sum: [\" + result [ 0 ] + \", \" + result [ 1 ] + \"]\" ); result = PairWithTargetSum . search ( new int [] { 2 , 5 , 9 , 11 }, 11 ); System . out . println ( \"Pair with target sum: [\" + result [ 0 ] + \", \" + result [ 1 ] + \"]\" ); } } 2 - If need top/max/min/closest k ele If we\u2019re dealing with top/maximum/minimum/closest k elements among n elements, we will use a Heap. Sample Problem K closest points to the origin Problem Statement: Given an array of points in a 2D plane, find K closest points to the origin. Example: Input: points = [[1,2],[1,3]], K = 1, Output: [[1,2]] Solution The Euclidean distance of a point P(x,y) from the origin can be calculated through the following formula: We can use a Max Heap to find K points closest to the origin. We can start by pushing K points in the heap. While iterating through the remaining points, if a point (say P) is closer to the origin than the top point of the max-heap, we will remove that top point from the heap and add P to always keep the closest points in the heap. import java.util.* ; class Point { int x ; int y ; public Point ( int x , int y ) { this . x = x ; this . y = y ; } public int distFromOrigin () { // ignoring sqrt return ( x * x ) + ( y * y ); } } class KClosestPointsToOrigin { public static List < Point > findClosestPoints ( Point [] points , int k ) { PriorityQueue < Point > maxHeap = new PriorityQueue <> ( ( p1 , p2 ) -> p2 . distFromOrigin () - p1 . distFromOrigin ()); // put first 'k' points in the max heap for ( int i = 0 ; i < k ; i ++ ) maxHeap . add ( points [ i ] ); // go through the remaining points of the input array, if a point is closer to // the origin than the top point of the max-heap, remove the top point from // heap and add the point from the input array for ( int i = k ; i < points . length ; i ++ ) { if ( points [ i ] . distFromOrigin () < maxHeap . peek (). distFromOrigin ()) { maxHeap . poll (); maxHeap . add ( points [ i ] ); } } // the heap has 'k' points closest to the origin, return them in a list return new ArrayList <> ( maxHeap ); } public static void main ( String [] args ) { Point [] points = new Point [] { new Point ( 1 , 3 ), new Point ( 3 , 4 ), new Point ( 2 , - 1 ) }; List < Point > result = KClosestPointsToOrigin . findClosestPoints ( points , 2 ); System . out . print ( \"Here are the k points closest the origin: \" ); for ( Point p : result ) System . out . print ( \"[\" + p . x + \" , \" + p . y + \"] \" ); } } 3 - If need all combinations (or perm) If we need to try all combinations (or permutations) of the input, we can either use recursive Backtracking or iterative Breadth-First Search . Sample Problem Subsets Problem Statement: Given a set with distinct elements, find all of its distinct subsets. Example: Input: [1, 5, 3] Output: [], [1], [5], [3], [1,5], [1,3], [5,3], [1,5,3] Solution We can use the Breadth-First Search (BFS) approach to generate all subsets of the given set. We can start with an empty set, iterate through all numbers one-by-one, and add them to existing sets to create new subsets. Let\u2019s take the aforementioned example to go through each step of our algorithm: Given set: [1, 5, 3] Start with an empty set: [[]] Add the first number (1) to all the existing subsets to create new subsets: [[], [1]]; Add the second number (5) to all the existing subsets: [[], [1], [5], [1,5]]; Add the third number (3) to all the existing subsets: [[], [1], [5], [1,5], [3], [1,3], [5,3], [1,5,3]]. Here is the visual representation of the above steps: import java.util.* ; class Subsets { public static List < List < Integer >> findSubsets ( int [] nums ) { List < List < Integer >> subsets = new ArrayList <> (); // start by adding the empty subset subsets . add ( new ArrayList <> ()); for ( int currentNumber : nums ) { // we will take all existing subsets and insert the current number in them to // create new subsets int n = subsets . size (); for ( int i = 0 ; i < n ; i ++ ) { // create a new subset from the existing subset and // insert the current element to it List < Integer > set = new ArrayList <> ( subsets . get ( i )); set . add ( currentNumber ); subsets . add ( set ); } } return subsets ; } public static void main ( String [] args ) { List < List < Integer >> result = Subsets . findSubsets ( new int [] { 1 , 3 }); System . out . println ( \"Here is the list of subsets: \" + result ); result = Subsets . findSubsets ( new int [] { 1 , 5 , 3 }); System . out . println ( \"Here is the list of subsets: \" + result ); } } 4 - If Trees or Graphs Most of the questions related to Trees or Graphs can be solved through BFS or DFS Sample Problem Binary Tree Path Sum Problem Statement: Given a binary tree and a number S, find if the tree has a path from root-to-leaf such that the sum of all the node values of that path equals S. Solution As we are trying to search for a root-to-leaf path, we can use the Depth First Search (DFS) technique to solve this problem. To recursively traverse a binary tree in a DFS fashion, we can start from the root and, at every step, make two recursive calls, one for the left and one for the right child. Here are the steps for our Binary Tree Path Sum problem: Start DFS with the root of the tree. If the current node is not a leaf node, do two things: a) Subtract the value of the current node from the given number to get a new sum => S = S - node.value, b) Make two recursive calls for both the children of the current node with the new number calculated in the previous step. At every step, see if the current node being visited is a leaf node and if its value is equal to the given number S. If both are true, we have found the required root-to-leaf path, therefore return true. If the current node is a leaf, but its value is not equal to the given number S, return false. class TreeNode { int val ; TreeNode left ; TreeNode right ; TreeNode ( int x ) { val = x ; } }; class TreePathSum { public static boolean hasPath ( TreeNode root , int sum ) { if ( root == null ) return false ; // if current node is a leaf and its value is equal to the sum, we've found a path if ( root . val == sum && root . left == null && root . right == null ) return true ; // recursively call to traverse the left and right sub-tree // return true if any of the two recursive call return true return hasPath ( root . left , sum - root . val ) || hasPath ( root . right , sum - root . val ); } public static void main ( String [] args ) { TreeNode root = new TreeNode ( 12 ); root . left = new TreeNode ( 7 ); root . right = new TreeNode ( 1 ); root . left . left = new TreeNode ( 9 ); root . right . left = new TreeNode ( 10 ); root . right . right = new TreeNode ( 5 ); System . out . println ( \"Tree has path: \" + TreePathSum . hasPath ( root , 23 )); System . out . println ( \"Tree has path: \" + TreePathSum . hasPath ( root , 16 )); } } 5 - Recursive to Iterative Every recursive solution can be converted to an iterative solution using a Stack 6 - Brute force with O(n2) and O(1) space If a problem where a brute force solution exists in O(n2) time and O(1) space, there must exist other 2 solutions: Using Map or a Set for O(n) time and O(n)space Using sorting for O(nlogn) time and O(1) space 7 - If optimization If the problem is asking for optimization (e.g. maximization or minimization ), we will use Dynamic Programming to solve it. 8 - Common substring among a set If we need to find common substring among a set of strings, we will be using a HashMap or a Trie. 9 - Search in strings If we need to search among a bunch of strings, Trie will be the best DS 10 - If a LinkedList If the problem involves a LinkedList and we can\u00b4t use extra space, then use Fast&Slow pointer approach","title":"Strategies for Solving Problems"},{"location":"Exe/pyStrategies/#1-if-input-is-sorted","text":"If the given input is sorted (array, list, or matrix), we will use a variation of Binary Search or a Two Pointers strategy. Sample Problem for Binary Search Bitonic array maximum Problem Statement: Find the maximum value in a given Bitonic array. An array is considered bitonic if it is monotonically increasing and then monotonically decreasing. Monotonically increasing or decreasing means that for any index i in the array, arr[i] != arr[i+1]. Example: Input: [1, 3, 8, 12, 4, 2], Output: 12 Solution A bitonic array is a sorted array; the only difference is that its first part is sorted in ascending order, and the second part is sorted in descending order. We can use a variation of Binary Search to solve this problem. Remember that in Binary Search, we have start, end, and middle indices, and in each step, we reduce our search space by moving start or end. Since no two consecutive numbers are the same (as the array is monotonically increasing or decreasing), whenever we calculate the middle index for Binary Search, we can compare the numbers pointed out by the index middle and middle+1 to find if we are in the ascending or the descending part. So: If arr[middle] > arr[middle + 1], we are in the second (descending) part of the bitonic array. Therefore, our required number could either be pointed out by middle or will be before middle. This means we will do end = middle. If arr[middle] <= arr[middle + 1], we are in the first (ascending) part of the bitonic array. Therefore, the required number will be after middle. This means we do start = middle + 1. We can break when start == end. Due to the above two points, both start and end will point at the maximum number of the Bitonic array. class MaxInBitonicArray { public static int findMax ( int [] arr ) { int start = 0 , end = arr . length - 1 ; while ( start < end ) { int mid = start + ( end - start ) / 2 ; if ( arr [ mid ] > arr [ mid + 1 ] ) { end = mid ; } else { start = mid + 1 ; } } // at the end of the while loop, 'start == end' return arr [ start ] ; } public static void main ( String [] args ) { System . out . println ( MaxInBitonicArray . findMax ( new int [] { 1 , 3 , 8 , 12 , 4 , 2 })); System . out . println ( MaxInBitonicArray . findMax ( new int [] { 3 , 8 , 3 , 1 })); System . out . println ( MaxInBitonicArray . findMax ( new int [] { 1 , 3 , 8 , 12 })); System . out . println ( MaxInBitonicArray . findMax ( new int [] { 10 , 9 , 8 })); } } Sample Problem for Two Pointers Pair with the target sum Problem Statement: Given an array of sorted numbers and a target sum, find a pair in the array whose sum is equal to the given target. Write a function to return the indices of the two numbers (i.e., the pair) such that they add up to the given target. Example: Input: [1, 2, 3, 4, 6], target = 6, Output: [1, 3] (The numbers at index 1 and 3 add up to 6: 2+4=6) Solution Since the given array is sorted, a brute-force solution could be to iterate through the array, taking one number at a time and searching for the second number through Binary Search. The time complexity of this algorithm will be O(N*logN). Can we do better than this? We can follow the Two Pointers approach. We will start with one pointer pointing to the beginning of the array and another pointing at the end. At every step, we will see if the numbers pointed by the two pointers add up to the target sum. If they do, we\u2019ve found our pair. Otherwise, we\u2019ll do one of two things: If the sum of the two numbers pointed by the two pointers is greater than the target sum, we need a pair with a smaller sum. So, to try more pairs, we can decrement the end-pointer. If the sum of the two numbers pointed by the two pointers is smaller than the target sum, this means that we need a pair with a larger sum. So, to try more pairs, we can increment the start-pointer. Here is the visual representation of this algorithm for the example mentioned above: class PairWithTargetSum { public static int [] search ( int [] arr , int targetSum ) { int left = 0 , right = arr . length - 1 ; while ( left < right ) { // comparing the sum of two numbers to the 'targetSum' can cause integer overflow // so, we will try to find a target difference instead int targetDiff = targetSum - arr [ left ] ; if ( targetDiff == arr [ right ] ) return new int [] { left , right }; // found the pair if ( targetDiff > arr [ right ] ) left ++ ; // we need a pair with a bigger sum else right -- ; // we need a pair with a smaller sum } return new int [] { - 1 , - 1 }; } public static void main ( String [] args ) { int [] result = PairWithTargetSum . search ( new int [] { 1 , 2 , 3 , 4 , 6 }, 6 ); System . out . println ( \"Pair with target sum: [\" + result [ 0 ] + \", \" + result [ 1 ] + \"]\" ); result = PairWithTargetSum . search ( new int [] { 2 , 5 , 9 , 11 }, 11 ); System . out . println ( \"Pair with target sum: [\" + result [ 0 ] + \", \" + result [ 1 ] + \"]\" ); } }","title":"1 - If input is sorted"},{"location":"Exe/pyStrategies/#2-if-need-topmaxminclosest-k-ele","text":"If we\u2019re dealing with top/maximum/minimum/closest k elements among n elements, we will use a Heap. Sample Problem K closest points to the origin Problem Statement: Given an array of points in a 2D plane, find K closest points to the origin. Example: Input: points = [[1,2],[1,3]], K = 1, Output: [[1,2]] Solution The Euclidean distance of a point P(x,y) from the origin can be calculated through the following formula: We can use a Max Heap to find K points closest to the origin. We can start by pushing K points in the heap. While iterating through the remaining points, if a point (say P) is closer to the origin than the top point of the max-heap, we will remove that top point from the heap and add P to always keep the closest points in the heap. import java.util.* ; class Point { int x ; int y ; public Point ( int x , int y ) { this . x = x ; this . y = y ; } public int distFromOrigin () { // ignoring sqrt return ( x * x ) + ( y * y ); } } class KClosestPointsToOrigin { public static List < Point > findClosestPoints ( Point [] points , int k ) { PriorityQueue < Point > maxHeap = new PriorityQueue <> ( ( p1 , p2 ) -> p2 . distFromOrigin () - p1 . distFromOrigin ()); // put first 'k' points in the max heap for ( int i = 0 ; i < k ; i ++ ) maxHeap . add ( points [ i ] ); // go through the remaining points of the input array, if a point is closer to // the origin than the top point of the max-heap, remove the top point from // heap and add the point from the input array for ( int i = k ; i < points . length ; i ++ ) { if ( points [ i ] . distFromOrigin () < maxHeap . peek (). distFromOrigin ()) { maxHeap . poll (); maxHeap . add ( points [ i ] ); } } // the heap has 'k' points closest to the origin, return them in a list return new ArrayList <> ( maxHeap ); } public static void main ( String [] args ) { Point [] points = new Point [] { new Point ( 1 , 3 ), new Point ( 3 , 4 ), new Point ( 2 , - 1 ) }; List < Point > result = KClosestPointsToOrigin . findClosestPoints ( points , 2 ); System . out . print ( \"Here are the k points closest the origin: \" ); for ( Point p : result ) System . out . print ( \"[\" + p . x + \" , \" + p . y + \"] \" ); } }","title":"2 - If need top/max/min/closest k ele"},{"location":"Exe/pyStrategies/#3-if-need-all-combinations-or-perm","text":"If we need to try all combinations (or permutations) of the input, we can either use recursive Backtracking or iterative Breadth-First Search . Sample Problem Subsets Problem Statement: Given a set with distinct elements, find all of its distinct subsets. Example: Input: [1, 5, 3] Output: [], [1], [5], [3], [1,5], [1,3], [5,3], [1,5,3] Solution We can use the Breadth-First Search (BFS) approach to generate all subsets of the given set. We can start with an empty set, iterate through all numbers one-by-one, and add them to existing sets to create new subsets. Let\u2019s take the aforementioned example to go through each step of our algorithm: Given set: [1, 5, 3] Start with an empty set: [[]] Add the first number (1) to all the existing subsets to create new subsets: [[], [1]]; Add the second number (5) to all the existing subsets: [[], [1], [5], [1,5]]; Add the third number (3) to all the existing subsets: [[], [1], [5], [1,5], [3], [1,3], [5,3], [1,5,3]]. Here is the visual representation of the above steps: import java.util.* ; class Subsets { public static List < List < Integer >> findSubsets ( int [] nums ) { List < List < Integer >> subsets = new ArrayList <> (); // start by adding the empty subset subsets . add ( new ArrayList <> ()); for ( int currentNumber : nums ) { // we will take all existing subsets and insert the current number in them to // create new subsets int n = subsets . size (); for ( int i = 0 ; i < n ; i ++ ) { // create a new subset from the existing subset and // insert the current element to it List < Integer > set = new ArrayList <> ( subsets . get ( i )); set . add ( currentNumber ); subsets . add ( set ); } } return subsets ; } public static void main ( String [] args ) { List < List < Integer >> result = Subsets . findSubsets ( new int [] { 1 , 3 }); System . out . println ( \"Here is the list of subsets: \" + result ); result = Subsets . findSubsets ( new int [] { 1 , 5 , 3 }); System . out . println ( \"Here is the list of subsets: \" + result ); } }","title":"3 - If need all combinations (or perm)"},{"location":"Exe/pyStrategies/#4-if-trees-or-graphs","text":"Most of the questions related to Trees or Graphs can be solved through BFS or DFS Sample Problem Binary Tree Path Sum Problem Statement: Given a binary tree and a number S, find if the tree has a path from root-to-leaf such that the sum of all the node values of that path equals S. Solution As we are trying to search for a root-to-leaf path, we can use the Depth First Search (DFS) technique to solve this problem. To recursively traverse a binary tree in a DFS fashion, we can start from the root and, at every step, make two recursive calls, one for the left and one for the right child. Here are the steps for our Binary Tree Path Sum problem: Start DFS with the root of the tree. If the current node is not a leaf node, do two things: a) Subtract the value of the current node from the given number to get a new sum => S = S - node.value, b) Make two recursive calls for both the children of the current node with the new number calculated in the previous step. At every step, see if the current node being visited is a leaf node and if its value is equal to the given number S. If both are true, we have found the required root-to-leaf path, therefore return true. If the current node is a leaf, but its value is not equal to the given number S, return false. class TreeNode { int val ; TreeNode left ; TreeNode right ; TreeNode ( int x ) { val = x ; } }; class TreePathSum { public static boolean hasPath ( TreeNode root , int sum ) { if ( root == null ) return false ; // if current node is a leaf and its value is equal to the sum, we've found a path if ( root . val == sum && root . left == null && root . right == null ) return true ; // recursively call to traverse the left and right sub-tree // return true if any of the two recursive call return true return hasPath ( root . left , sum - root . val ) || hasPath ( root . right , sum - root . val ); } public static void main ( String [] args ) { TreeNode root = new TreeNode ( 12 ); root . left = new TreeNode ( 7 ); root . right = new TreeNode ( 1 ); root . left . left = new TreeNode ( 9 ); root . right . left = new TreeNode ( 10 ); root . right . right = new TreeNode ( 5 ); System . out . println ( \"Tree has path: \" + TreePathSum . hasPath ( root , 23 )); System . out . println ( \"Tree has path: \" + TreePathSum . hasPath ( root , 16 )); } }","title":"4 - If Trees or Graphs"},{"location":"Exe/pyStrategies/#5-recursive-to-iterative","text":"Every recursive solution can be converted to an iterative solution using a Stack","title":"5 - Recursive to Iterative"},{"location":"Exe/pyStrategies/#6-brute-force-with-on2-and-o1-space","text":"If a problem where a brute force solution exists in O(n2) time and O(1) space, there must exist other 2 solutions: Using Map or a Set for O(n) time and O(n)space Using sorting for O(nlogn) time and O(1) space","title":"6 - Brute force with O(n2) and O(1) space"},{"location":"Exe/pyStrategies/#7-if-optimization","text":"If the problem is asking for optimization (e.g. maximization or minimization ), we will use Dynamic Programming to solve it.","title":"7 - If optimization"},{"location":"Exe/pyStrategies/#8-common-substring-among-a-set","text":"If we need to find common substring among a set of strings, we will be using a HashMap or a Trie.","title":"8 - Common substring among a set"},{"location":"Exe/pyStrategies/#9-search-in-strings","text":"If we need to search among a bunch of strings, Trie will be the best DS","title":"9 - Search in strings"},{"location":"Exe/pyStrategies/#10-if-a-linkedlist","text":"If the problem involves a LinkedList and we can\u00b4t use extra space, then use Fast&Slow pointer approach","title":"10 - If a LinkedList"},{"location":"Func/pyAnn/","text":"PEP 3107 PEP stands for Python Enhancement Proposal. It is a design document that describes new features for Python or its processes or environment. It also provides information to the python community. PEP-3107 introduced the concept and syntax for adding arbitrary metadata annotations to Python. There's no preconceived use case, but the PEP suggests several. One very handy one is to allow you to annotate parameters with their expected types; it would then be easy to write a decorator that verifies the annotations or coerces the arguments to the right type. Another is to allow parameter-specific documentation instead of encoding it into the docstring. Function annotations are only supported in python 3x. Details here the -> marks the return function annotation. def kinetic_energy ( m : 'in KG' , v : 'in M/S' ) -> 'Joules' : return 1 / 2 * m * v ** 2 kinetic_energy . __annotations__ {'m': 'in KG', 'return': 'Joules', 'v': 'in M/S'} Annotations are dictionaries, so you can do this: ' {:,} {} ' . format ( kinetic_energy ( 12 , 30 ), kinetic_energy . __annotations__ [ 'return' ]) 5,400.0 Joules You can also have a python data structure rather than just a string: rd = { 'type' : float , 'units' : 'Joules' , 'docstring' : 'Given mass and velocity returns kinetic energy in Joules' } def f () -> rd : pass f . __annotations__ [ 'return' ][ 'type' ] float f . __annotations__ [ 'return' ][ 'units' ] Joules f . __annotations__ [ 'return' ][ 'docstring' ] Given mass and velocity returns kinetic energy in Joules Or, you can use function attributes to validate called values: def validate ( func , locals ): for var , test in func . __annotations__ . items (): value = locals [ var ] try : pr = test . __name__ + ': ' + test . __docstring__ except AttributeError : pr = test . __name__ msg = ' {} == {} ; Test: {} ' . format ( var , value , pr ) assert test ( value ), msg def between ( lo , hi ): def _between ( x ): return lo <= x <= hi _between . __docstring__ = 'must be between {} and {} ' . format ( lo , hi ) return _between def f ( x : between ( 3 , 10 ), y : lambda _y : isinstance ( _y , int )): validate ( f , locals ()) print ( x , y ) f ( 2 , 2 ) --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-8-e43489394a12> in <module>() ----> 1 f(2,2) 1 frames <ipython-input-7-b73806aca747> in validate(func, locals) 7 pr=test.__name__ 8 msg = '{}=={}; Test: {}'.format(var, value, pr) ----> 9 assert test(value), msg 10 11 def between(lo, hi): AssertionError: x==2; Test: _between: must be between 3 and 10 f ( 3 , 2.1 ) --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-9-d705e73c1fc6> in <module>() ----> 1 f(3,2.1) 1 frames <ipython-input-7-b73806aca747> in validate(func, locals) 7 pr=test.__name__ 8 msg = '{}=={}; Test: {}'.format(var, value, pr) ----> 9 assert test(value), msg 10 11 def between(lo, hi): AssertionError: y==2.1; Test: <lambda> f ( 3 , 4 ) 3 4","title":"Annotations"},{"location":"Func/pyCtxtMgr/","text":"The with Keyword Context managers are a really helpful structure in Python. Each context manager executes specific code before and after the statements you specify. To use one, you use the with keyword: with <context manager> as <var>: <statements> Using with gives you a way to define code to be executed within the context manager\u2019s scope. The most basic example of this is when you\u2019re working with file I/O in Python. If you wanted to open a file, do something with that file, and then make sure that the file was closed correctly, then you would use a context manager. Consider this example in which names.txt contains a list of names, one per line: with open ( \"names.txt\" ) as input_file : for name in input_file : print ( name . strip ()) The file I/O context manager provided by open() and initiated with the with keyword opens the file for reading, assigns the open file pointer to input_file, then executes whatever code you specify in the with block. Then, after the block is executed, the file pointer closes. Even if your code in the with block raises an exception, the file pointer would still close. The as Keyword If you want access to the results of the expression or context manager passed to with, you\u2019ll need to alias it using as. You may have also seen as used to alias imports and exceptions, and this is no different. The alias is available in the with block: with <expr> as <alias>: <statements> Most of the time, you\u2019ll see these two Python keywords, with and as, used together.","title":"Context Managers"},{"location":"Func/pyCtxtMgr/#the-with-keyword","text":"Context managers are a really helpful structure in Python. Each context manager executes specific code before and after the statements you specify. To use one, you use the with keyword: with <context manager> as <var>: <statements> Using with gives you a way to define code to be executed within the context manager\u2019s scope. The most basic example of this is when you\u2019re working with file I/O in Python. If you wanted to open a file, do something with that file, and then make sure that the file was closed correctly, then you would use a context manager. Consider this example in which names.txt contains a list of names, one per line: with open ( \"names.txt\" ) as input_file : for name in input_file : print ( name . strip ()) The file I/O context manager provided by open() and initiated with the with keyword opens the file for reading, assigns the open file pointer to input_file, then executes whatever code you specify in the with block. Then, after the block is executed, the file pointer closes. Even if your code in the with block raises an exception, the file pointer would still close.","title":"The with Keyword"},{"location":"Func/pyCtxtMgr/#the-as-keyword","text":"If you want access to the results of the expression or context manager passed to with, you\u2019ll need to alias it using as. You may have also seen as used to alias imports and exceptions, and this is no different. The alias is available in the with block: with <expr> as <alias>: <statements> Most of the time, you\u2019ll see these two Python keywords, with and as, used together.","title":"The as Keyword"},{"location":"Func/pyDecor/","text":"A Decorator is a callable that takes another function as an argument, extending the behavior of that function without explicitly modifying that function. Decorators provide a way to modify functions using other functions. This is ideal when you need to extend the functionality of functions that you don't want to modify. Functions within functions def fib_3 ( a , b , c ): def get_3 (): return a , b , c return get_3 fib_3 ( 1 , 1 , 2 ) <function __main__.fib_3.<locals>.get_3> f = fib_3 ( 1 , 1 , 2 ) f () (1, 1, 2) We defined a function named decor that has a single parameter func. Inside decor, we defined a nested function named wrap. The wrap function will print a string, then call func(), and print another string. The decor function returns the wrap function as its result. We could say that the variable decorated is a decorated version of print_text - it's print_text plus something. def decor ( func ): def wrap (): '''This is the wrapper''' #Do something before print ( \"***************\" ) func () #Do something after print ( \"***************\" ) return wrap def print_text (): '''Prints Hello''' print ( \"Hello\" ) decorated = decor ( print_text ) decorated () *************** Hello *************** print_text . __name__ print_text print_text . __doc__ Prints Hello Python provides support to wrap a function in a decorator by pre-pending the function definition with a decorator name and the @ symbol. If we are defining a function we can \"decorate\" it with the @ symbol like. A single function can have multiple decorators. def print_text (): print ( \"Hello\" ) print_text = decor ( print_text ) print_text () *************** Hello *************** Is equivalent to: @decor def print_text (): '''Prints Hello''' print ( \"Hello\" ) print_text () *************** Hello *************** print_text . __name__ wrap print_text . __doc__ This is the wrapper To avoid this problem use functools from functools import wraps def decor ( func ): @wraps ( func ) def wrap (): '''This is the wrapper''' #Do something before print ( \"***************\" ) func () #Do something after print ( \"***************\" ) return wrap @decor def print_text (): '''Prints Hello''' print ( \"Hello\" ) print_text () *************** Hello *************** print_text . __name__ print_text print_text . __doc__ Prints Hello Decorators with arguments def pfib ( a , b , c ): print ( a , b , c ) pfib ( 1 , 1 , 2 ) 1 1 2 If I dont want only 3 args but more def pfib ( a , * args ): print ( a ) print ( args ) pfib ( 1 , 1 , 2 , 3 ) 1 (1, 2, 3) def pfib ( a , ** kwargs ): print ( a ) print ( kwargs ) pfib ( 1 , se = 1 , th = 2 , fo = 3 , fi = 5 ) 1 {'se': 1, 'th': 2, 'fo': 3, 'fi': 5} def pfib ( * args , ** kwargs ): print ( args ) print ( kwargs ) pfib ( 1 , 2 , 2 , se = 1 , th = 2 , fo = 3 , fi = 5 ) (1, 2, 2) {'se': 1, 'th': 2, 'fo': 3, 'fi': 5} def wrapper ( * args , ** kwargs ): print ( * args ) print ( 'Leaving wrapper' ) pfib ( * args , ** kwargs ) print ( kwargs ) wrapper ( 1 , 1 , th = 2 ) 1 1 Leaving wrapper (1, 1) {'th': 2} {'th': 2} The template is: from functools import wraps def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): #Do something before result = func ( * args , ** kwargs ) #Do something after return result return wrapper @decorator def func (): pass Decorators with classes We can define a decorator as a class in order to do that, we have to use a call method of classes. # Python program showing # use of __call__() method class MyDecorator : def __init__ ( self , function ): self . function = function def __call__ ( self ): # We can add some code # before function call self . function () # We can also add some code # after function call. # adding class decorator to the function @MyDecorator def function (): print ( \"Meps3\" ) function () Meps3 # Python program showing # class decorator with *args # and **kwargs class MyDecorator : def __init__ ( self , function ): self . function = function def __call__ ( self , * args , ** kwargs ): # We can add some code # before function call self . function ( * args , ** kwargs ) # We can also add some code # after function call. # adding class decorator to the function @MyDecorator def function ( name , message = 'Hello' ): print ( \" {} , {} \" . format ( message , name )) function ( \"gpes3\" , \"hello\" ) hello, gpes3 # Python program to execute # time of a program # importing time module from time import time class Timer : def __init__ ( self , func ): self . function = func def __call__ ( self , * args , ** kwargs ): start_time = time () result = self . function ( * args , ** kwargs ) end_time = time () print ( \"Execution took {} seconds\" . format ( end_time - start_time )) return result # adding a decorator to the function @Timer def some_function ( delay ): from time import sleep # Introducing some time delay to # simulate a time taking function. sleep ( delay ) some_function ( 3 ) Execution took 3.003098487854004 seconds Decorators as a cache: Memoization LRU is the cache replacement algorithm that removes the least recently used data and stores the new data. Suppose we have a cache space of 10 memory frames. And each frame is filled with a file. Now if we want to store the new file, we need to remove the oldest file in the cache and add the new file. This is how LRU works. LRU cache consists of Queue and Dictionary data structures. Queue: to store the most recently used to least recently used files Hash table: to store the file and its position in the cache lru_cache() is one such function in functools module which helps in reducing the execution time of the function by using memoization technique. from functools import lru_cache import time # Function that computes Fibonacci # numbers without lru_cache def fib_without_cache ( n ): if n < 2 : return n return fib_without_cache ( n - 1 ) + fib_without_cache ( n - 2 ) # Execution start time begin = time . time () fib_without_cache ( 30 ) # Execution end time end = time . time () print ( \"Time taken to execute the \\ function without lru_cache is\" , end - begin ) # Function that computes Fibonacci # numbers with lru_cache @lru_cache ( maxsize = 128 ) def fib_with_cache ( n ): if n < 2 : return n return fib_with_cache ( n - 1 ) + fib_with_cache ( n - 2 ) begin = time . time () fib_with_cache ( 30 ) end = time . time () print ( \"Time taken to execute the \\ function with lru_cache is\" , end - begin ) Time taken to execute the function without lru_cache is 0.3661181926727295 Time taken to execute the function with lru_cache is 7.653236389160156e-05","title":"Decorators"},{"location":"Func/pyDecor/#functions-within-functions","text":"def fib_3 ( a , b , c ): def get_3 (): return a , b , c return get_3 fib_3 ( 1 , 1 , 2 ) <function __main__.fib_3.<locals>.get_3> f = fib_3 ( 1 , 1 , 2 ) f () (1, 1, 2) We defined a function named decor that has a single parameter func. Inside decor, we defined a nested function named wrap. The wrap function will print a string, then call func(), and print another string. The decor function returns the wrap function as its result. We could say that the variable decorated is a decorated version of print_text - it's print_text plus something. def decor ( func ): def wrap (): '''This is the wrapper''' #Do something before print ( \"***************\" ) func () #Do something after print ( \"***************\" ) return wrap def print_text (): '''Prints Hello''' print ( \"Hello\" ) decorated = decor ( print_text ) decorated () *************** Hello *************** print_text . __name__ print_text print_text . __doc__ Prints Hello Python provides support to wrap a function in a decorator by pre-pending the function definition with a decorator name and the @ symbol. If we are defining a function we can \"decorate\" it with the @ symbol like. A single function can have multiple decorators. def print_text (): print ( \"Hello\" ) print_text = decor ( print_text ) print_text () *************** Hello *************** Is equivalent to: @decor def print_text (): '''Prints Hello''' print ( \"Hello\" ) print_text () *************** Hello *************** print_text . __name__ wrap print_text . __doc__ This is the wrapper To avoid this problem use functools from functools import wraps def decor ( func ): @wraps ( func ) def wrap (): '''This is the wrapper''' #Do something before print ( \"***************\" ) func () #Do something after print ( \"***************\" ) return wrap @decor def print_text (): '''Prints Hello''' print ( \"Hello\" ) print_text () *************** Hello *************** print_text . __name__ print_text print_text . __doc__ Prints Hello","title":"Functions within functions"},{"location":"Func/pyDecor/#decorators-with-arguments","text":"def pfib ( a , b , c ): print ( a , b , c ) pfib ( 1 , 1 , 2 ) 1 1 2 If I dont want only 3 args but more def pfib ( a , * args ): print ( a ) print ( args ) pfib ( 1 , 1 , 2 , 3 ) 1 (1, 2, 3) def pfib ( a , ** kwargs ): print ( a ) print ( kwargs ) pfib ( 1 , se = 1 , th = 2 , fo = 3 , fi = 5 ) 1 {'se': 1, 'th': 2, 'fo': 3, 'fi': 5} def pfib ( * args , ** kwargs ): print ( args ) print ( kwargs ) pfib ( 1 , 2 , 2 , se = 1 , th = 2 , fo = 3 , fi = 5 ) (1, 2, 2) {'se': 1, 'th': 2, 'fo': 3, 'fi': 5} def wrapper ( * args , ** kwargs ): print ( * args ) print ( 'Leaving wrapper' ) pfib ( * args , ** kwargs ) print ( kwargs ) wrapper ( 1 , 1 , th = 2 ) 1 1 Leaving wrapper (1, 1) {'th': 2} {'th': 2} The template is: from functools import wraps def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): #Do something before result = func ( * args , ** kwargs ) #Do something after return result return wrapper @decorator def func (): pass","title":"Decorators with arguments"},{"location":"Func/pyDecor/#decorators-with-classes","text":"We can define a decorator as a class in order to do that, we have to use a call method of classes. # Python program showing # use of __call__() method class MyDecorator : def __init__ ( self , function ): self . function = function def __call__ ( self ): # We can add some code # before function call self . function () # We can also add some code # after function call. # adding class decorator to the function @MyDecorator def function (): print ( \"Meps3\" ) function () Meps3 # Python program showing # class decorator with *args # and **kwargs class MyDecorator : def __init__ ( self , function ): self . function = function def __call__ ( self , * args , ** kwargs ): # We can add some code # before function call self . function ( * args , ** kwargs ) # We can also add some code # after function call. # adding class decorator to the function @MyDecorator def function ( name , message = 'Hello' ): print ( \" {} , {} \" . format ( message , name )) function ( \"gpes3\" , \"hello\" ) hello, gpes3 # Python program to execute # time of a program # importing time module from time import time class Timer : def __init__ ( self , func ): self . function = func def __call__ ( self , * args , ** kwargs ): start_time = time () result = self . function ( * args , ** kwargs ) end_time = time () print ( \"Execution took {} seconds\" . format ( end_time - start_time )) return result # adding a decorator to the function @Timer def some_function ( delay ): from time import sleep # Introducing some time delay to # simulate a time taking function. sleep ( delay ) some_function ( 3 ) Execution took 3.003098487854004 seconds","title":"Decorators with classes"},{"location":"Func/pyDecor/#decorators-as-a-cache-memoization","text":"LRU is the cache replacement algorithm that removes the least recently used data and stores the new data. Suppose we have a cache space of 10 memory frames. And each frame is filled with a file. Now if we want to store the new file, we need to remove the oldest file in the cache and add the new file. This is how LRU works. LRU cache consists of Queue and Dictionary data structures. Queue: to store the most recently used to least recently used files Hash table: to store the file and its position in the cache lru_cache() is one such function in functools module which helps in reducing the execution time of the function by using memoization technique. from functools import lru_cache import time # Function that computes Fibonacci # numbers without lru_cache def fib_without_cache ( n ): if n < 2 : return n return fib_without_cache ( n - 1 ) + fib_without_cache ( n - 2 ) # Execution start time begin = time . time () fib_without_cache ( 30 ) # Execution end time end = time . time () print ( \"Time taken to execute the \\ function without lru_cache is\" , end - begin ) # Function that computes Fibonacci # numbers with lru_cache @lru_cache ( maxsize = 128 ) def fib_with_cache ( n ): if n < 2 : return n return fib_with_cache ( n - 1 ) + fib_with_cache ( n - 2 ) begin = time . time () fib_with_cache ( 30 ) end = time . time () print ( \"Time taken to execute the \\ function with lru_cache is\" , end - begin ) Time taken to execute the function without lru_cache is 0.3661181926727295 Time taken to execute the function with lru_cache is 7.653236389160156e-05","title":"Decorators as a cache: Memoization"},{"location":"Func/pyDoc/","text":"Docstrings (documentation strings) serve a similar purpose to comments, as they are designed to explain code. However, they are more specific and have a different syntax. They are created by putting a multiline string containing an explanation of the function below the function's first line. Unlike conventional comments, docstrings are retained throughout the runtime of the program. This allows the programmer to inspect these comments at run time. def sumt ( x , y ): \"\"\" Multiplies by two the sum of parameters Parameters ---------- x : int First Number. y : int Second Number. Returns ------- z : int Result of the operation Examples -------- >>> sumt(2, 3) 12 \"\"\" return ( x + y ) * 2 The docstring is a triple-quoted string (which may span multiple lines) that comes immediately after the header of a function. When we call help() or .doc on a function, it shows the docstring. help ( sumt ) Help on function sumt in module __main__: sumt(x, y) Multiplies by two the sum of parameters Parameters ---------- x : int First Number. y : int Second Number. Returns ------- z : int Result of the operation Examples -------- >>> sumt(2, 3) 12 print ( sumt . __doc__ ) Multiplies by two the sum of parameters Parameters ---------- x : int First Number. y : int Second Number. Returns ------- z : int Result of the operation Examples -------- >>> sumt(2, 3) 12","title":"Documentation"},{"location":"Func/pyFnArgs/","text":"Python allows to have function with varying number of arguments. Using args as a function parameter enables you to pass an arbitrary number of arguments to that function. The arguments are then accessible as the tuple args in the body of the function. The parameter args must come after the named parameters to a function. The name args is just a convention; you can choose to use another. *args is called an argument list. def function ( named_args , * args ): print ( named_args ) print ( args ) function ( 1 , 2 , 3 , 4 , 5 , 6 , 7 ) 1 (2, 3, 4, 5, 6, 7) Default Values Named parameters to a function can be made optional by giving them a default value. These must come after named parameters without a default value. In case the argument is passed in, the default value is ignored. If the argument is not passed in, the default value is used. def function ( x , y , food = \"spam\" ): print ( food ) function ( 1 , 2 ) function ( 2 , 3 , \"eggs\" ) spam eggs Keyword Arguments kwargs (standing for keyword arguments) allows you to handle named arguments that you have not defined in advance. The keyword arguments return a **dictionary in which the keys are the argument names, and the values are the argument values. The arguments returned by * kwargs are not included in args def my_func ( x , y = 7 , * args , ** kwargs ): print ( kwargs ) my_func ( 2 , 3 , 4 , 5 , 6 , a = 7 , b = 8 ) #a and b are the names of the arguments that we passed to the function call. {'a': 7, 'b': 8} Keyword-only Arguments These are only specifiable via the name of the argument, and cannot be specified as a positional argument. For example, the following function takes a positional argument and two keyword-only arguments def keyword_only_function ( parameter , * , option1 = False , option2 = '' ): pass In this example, option1, and option2 are only specifiable via the keyword argument syntax. The following is valid keyword_only_function ( 3 , option1 = True , option2 = 'Hello World!' ) But this example will raise an error keyword_only_function ( 3 , True , 'Hello World!' ) Passing Arguments By Value and By Reference Manipulating provided arguments inside the function will leave immutable arguments intact, but will update mutable arguments. Lists are mutable, int is inmutable. Passing List as arguments is similar to passing by reference. Passing int is similar to passing by value. def Func ( a , b ): a = 1 b [ 0 ] = 1 x = 0 y = [ 0 ] Func ( x , y ) print ( x , y ) 0 [1]","title":"Arguments"},{"location":"Func/pyFnArgs/#default-values","text":"Named parameters to a function can be made optional by giving them a default value. These must come after named parameters without a default value. In case the argument is passed in, the default value is ignored. If the argument is not passed in, the default value is used. def function ( x , y , food = \"spam\" ): print ( food ) function ( 1 , 2 ) function ( 2 , 3 , \"eggs\" ) spam eggs","title":"Default Values"},{"location":"Func/pyFnArgs/#keyword-arguments","text":"kwargs (standing for keyword arguments) allows you to handle named arguments that you have not defined in advance. The keyword arguments return a **dictionary in which the keys are the argument names, and the values are the argument values. The arguments returned by * kwargs are not included in args def my_func ( x , y = 7 , * args , ** kwargs ): print ( kwargs ) my_func ( 2 , 3 , 4 , 5 , 6 , a = 7 , b = 8 ) #a and b are the names of the arguments that we passed to the function call. {'a': 7, 'b': 8}","title":"Keyword Arguments"},{"location":"Func/pyFnArgs/#keyword-only-arguments","text":"These are only specifiable via the name of the argument, and cannot be specified as a positional argument. For example, the following function takes a positional argument and two keyword-only arguments def keyword_only_function ( parameter , * , option1 = False , option2 = '' ): pass In this example, option1, and option2 are only specifiable via the keyword argument syntax. The following is valid keyword_only_function ( 3 , option1 = True , option2 = 'Hello World!' ) But this example will raise an error keyword_only_function ( 3 , True , 'Hello World!' )","title":"Keyword-only Arguments"},{"location":"Func/pyFnArgs/#passing-arguments-by-value-and-by-reference","text":"Manipulating provided arguments inside the function will leave immutable arguments intact, but will update mutable arguments. Lists are mutable, int is inmutable. Passing List as arguments is similar to passing by reference. Passing int is similar to passing by value. def Func ( a , b ): a = 1 b [ 0 ] = 1 x = 0 y = [ 0 ] Func ( x , y ) print ( x , y ) 0 [1]","title":"Passing Arguments By Value and By Reference"},{"location":"Func/pyFnTools/","text":"Functools module is for higher-order functions that work on other functions. It provides functions for working with other functions and callable objects to use or extend them without completely rewriting them. Reduce functools.reduce(function, iterable[, initializer]) Apply function of two arguments cumulatively to the items of iterable, from left to right, so as to reduce the iterable to a single value. For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates ((((1+2)+3)+4)+5). If the optional initializer is present, it is placed before the items of the iterable in the calculation, and serves as a default when the iterable is empty. If initializer is not given and iterable contains only one item, the first item is returned. Roughly equivalent to: def reduce ( function , iterable , initializer = None ): it = iter ( iterable ) if initializer is None : value = next ( it ) else : value = initializer for element in it : value = function ( value , element ) return value See itertools.accumulate() for an iterator that yields all intermediate values.","title":"Functools"},{"location":"Func/pyFnTools/#reduce","text":"functools.reduce(function, iterable[, initializer]) Apply function of two arguments cumulatively to the items of iterable, from left to right, so as to reduce the iterable to a single value. For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates ((((1+2)+3)+4)+5). If the optional initializer is present, it is placed before the items of the iterable in the calculation, and serves as a default when the iterable is empty. If initializer is not given and iterable contains only one item, the first item is returned. Roughly equivalent to: def reduce ( function , iterable , initializer = None ): it = iter ( iterable ) if initializer is None : value = next ( it ) else : value = initializer for element in it : value = function ( value , element ) return value See itertools.accumulate() for an iterator that yields all intermediate values.","title":"Reduce"},{"location":"Func/pyFuncObj/","text":"Functions can also be used as arguments of other functions. def add ( x , y ): return x + y def twice ( func , x , y ): return func ( func ( x , y ), func ( x , y )) a = 2 b = 3 print ( twice ( add , a , b )) 10 Functions that operate on other functions are called \"higher-order functions.\" there are higher-order functions built into Python that you might find useful to call. Here's an interesting example using the max function. By default, max returns the largest of its arguments. But if we pass in a function using the optional key argument, it returns the argument x that maximizes key(x) (aka the 'argmax'). def mod_5 ( x ): \"\"\"Return the remainder of x after dividing by 5\"\"\" return x % 5 print ( 'Which number is biggest?' , max ( 100 , 51 , 14 ), 'Which number is the biggest modulo 5?' , max ( 100 , 51 , 14 , key = mod_5 ), sep = ' \\n ' , ) Which number is biggest? 100 Which number is the biggest modulo 5? 14","title":"Function As Objects"},{"location":"Func/pyGen/","text":"The help() function is possibly the most important Python function you can learn. If you can remember how to use help(), you hold the key to understanding most other functions. help ( round ) Help on built-in function round in module builtins: round(number, ndigits=None) Round a number to a given precision in decimal digits. The return value is an integer if ndigits is omitted or None. Otherwise the return value has the same type as the number. ndigits may be negative. You can create your own functions by using the def statement. The code block within every function starts with a colon (:) and is indented. You must define functions before they are called, in the same way that you must assign variables before using them. def my_func (): print ( \"spam \" ) print ( \"spam \" ) my_func () def excla ( word ): print ( \"Spam with \" + word + \"!\" ) excla ( \"eggs\" ) def sumtwice ( x , y ): return ( x + y ) * 2 print ( sumtwice ( 3 , 4 )) spam spam Spam with eggs! 14","title":"General"},{"location":"Func/pyLambdas/","text":"Creating a function normally (using def) assigns it to a variable automatically. This is different from the creation of other objects - such as strings and integers - which can be created on the fly, without assigning them to a variable. The same is possible with functions, provided that they are created using lambda syntax. Functions created this way are known as anonymous . This approach is most commonly used when passing a simple function as an argument to another function. The syntax is shown in the next example and consists of the lambda keyword followed by a list of arguments, a colon, and the expression to evaluate and return. def my_func ( f , arg ): return f ( arg ) my_func ( lambda x : 2 * x * x , 5 ) 50 Lambda functions aren't as powerful as named functions. They can only do things that require a single expression - usually equivalent to a single line of code. Lambda functions can be assigned to variables, and used like normal functions. #It is usually better to define a function with def instead. ec = lambda x : x * x * 2 + 4 * x + 3 print ( ec ( 8 )) 163","title":"Lambdas"},{"location":"Func/pyMain/","text":"Most Python code is either a module to be imported, or a script that does something. However, sometimes it is useful to make a file that can be both imported as a module and run as a script. To do this, place script code inside if name == \"main\". This ensures that it won't be run if the file is imported. When the Python interpreter reads a source file, it executes all of the code it finds in the file. Before executing the code, it defines a few special variables. For example, if the Python interpreter is running that module (the source file) as the main program, it sets the special name variable to have a value \"main\". If this file is being imported from another module, name will be set to the module's name. def function (): print ( \"This is a module function!\" ) if __name__ == \"__main__\" : print ( \"This is a script\" ) This is a script If we save the code from our previous example as a file called example.py, we can then import it to another script as a module, using the name example. import example example . function () And the result will be This is a module function Before executing code, Python interpreter reads source file and define few special variables/global variables. If the python interpreter is running that module (the source file) as the main program, it sets the special __name__ variable to have a value \u201c__main__\u201d. If this file is being imported from another module, __name__ will be set to the module\u2019s name. Module\u2019s name is available as value to __name__ global variable. A module is a file containing Python definitions and statements. The file name is the module name with the suffix .py appended. # Python program to execute # function directly def my_function (): print ( \"I am inside function\" ) # We can test function by calling it. my_function () Now if we want to use that module by importing we have to comment out our call. Rather than that approach best approach is to use following code: # Python program to use # main for function call. if __name__ == \"__main__\" : my_function () import myscript myscript . my_function () Advantages : Every Python module has it\u2019s __name__ defined and if this is \u2018__main__\u2019, it implies that the module is being run standalone by the user and we can do corresponding appropriate actions. If you import this script as a module in another script, the __name__ is set to the name of the script/module. Python files can act as either reusable modules, or as standalone programs. if __name__ == \u201cmain\u201d: is used to execute some code only if the file was run directly, and not imported.","title":"Main"},{"location":"Func/pyNumFunc/","text":"To find the maximum or minimum of some numbers or a list, you can use max or min. To find the distance of a number from zero (its absolute value), use abs. To round a number to a certain number of decimal places, use round. To find the total of a list, use sum. Numeric Functions x =- 47.3 y = abs ( x ) print ( f 'x is { type ( x ) } ' ) print ( f 'x is { x } ' ) print ( f 'x is { type ( y ) } ' ) print ( f 'x is { y } ' ) x is <class 'float'> x is -47.3 x is <class 'float'> x is 47.3 x = 47 y = divmod ( x , 3 ) print ( f 'x is { type ( x ) } ' ) print ( f 'x is { x } ' ) print ( f 'x is { type ( y ) } ' ) print ( f 'x is { y } ' ) x is <class 'int'> x is 47 x is <class 'tuple'> x is (15, 2) x = 47 y = x + 73 j #y=complex(x, 73) print ( f 'x is { type ( x ) } ' ) print ( f 'x is { x } ' ) print ( f 'x is { type ( y ) } ' ) print ( f 'x is { y } ' ) x is <class 'int'> x is 47 x is <class 'complex'> x is (47+73j) Mathematical Functions Mathematical functions import math Mathematical functions for complex numbers import cmath print ( min ( 0 , 1 , 2 , 3 , - 1 , 4 , 5 , 6 )) print ( min ([ 0 , 1 , 2 , 3 , - 1 , 4 , 5 , 6 ])) print ( abs ( - 54 )) print ( round ( 0.6464 , 2 )) print ( sum ([ 0 , 1 , 4 , 5 , - 3 , 9 ])) -1 -1 54 0.65 16 Statistics For more advanced function use numpy # using Python statistics functions import statistics # simple statistics operations sample_data1 = [ 1 , 3 , 5 , 7 ] sample_data2 = [ 2 , 3 , 5 , 4 , 3 , 5 , 3 , 2 , 5 , 6 , 4 , 3 ] # Use the mean function - calculates an average value print ( statistics . mean ( sample_data1 )) # Use the different median functions print ( statistics . median ( sample_data1 )) print ( statistics . median_low ( sample_data1 )) print ( statistics . median_high ( sample_data1 )) # The mode function indicates which data item occurs # most frequently print ( statistics . mode ( sample_data2 )) 4 4.0 3 5 3","title":"Numeric Functions"},{"location":"Func/pyNumFunc/#numeric-functions","text":"x =- 47.3 y = abs ( x ) print ( f 'x is { type ( x ) } ' ) print ( f 'x is { x } ' ) print ( f 'x is { type ( y ) } ' ) print ( f 'x is { y } ' ) x is <class 'float'> x is -47.3 x is <class 'float'> x is 47.3 x = 47 y = divmod ( x , 3 ) print ( f 'x is { type ( x ) } ' ) print ( f 'x is { x } ' ) print ( f 'x is { type ( y ) } ' ) print ( f 'x is { y } ' ) x is <class 'int'> x is 47 x is <class 'tuple'> x is (15, 2) x = 47 y = x + 73 j #y=complex(x, 73) print ( f 'x is { type ( x ) } ' ) print ( f 'x is { x } ' ) print ( f 'x is { type ( y ) } ' ) print ( f 'x is { y } ' ) x is <class 'int'> x is 47 x is <class 'complex'> x is (47+73j)","title":"Numeric Functions"},{"location":"Func/pyNumFunc/#mathematical-functions","text":"Mathematical functions import math Mathematical functions for complex numbers import cmath print ( min ( 0 , 1 , 2 , 3 , - 1 , 4 , 5 , 6 )) print ( min ([ 0 , 1 , 2 , 3 , - 1 , 4 , 5 , 6 ])) print ( abs ( - 54 )) print ( round ( 0.6464 , 2 )) print ( sum ([ 0 , 1 , 4 , 5 , - 3 , 9 ])) -1 -1 54 0.65 16","title":"Mathematical Functions"},{"location":"Func/pyNumFunc/#statistics","text":"For more advanced function use numpy # using Python statistics functions import statistics # simple statistics operations sample_data1 = [ 1 , 3 , 5 , 7 ] sample_data2 = [ 2 , 3 , 5 , 4 , 3 , 5 , 3 , 2 , 5 , 6 , 4 , 3 ] # Use the mean function - calculates an average value print ( statistics . mean ( sample_data1 )) # Use the different median functions print ( statistics . median ( sample_data1 )) print ( statistics . median_low ( sample_data1 )) print ( statistics . median_high ( sample_data1 )) # The mode function indicates which data item occurs # most frequently print ( statistics . mode ( sample_data2 )) 4 4.0 3 5 3","title":"Statistics"},{"location":"Func/pyRegex/","text":"Regular expressions are a powerful tool for various kinds of string manipulation. They are a domain specific language (DSL) that is present as a library in most modern programming languages, not just Python. Regex Site They are useful for two main tasks: - verifying that strings match a pattern (for instance, that a string has the format of an email address), - performing substitutions in a string (such as changing all American spellings to British ones). Domain specific languages are highly specialized mini programming languages. Regular expressions are a popular example, and SQL (for database manipulation) is another. Private domain-specific languages are often used for specific industrial purposes. Regular expressions in Python can be accessed using the re module, which is part of the standard library. After you've defined a regular expression, the re.match function can be used to determine whether it matches at the beginning of a string. If it does, match returns an object representing the match, if not, it returns None. To avoid any confusion while working with regular expressions, we would use raw strings as r\"expression\". Raw strings don't escape anything, which makes use of regular expressions easier. import re pattern = r \"spam\" if re . match ( pattern , \"spamspamspam\" ): print ( \"Match\" ) else : print ( \"No match\" ) Match The function re.search finds a match of a pattern anywhere in the string. The function re.findall returns a list of all substrings that match a pattern. The function re.finditer does the same thing as re.findall, except it returns an iterator, rather than a list. import re pattern = r \"spam\" if re . match ( pattern , \"eggspamsausagespam\" ): print ( \"Match\" ) else : print ( \"No match\" ) if re . search ( pattern , \"eggspamsausagespam\" ): print ( \"Match\" ) else : print ( \"No match\" ) print ( re . findall ( pattern , \"eggspamsausagespam\" )) No match Match ['spam', 'spam'] The regex search returns an object with several methods that give details about it. These methods include group which returns the string matched, start and end which return the start and ending positions of the first match, and span which returns the start and end positions of the first match as a tuple. import re pattern = r \"pam\" match = re . search ( pattern , \"eggspamsausage\" ) if match : print ( match . group ()) print ( match . start ()) print ( match . end ()) print ( match . span ()) pam 4 7 (4, 7) One of the most important re methods that use regular expressions is sub. re.sub(pattern, repl, string, count=0) This method replaces all occurrences of the pattern in string with repl, substituting all occurrences, unless count provided. This method returns the modified string. import re hello = \"My name is Carlos, Hi Carlos\" pattern = r \"Carlos\" newstr = re . sub ( pattern , \"Diana\" , hello , count = 1 ) print ( newstr ) My name is Diana, Hi Carlos Metacharacters Metacharacters are what make regular expressions more powerful than normal string methods. They allow you to create regular expressions to represent concepts like \"one or more repetitions of a vowel\". The existence of metacharacters poses a problem if you want to create a regular expression (or regex) that matches a literal metacharacter, such as \"$\". You can do this by escaping the metacharacters by putting a backslash in front of them. However, this can cause problems, since backslashes also have an escaping function in normal Python strings. This can mean putting three or four backslashes in a row to do all the escaping. To avoid this, you can use a raw string, which is a normal string with an \"r\" in front of it. . (dot) : This matches any character, other than a new line. ^ and $ : These match the start and end of a string, respectively. import re pattern = r \"gr.y\" pattern2 = r \"....\" #Any four character string with no newlines patt3 = r \"^gr.y$\" if re . match ( pattern , \"grey\" ): print ( \"Match 1\" ) if re . match ( pattern , \"gray\" ): print ( \"Match 2\" ) if re . match ( pattern , \"blue\" ): print ( \"Match 3\" ) if re . match ( patt3 , \"gray\" ): print ( \"Match 4\" ) if re . match ( patt3 , \"Stingray\" ): print ( \"Match 5\" ) Match 1 Match 2 Match 4 Character Classes Character classes provide a way to match only one of a specific set of characters. A character class is created by putting the characters it matches inside square brackets. The pattern [aeiou] in the search function matches all strings that contain any one of the characters defined. import re patt = r \"[aeiou]\" if re . search ( patt , \"grey\" ): print ( \"Match 1\" ) if re . search ( patt , \"qwertyuiop\" ): print ( \"Match 2\" ) if re . search ( patt , \"rhythm myths\" ): print ( \"Match 3\" ) Match 1 Match 2 Character classes can also match ranges of characters. Some examples: The class [a-z] matches any lowercase alphabetic character. The class [G-P] matches any uppercase character from G to P. The class [0-9] matches any digit. Multiple ranges can be included in one class. For example, [A-Za-z] matches a letter of any case. import re #strings that contain two alphabetic uppercase letters followed by a digit. patt = r \"[A-Z][A-Z][0-9]\" if re . search ( patt , \"LS8\" ): print ( \"Match 1\" ) if re . search ( patt , \"E3\" ): print ( \"Match 2\" ) if re . search ( patt , \"1ab\" ): print ( \"Match 3\" ) Match 1 Place a ^ at the start of a character class to invert it. This causes it to match any character other than the ones included. Other metacharacters such as $ and ., have no meaning within character classes. The metacharacter ^ has no meaning unless it is the first character in a class. The pattern [^A-Z] excludes uppercase strings. Note, that the ^ should be inside the brackets to invert the character class. import re patt = r \"[^A-Z]\" if re . search ( patt , \"this is all quiet\" ): print ( \"Match 1\" ) if re . search ( patt , \"E31hsDD\" ): print ( \"Match 2\" ) if re . search ( patt , \"SHOUT\" ): print ( \"Match 3\" ) Match 1 Match 2 Metacharacters for Repetition Some more metacharacters are * + ? { and }. These specify numbers of repetitions. * The metacharacter * means \"zero or more repetitions of the previous thing\". It tries to match as many repetitions as possible. The \"previous thing\" can be a single character, a class, or a group of characters in parentheses. * matches 0 or more occurrences of the preceding expression. * The metacharacter + is very similar to *, except it means \"one or more repetitions\", as opposed to \"zero or more repetitions\". + matches 1 or more occurrence of the preceding expression. * The metacharacter ? means \"zero or one repetitions\". * Curly braces can be used to represent the number of repetitions between two numbers. The regex {x,y} means \"between x and y repetitions of something\". Hence {0,1} is the same thing as ?. If the first number is missing, it is taken to be zero. If the second number is missing, it is taken to be infinity. import re # strings that start with \"egg\" and follow with zero or more \"spam\"s patt = r \"egg(spam)*\" patt1 = r \"[a^]*\" #Zero or more repetitions of \"a\" or \"^\" patt2 = r \"g+\" patt3 = r \"(42)+$\" #One or more 42s patt4 = r \"ice(-)?cream\" patt5 = r \"colo(u)?r\" #Matches both color and colour patt6 = r \"9{1,3}$\" # matches string that have 1 to 3 nines. if re . match ( patt , \"egg\" ): print ( \"Match 1\" ) if re . match ( patt , \"eggspamspamegg\" ): print ( \"Match 2\" ) if re . match ( patt , \"spam\" ): print ( \"Match 3\" ) if re . match ( patt2 , \"g\" ): print ( \"Match 4\" ) if re . match ( patt2 , \"gggggg\" ): print ( \"Match 5\" ) if re . match ( patt2 , \"abcde\" ): print ( \"Match 6\" ) if re . match ( patt4 , \"ice-cream\" ): print ( \"Match 7\" ) if re . match ( patt4 , \"icecream\" ): print ( \"Match 8\" ) if re . match ( patt4 , \"ice--cream\" ): print ( \"Match 9\" ) if re . match ( patt6 , \"9\" ): print ( \"Match 10\" ) if re . match ( patt6 , \"999\" ): print ( \"Match 11\" ) if re . match ( patt6 , \"9999\" ): print ( \"Match 12\" ) Match 1 Match 2 Match 4 Match 5 Match 7 Match 8 Match 10 Match 11 Groups A group can be created by surrounding part of a regular expression with parentheses. This means that a group can be given as an argument to metacharacters such as * and ?. (spam) represents a group in the example pattern shown below. The content of groups in a match can be accessed using the group function. A call of group(0) or group() returns the whole match. A call of group(n), where n is greater than 0, returns the nth group from the left. The method groups() returns all groups up from 1. Groups can be nested import re # strings that start with \"egg\" and follow with zero or more \"spam\"s patt = r \"egg(spam)*\" patt1 = r \"([^aeiou][aeiou][^aeiou])+\" #one or more repetitions of a nonvowel, a vowel and nonvowel patt2 = r \"a(bc)(de)(f(g)h)i\" if re . match ( patt , \"egg\" ): print ( \"Match 1\" ) if re . match ( patt , \"eggspamspamegg\" ): print ( \"Match 2\" ) if re . match ( patt , \"spam\" ): print ( \"Match 3\" ) match = re . match ( patt2 , \"abcdefghijklmnop\" ) if match : print ( match . group ()) print ( match . group ( 0 )) print ( match . group ( 1 )) print ( match . group ( 2 )) print ( match . groups ()) Match 1 Match 2 abcdefghi abcdefghi bc de ('bc', 'de', 'fgh', 'g') There are several kinds of special groups. Two useful ones are named groups and non-capturing groups. * Named groups have the format (?P ...), where name is the name of the group, and ... is the content. They behave exactly the same as normal groups, except they can be accessed by group(name) in addition to its number. * Non-capturing groups have the format (?:...). They are not accessible by the group method, so they can be added to an existing regular expression without breaking the numbering. import re patt = r \"(?P<first>abc)(?:def)(ghi)\" match = re . match ( patt , \"abcdefghijklmnop\" ) if match : print ( match . group ( \"first\" )) print ( match . groups ()) abc ('abc', 'ghi') Another important metacharacter is |. This means \"or\", so red|blue matches either \"red\" or \"blue\". import re patt = r \"gr(a|e)y\" match = re . match ( patt , \"gray\" ) if match : print ( \"Match 1\" ) match = re . match ( patt , \"grey\" ) if match : print ( \"Match 2\" ) match = re . match ( patt , \"griy\" ) if match : print ( \"Match 3\" ) Match 1 Match 2 Special Sequences There are various special sequences you can use in regular expressions. They are written as a backslash followed by another character. One useful special sequence is a backslash and a number between 1 and 99, e.g., \\1 or \\17. This matches the expression of the group of that number. Note, that \"(.+) \\1\" is not the same as \"(.+) (.+)\", because \\1 refers to the first group's subexpression, which is the matched expression itself, and not the regex pattern. More useful special sequences are \\d, \\s, and \\w. These match digits, whitespace, and word characters respectively. In ASCII mode they are equivalent to [0-9], [ \\t\\n\\r\\f\\v], and [a-zA-Z0-9_]. In Unicode mode they match certain other characters, as well. For instance, \\w matches letters with accents. Versions of these special sequences with upper case letters - \\D, \\S, and \\W - mean the opposite to the lower-case versions. For instance, \\D matches anything that isn't a digit. Additional special sequences are \\A, \\Z, and \\b. The sequences \\A and \\Z match the beginning and end of a string, respectively. The sequence \\b matches the empty string between \\w and \\W characters, or \\w characters and the beginning or end of the string. Informally, it represents the boundary between words. The sequence \\B matches the empty string anywhere else. import re #[\\w\\.-]+ matches one or more word character, dot or dash. emailpattern = r \"([\\w\\.-]+)@([\\w\\.-]+)(\\.[\\w\\.]+)\" patt = r \"(.+) \\1\" match = re . match ( patt , \"word word\" ) if match : print ( \"Match 1\" ) match = re . match ( patt , \"?! ?!\" ) if match : print ( \"Match 2\" ) match = re . match ( patt , \"abc def\" ) if match : print ( \"Match 3\" ) patt = r \"(\\D+\\d)\" # matches one or more non-digits followed by a digit. match = re . match ( patt , \"Hi 999!\" ) if match : print ( \"Match 4\" ) match = re . match ( patt , \"1, 23, 456!!\" ) if match : print ( \"Match 5\" ) match = re . match ( patt , \" ! $?\" ) if match : print ( \"Match 6\" ) patt = r \"\\b(cat)\\b\" # matches the word \"cat\" surrounded by word boundaries. match = re . search ( patt , \"The cat sat!\" ) #needs search not match if match : print ( \"Match 7\" ) match = re . search ( patt , \"we s>cat<tered?\" ) if match : print ( \"Match 8\" ) match = re . search ( patt , \"We scattered.\" ) if match : print ( \"Match 9\" ) patt = r \"\\B(cat)\\B\" match = re . search ( patt , \"We scattered.\" ) if match : print ( \"Match 10\" ) #[\\w\\.-]+ matches one or more word character, dot or dash. the dot is preceded by \\ to treat it as a character emailpattern = r \"([\\w\\.-]+)@([\\w\\.-]+)(\\.[\\w\\.]+)\" match = re . search ( emailpattern , \"Please contact car.ar@laposte.net for any assitance\" ) #needs search not match if match : print ( match . group ()) Match 1 Match 2 Match 4 Match 7 Match 8 Match 10 car.ar@laposte.net Regexes can be more readable with the verbose flag #The usual compact way: email_rx = r \"^[^@ ]+@[^@ ]+\\.[^@ ]+$\" import re #The verbose way: email_rx = r \"\"\"(?x) #verbose flag ^ #beginning of string [^@ ]+ #stuff without @ or space (name) @ #an @ sign [^@ ]+ #more stuff (domain) \\. #a dot [^@ ]+ #final stuff(com, org,..) $ #end of string \"\"\" print ( re . match ( email_rx , \"carlos@pollo.com\" )) <re.Match object; span=(0, 16), match='carlos@pollo.com'>","title":"Regular Expressions"},{"location":"Func/pyRegex/#metacharacters","text":"Metacharacters are what make regular expressions more powerful than normal string methods. They allow you to create regular expressions to represent concepts like \"one or more repetitions of a vowel\". The existence of metacharacters poses a problem if you want to create a regular expression (or regex) that matches a literal metacharacter, such as \"$\". You can do this by escaping the metacharacters by putting a backslash in front of them. However, this can cause problems, since backslashes also have an escaping function in normal Python strings. This can mean putting three or four backslashes in a row to do all the escaping. To avoid this, you can use a raw string, which is a normal string with an \"r\" in front of it. . (dot) : This matches any character, other than a new line. ^ and $ : These match the start and end of a string, respectively. import re pattern = r \"gr.y\" pattern2 = r \"....\" #Any four character string with no newlines patt3 = r \"^gr.y$\" if re . match ( pattern , \"grey\" ): print ( \"Match 1\" ) if re . match ( pattern , \"gray\" ): print ( \"Match 2\" ) if re . match ( pattern , \"blue\" ): print ( \"Match 3\" ) if re . match ( patt3 , \"gray\" ): print ( \"Match 4\" ) if re . match ( patt3 , \"Stingray\" ): print ( \"Match 5\" ) Match 1 Match 2 Match 4","title":"Metacharacters"},{"location":"Func/pyRegex/#character-classes","text":"Character classes provide a way to match only one of a specific set of characters. A character class is created by putting the characters it matches inside square brackets. The pattern [aeiou] in the search function matches all strings that contain any one of the characters defined. import re patt = r \"[aeiou]\" if re . search ( patt , \"grey\" ): print ( \"Match 1\" ) if re . search ( patt , \"qwertyuiop\" ): print ( \"Match 2\" ) if re . search ( patt , \"rhythm myths\" ): print ( \"Match 3\" ) Match 1 Match 2 Character classes can also match ranges of characters. Some examples: The class [a-z] matches any lowercase alphabetic character. The class [G-P] matches any uppercase character from G to P. The class [0-9] matches any digit. Multiple ranges can be included in one class. For example, [A-Za-z] matches a letter of any case. import re #strings that contain two alphabetic uppercase letters followed by a digit. patt = r \"[A-Z][A-Z][0-9]\" if re . search ( patt , \"LS8\" ): print ( \"Match 1\" ) if re . search ( patt , \"E3\" ): print ( \"Match 2\" ) if re . search ( patt , \"1ab\" ): print ( \"Match 3\" ) Match 1 Place a ^ at the start of a character class to invert it. This causes it to match any character other than the ones included. Other metacharacters such as $ and ., have no meaning within character classes. The metacharacter ^ has no meaning unless it is the first character in a class. The pattern [^A-Z] excludes uppercase strings. Note, that the ^ should be inside the brackets to invert the character class. import re patt = r \"[^A-Z]\" if re . search ( patt , \"this is all quiet\" ): print ( \"Match 1\" ) if re . search ( patt , \"E31hsDD\" ): print ( \"Match 2\" ) if re . search ( patt , \"SHOUT\" ): print ( \"Match 3\" ) Match 1 Match 2","title":"Character Classes"},{"location":"Func/pyRegex/#metacharacters-for-repetition","text":"Some more metacharacters are * + ? { and }. These specify numbers of repetitions. * The metacharacter * means \"zero or more repetitions of the previous thing\". It tries to match as many repetitions as possible. The \"previous thing\" can be a single character, a class, or a group of characters in parentheses. * matches 0 or more occurrences of the preceding expression. * The metacharacter + is very similar to *, except it means \"one or more repetitions\", as opposed to \"zero or more repetitions\". + matches 1 or more occurrence of the preceding expression. * The metacharacter ? means \"zero or one repetitions\". * Curly braces can be used to represent the number of repetitions between two numbers. The regex {x,y} means \"between x and y repetitions of something\". Hence {0,1} is the same thing as ?. If the first number is missing, it is taken to be zero. If the second number is missing, it is taken to be infinity. import re # strings that start with \"egg\" and follow with zero or more \"spam\"s patt = r \"egg(spam)*\" patt1 = r \"[a^]*\" #Zero or more repetitions of \"a\" or \"^\" patt2 = r \"g+\" patt3 = r \"(42)+$\" #One or more 42s patt4 = r \"ice(-)?cream\" patt5 = r \"colo(u)?r\" #Matches both color and colour patt6 = r \"9{1,3}$\" # matches string that have 1 to 3 nines. if re . match ( patt , \"egg\" ): print ( \"Match 1\" ) if re . match ( patt , \"eggspamspamegg\" ): print ( \"Match 2\" ) if re . match ( patt , \"spam\" ): print ( \"Match 3\" ) if re . match ( patt2 , \"g\" ): print ( \"Match 4\" ) if re . match ( patt2 , \"gggggg\" ): print ( \"Match 5\" ) if re . match ( patt2 , \"abcde\" ): print ( \"Match 6\" ) if re . match ( patt4 , \"ice-cream\" ): print ( \"Match 7\" ) if re . match ( patt4 , \"icecream\" ): print ( \"Match 8\" ) if re . match ( patt4 , \"ice--cream\" ): print ( \"Match 9\" ) if re . match ( patt6 , \"9\" ): print ( \"Match 10\" ) if re . match ( patt6 , \"999\" ): print ( \"Match 11\" ) if re . match ( patt6 , \"9999\" ): print ( \"Match 12\" ) Match 1 Match 2 Match 4 Match 5 Match 7 Match 8 Match 10 Match 11","title":"Metacharacters for Repetition"},{"location":"Func/pyRegex/#groups","text":"A group can be created by surrounding part of a regular expression with parentheses. This means that a group can be given as an argument to metacharacters such as * and ?. (spam) represents a group in the example pattern shown below. The content of groups in a match can be accessed using the group function. A call of group(0) or group() returns the whole match. A call of group(n), where n is greater than 0, returns the nth group from the left. The method groups() returns all groups up from 1. Groups can be nested import re # strings that start with \"egg\" and follow with zero or more \"spam\"s patt = r \"egg(spam)*\" patt1 = r \"([^aeiou][aeiou][^aeiou])+\" #one or more repetitions of a nonvowel, a vowel and nonvowel patt2 = r \"a(bc)(de)(f(g)h)i\" if re . match ( patt , \"egg\" ): print ( \"Match 1\" ) if re . match ( patt , \"eggspamspamegg\" ): print ( \"Match 2\" ) if re . match ( patt , \"spam\" ): print ( \"Match 3\" ) match = re . match ( patt2 , \"abcdefghijklmnop\" ) if match : print ( match . group ()) print ( match . group ( 0 )) print ( match . group ( 1 )) print ( match . group ( 2 )) print ( match . groups ()) Match 1 Match 2 abcdefghi abcdefghi bc de ('bc', 'de', 'fgh', 'g') There are several kinds of special groups. Two useful ones are named groups and non-capturing groups. * Named groups have the format (?P ...), where name is the name of the group, and ... is the content. They behave exactly the same as normal groups, except they can be accessed by group(name) in addition to its number. * Non-capturing groups have the format (?:...). They are not accessible by the group method, so they can be added to an existing regular expression without breaking the numbering. import re patt = r \"(?P<first>abc)(?:def)(ghi)\" match = re . match ( patt , \"abcdefghijklmnop\" ) if match : print ( match . group ( \"first\" )) print ( match . groups ()) abc ('abc', 'ghi') Another important metacharacter is |. This means \"or\", so red|blue matches either \"red\" or \"blue\". import re patt = r \"gr(a|e)y\" match = re . match ( patt , \"gray\" ) if match : print ( \"Match 1\" ) match = re . match ( patt , \"grey\" ) if match : print ( \"Match 2\" ) match = re . match ( patt , \"griy\" ) if match : print ( \"Match 3\" ) Match 1 Match 2","title":"Groups"},{"location":"Func/pyRegex/#special-sequences","text":"There are various special sequences you can use in regular expressions. They are written as a backslash followed by another character. One useful special sequence is a backslash and a number between 1 and 99, e.g., \\1 or \\17. This matches the expression of the group of that number. Note, that \"(.+) \\1\" is not the same as \"(.+) (.+)\", because \\1 refers to the first group's subexpression, which is the matched expression itself, and not the regex pattern. More useful special sequences are \\d, \\s, and \\w. These match digits, whitespace, and word characters respectively. In ASCII mode they are equivalent to [0-9], [ \\t\\n\\r\\f\\v], and [a-zA-Z0-9_]. In Unicode mode they match certain other characters, as well. For instance, \\w matches letters with accents. Versions of these special sequences with upper case letters - \\D, \\S, and \\W - mean the opposite to the lower-case versions. For instance, \\D matches anything that isn't a digit. Additional special sequences are \\A, \\Z, and \\b. The sequences \\A and \\Z match the beginning and end of a string, respectively. The sequence \\b matches the empty string between \\w and \\W characters, or \\w characters and the beginning or end of the string. Informally, it represents the boundary between words. The sequence \\B matches the empty string anywhere else. import re #[\\w\\.-]+ matches one or more word character, dot or dash. emailpattern = r \"([\\w\\.-]+)@([\\w\\.-]+)(\\.[\\w\\.]+)\" patt = r \"(.+) \\1\" match = re . match ( patt , \"word word\" ) if match : print ( \"Match 1\" ) match = re . match ( patt , \"?! ?!\" ) if match : print ( \"Match 2\" ) match = re . match ( patt , \"abc def\" ) if match : print ( \"Match 3\" ) patt = r \"(\\D+\\d)\" # matches one or more non-digits followed by a digit. match = re . match ( patt , \"Hi 999!\" ) if match : print ( \"Match 4\" ) match = re . match ( patt , \"1, 23, 456!!\" ) if match : print ( \"Match 5\" ) match = re . match ( patt , \" ! $?\" ) if match : print ( \"Match 6\" ) patt = r \"\\b(cat)\\b\" # matches the word \"cat\" surrounded by word boundaries. match = re . search ( patt , \"The cat sat!\" ) #needs search not match if match : print ( \"Match 7\" ) match = re . search ( patt , \"we s>cat<tered?\" ) if match : print ( \"Match 8\" ) match = re . search ( patt , \"We scattered.\" ) if match : print ( \"Match 9\" ) patt = r \"\\B(cat)\\B\" match = re . search ( patt , \"We scattered.\" ) if match : print ( \"Match 10\" ) #[\\w\\.-]+ matches one or more word character, dot or dash. the dot is preceded by \\ to treat it as a character emailpattern = r \"([\\w\\.-]+)@([\\w\\.-]+)(\\.[\\w\\.]+)\" match = re . search ( emailpattern , \"Please contact car.ar@laposte.net for any assitance\" ) #needs search not match if match : print ( match . group ()) Match 1 Match 2 Match 4 Match 7 Match 8 Match 10 car.ar@laposte.net Regexes can be more readable with the verbose flag #The usual compact way: email_rx = r \"^[^@ ]+@[^@ ]+\\.[^@ ]+$\" import re #The verbose way: email_rx = r \"\"\"(?x) #verbose flag ^ #beginning of string [^@ ]+ #stuff without @ or space (name) @ #an @ sign [^@ ]+ #more stuff (domain) \\. #a dot [^@ ]+ #final stuff(com, org,..) $ #end of string \"\"\" print ( re . match ( email_rx , \"carlos@pollo.com\" )) <re.Match object; span=(0, 16), match='carlos@pollo.com'>","title":"Special Sequences"},{"location":"ML/pyClust/","text":"Clustering is a type of unsupervised learning that allows us to find groups of similar objects, objects that are more related to each other than to the objects in other groups. This is often used when we don\u2019t have access to the ground truth, in other words, the labels are missing. Examples of business use cases include the grouping of documents, music, and movies based on their contents, or finding customer segments based on purchase behavior as a basis for recommendation engines. The goal of clustering is to separate the data into groups, or clusters, with more similar traits to each other than to the data in the other clusters. In general, there are four types: Centroid based models - each cluster is represented by a single mean vector (e.g., k-means ), Connectivity based models - built based on distance connectivity (e.g., hierarchical clustering) Distribution based models - built using statistical distributions (e.g., Gaussian mixtures) Density based models - clusters are defined as dense areas (e.g., DBSCAN) K-means The algorithm has gained great popularity because it is easy to implement and scales well to large datasets. However, it is difficult to predict the number of clusters, it can get stuck in local optimums, and it can perform poorly when the clusters are of varying sizes and density. One major shortcoming of k-means is that the random initial guess for the centroids can result in bad clustering, and k-means++ algorithm addresses this obstacle by specifying a procedure to initialize the centroids before proceeding with the standard k-means algorithm. In scikit-learn, the initialization mechanism is set to k-means++, by default. Pre-processing: Standardization from sklearn.cluster import KMeans from sklearn.datasets import load_wine wine = load_wine () from sklearn.preprocessing import StandardScaler # instantiate the scaler scale = StandardScaler () # compute the mean and std to be used later for scaling scale . fit ( X ) # StandardScaler(copy=True, with_mean=True, with_std=True) X_scaled = scale . transform ( X )","title":"Clustering"},{"location":"ML/pyClust/#k-means","text":"The algorithm has gained great popularity because it is easy to implement and scales well to large datasets. However, it is difficult to predict the number of clusters, it can get stuck in local optimums, and it can perform poorly when the clusters are of varying sizes and density. One major shortcoming of k-means is that the random initial guess for the centroids can result in bad clustering, and k-means++ algorithm addresses this obstacle by specifying a procedure to initialize the centroids before proceeding with the standard k-means algorithm. In scikit-learn, the initialization mechanism is set to k-means++, by default. Pre-processing: Standardization from sklearn.cluster import KMeans from sklearn.datasets import load_wine wine = load_wine () from sklearn.preprocessing import StandardScaler # instantiate the scaler scale = StandardScaler () # compute the mean and std to be used later for scaling scale . fit ( X ) # StandardScaler(copy=True, with_mean=True, with_std=True) X_scaled = scale . transform ( X )","title":"K-means"},{"location":"ML/pyCrossVal/","text":"Machine learning is an iterative process. We face choices about what predictive variables to use, what types of models to use, what arguments to supply to those models, etc. These choices are usually taken in a data-driven way by measuring model quality with a validation (or holdout) set. But there are some drawbacks to this approach. To see this, imagine you have a dataset with 5000 rows. We typically keep about 20% of the data as a validation dataset, or 1000 rows. But this leaves some random chance in determining model scores. That is, a model might do well on one set of 1000 rows, even if it would be inaccurate on a different 1000 rows. At an extreme, we can imagine having only 1 row of data in the validation set. If you compare alternative models, which one makes the best predictions on a single data point will be mostly a matter of luck! In general, the larger the validation set, the less randomness (aka \"noise\") there is in our measure of model quality, and the more reliable it will be. Unfortunately, we can only get a large validation set by removing rows from our training data, and smaller training datasets mean worse models! In cross-validation, we run our modeling process on different subsets of the data to get multiple measures of model quality. For example, we could begin by dividing the data into 5 pieces, each 20% of the full dataset. In this case, we say that we have broken the data into 5 \"folds\". Then, we run one experiment for each fold: In Experiment 1, we use the first fold as a validation (or holdout) set and everything else as training data. This gives us a measure of model quality based on a 20% holdout set. In Experiment 2, we hold out data from the second fold (and use everything except the second fold for training the model). The holdout set is then used to get a second estimate of model quality. We repeat this process, using every fold once as the holdout set. Putting this together, 100% of the data is used as holdout at some point, and we end up with a measure of model quality that is based on all of the rows in the dataset (even if we don't use all rows simultaneously). Cross-validation gives a more accurate measure of model quality, which is especially important if you are making a lot of modeling decisions. However, it can take longer to run, because it estimates multiple models (one for each fold). So, given these tradeoffs, when should you use each approach? For small datasets, where extra computational burden isn't a big deal, you should run cross-validation. For larger datasets, a single validation set is sufficient. Your code will run faster, and you may have enough data that there's little need to re-use some of it for holdout. There's no simple threshold for what constitutes a large vs. small dataset. But if your model takes a couple minutes or less to run, it's probably worth switching to cross-validation. Alternatively, you can run cross-validation and see if the scores for each experiment seem close. If each experiment yields the same results, a single validation set is probably sufficient. import pandas as pd # Read the data data = pd . read_csv ( 'https://raw.githubusercontent.com/esabunor/MLWorkspace/master/melb_data.csv' ) # Select subset of predictors cols_to_use = [ 'Rooms' , 'Distance' , 'Landsize' , 'BuildingArea' , 'YearBuilt' ] X = data [ cols_to_use ] # Select target y = data . Price Then, we define a pipeline that uses an imputer to fill in missing values and a random forest model to make predictions. While it's possible to do cross-validation without pipelines, it is quite difficult! Using a pipeline will make the code remarkably straightforward. from sklearn.ensemble import RandomForestRegressor from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer my_pipeline = Pipeline ( steps = [( 'preprocessor' , SimpleImputer ()), ( 'model' , RandomForestRegressor ( n_estimators = 50 , random_state = 0 )) ]) We obtain the cross-validation scores with the cross_val_score() function from scikit-learn. We set the number of folds with the cv parameter. from sklearn.model_selection import cross_val_score # Multiply by -1 since sklearn calculates *negative* MAE scores = - 1 * cross_val_score ( my_pipeline , X , y , cv = 5 , scoring = 'neg_mean_absolute_error' ) print ( \"MAE scores: \\n \" , scores ) MAE scores: [330153.84614038 315703.21408613 299410.61884391 243541.74763508 251301.76683685] The scoring parameter chooses a measure of model quality to report: in this case, we chose negative mean absolute error (MAE). It is a little surprising that we specify negative MAE. Scikit-learn has a convention where all metrics are defined so a high number is better. Using negatives here allows them to be consistent with that convention, though negative MAE is almost unheard of elsewhere. We typically want a single measure of model quality to compare alternative models. So we take the average across experiments. print ( \"Average MAE score (across experiments):\" ) print ( scores . mean ()) Average MAE score (across experiments): 288022.23870846874 Using cross-validation yields a much better measure of model quality, with the added benefit of cleaning up our code: note that we no longer need to keep track of separate training and validation sets. So, especially for small datasets, it's a good improvement!","title":"Cross-Validation"},{"location":"ML/pyDataLeak/","text":"Data leakage (or leakage) happens when your training data contains information about the target, but similar data will not be available when the model is used for prediction. This leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production. In other words, leakage causes a model to look accurate until you start making decisions with the model, and then the model becomes very inaccurate. There are two main types of leakage: target leakage and train-test contamination. Target Leakage Target leakage occurs when your predictors include data that will not be available at the time you make predictions. It is important to think about target leakage in terms of the timing or chronological order that data becomes available, not merely whether a feature helps make good predictions. An example will be helpful. Imagine you want to predict who will get sick with pneumonia. The top few rows of your raw data look like this: got_pneumonia age weight male took_antibiotic_medicine ... False 65 100 False False ... False 72 130 True False ... True 58 100 False True ... People take antibiotic medicines after getting pneumonia in order to recover. The raw data shows a strong relationship between those columns, but took_antibiotic_medicine is frequently changed after the value for got_pneumonia is determined. This is target leakage. The model would see that anyone who has a value of False for took_antibiotic_medicine didn't have pneumonia. Since validation data comes from the same source as training data, the pattern will repeat itself in validation, and the model will have great validation (or cross-validation) scores. But the model will be very inaccurate when subsequently deployed in the real world, because even patients who will get pneumonia won't have received antibiotics yet when we need to make predictions about their future health. To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded. Train-Test Contamination A different type of leak occurs when you aren't careful to distinguish training data from validation data. Recall that validation is meant to be a measure of how the model does on data that it hasn't considered before. You can corrupt this process in subtle ways if the validation data affects the preprocessing behavior. This is sometimes called train-test contamination. For example, imagine you run preprocessing (like fitting an imputer for missing values) before calling train_test_split(). The end result? Your model may get good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions. After all, you incorporated data from the validation or test data into how you make predictions, so the may do well on that particular data even if it can't generalize to new data. This problem becomes even more subtle (and more dangerous) when you do more complex feature engineering. If your validation is based on a simple train-test split, exclude the validation data from any type of fitting, including the fitting of preprocessing steps. This is easier if you use scikit-learn pipelines. When using cross-validation, it's even more critical that you do your preprocessing inside the pipeline! import pandas as pd # Read the data data = pd . read_csv ( 'https://raw.githubusercontent.com/YoshiKitaguchi/Credit-card-verification-project/master/AER_credit_card_data.csv' , true_values = [ 'yes' ], false_values = [ 'no' ]) # Select target y = data . card # Select predictors X = data . drop ([ 'card' ], axis = 1 ) print ( \"Number of rows in the dataset:\" , X . shape [ 0 ]) X . head () Number of rows in the dataset: 1319 reports age income share expenditure owner selfemp dependents months majorcards active 0 0 37.66667 4.5200 0.033270 124.983300 True False 3 54 1 1 0 33.25000 2.4200 0.005217 9.854167 False False 3 34 1 2 0 33.66667 4.5000 0.004156 15.000000 True False 4 58 1 3 0 30.50000 2.5400 0.065214 137.869200 False False 0 25 1 4 0 32.16667 9.7867 0.067051 546.503300 True False 2 64 1 from sklearn.pipeline import make_pipeline from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score # Since there is no preprocessing, we don't need a pipeline (used anyway as best practice!) my_pipeline = make_pipeline ( RandomForestClassifier ( n_estimators = 100 )) cv_scores = cross_val_score ( my_pipeline , X , y , cv = 5 , scoring = 'accuracy' ) print ( \"Cross-validation accuracy: %f \" % cv_scores . mean ()) Cross-validation accuracy: 0.981049 With experience, you'll find that it's very rare to find models that are accurate 98% of the time. It happens, but it's uncommon enough that we should inspect the data more closely for target leakage. Here is a summary of the data, which you can also find under the data tab: card: 1 if credit card application accepted, 0 if not reports: Number of major derogatory reports age: Age n years plus twelfths of a year income: Yearly income (divided by 10,000) share: Ratio of monthly credit card expenditure to yearly income expenditure: Average monthly credit card expenditure owner: 1 if owns home, 0 if rents selfempl: 1 if self-employed, 0 if not dependents: 1 + number of dependents months: Months living at current address majorcards: Number of major credit cards held active: Number of active credit accounts A few variables look suspicious. For example, does expenditure mean expenditure on this card or on cards used before appying? At this point, basic data comparisons can be very helpful: # Drop leaky predictors from dataset potential_leaks = [ 'expenditure' , 'share' , 'active' , 'majorcards' ] X2 = X . drop ( potential_leaks , axis = 1 ) # Evaluate the model with leaky predictors removed cv_scores = cross_val_score ( my_pipeline , X2 , y , cv = 5 , scoring = 'accuracy' ) print ( \"Cross-val accuracy: %f \" % cv_scores . mean ()) Cross-val accuracy: 0.830168 This accuracy is quite a bit lower, which might be disappointing. However, we can expect it to be right about 80% of the time when used on new applications, whereas the leaky model would likely do much worse than that (in spite of its higher apparent score in cross-validation). Data leakage can be multi-million dollar mistake in many data science applications. Careful separation of training and validation data can prevent train-test contamination, and pipelines can help implement this separation. Likewise, a combination of caution, common sense, and data exploration can help identify target leakage. Case 1: The Data Science of Shoelaces Nike has hired you as a data science consultant to help them save money on shoe materials. Your first assignment is to review a model one of their employees built to predict how many shoelaces they'll need each month. The features going into the machine learning model include: The current month (January, February, etc) Advertising expenditures in the previous month Various macroeconomic features (like the unemployment rate) as of the beginning of the current month The amount of leather they ended up using in the current month The results show the model is almost perfectly accurate if you include the feature about how much leather they used. But it is only moderately accurate if you leave that feature out. You realize this is because the amount of leather they use is a perfect indicator of how many shoes they produce, which in turn tells you how many shoelaces they need. This is tricky, and it depends on details of how data is collected (which is common when thinking about leakage). Would you at the beginning of the month decide how much leather will be used that month? If so, this is ok. But if that is determined during the month, you would not have access to it when you make the prediction. If you have a guess at the beginning of the month, and it is subsequently changed during the month, the actual amount used during the month cannot be used as a feature (because it causes leakage). You have a new idea. You could use the amount of leather Nike ordered (rather than the amount they actually used) leading up to a given month as a predictor in your shoelace model. Does this change your answer about whether there is a leakage problem? If you answer \"it depends,\" what does it depend on? This could be fine, but it depends on whether they order shoelaces first or leather first. If they order shoelaces first, you won't know how much leather they've ordered when you predict their shoelace needs. If they order leather first, then you'll have that number available when you place your shoelace order, and you should be ok. Case 2: Crypto Rush You saved Nike so much money that they gave you a bonus. Congratulations. Your friend, who is also a data scientist, says he has built a model that will let you turn your bonus into millions of dollars. Specifically, his model predicts the price of a new cryptocurrency (like Bitcoin, but a newer one) one day ahead of the moment of prediction. His plan is to purchase the cryptocurrency whenever the model says the price of the currency (in dollars) is about to go up. The most important features in his model are: Current price of the currency Amount of the currency sold in the last 24 hours Change in the currency price in the last 24 hours Change in the currency price in the last 1 hour Number of new tweets in the last 24 hours that mention the currency The value of the cryptocurrency in dollars has fluctuated up and down by over \\(\\$\\) 100 in the last year, and yet his model's average error is less than \\(\\$\\) 1. He says this is proof his model is accurate, and you should invest with him, buying the currency whenever the model says it is about to go up. Is he right? If there is a problem with his model, what is it? There is no source of leakage here. These features should be available at the moment you want to make a predition, and they're unlikely to be changed in the training data after the prediction target is determined. But, the way he describes accuracy could be misleading if you aren't careful. If the price moves gradually, today's price will be an accurate predictor of tomorrow's price, but it may not tell you whether it's a good time to invest. For instance, if it is 100 today predicting a price of 100 tomorrow may seem accurate, even if it can't tell you whether the price is going up or down from the current price. A better prediction target would be the change in price over the next day. If you can consistently predict whether the price is about to go up or down (and by how much), you may have a winning investment opportunity. Case 3: Preventing Infections An agency that provides healthcare wants to predict which patients from a rare surgery are at risk of infection, so it can alert the nurses to be especially careful when following up with those patients. You want to build a model. Each row in the modeling dataset will be a single patient who received the surgery, and the prediction target will be whether they got an infection. Some surgeons may do the procedure in a manner that raises or lowers the risk of infection. But how can you best incorporate the surgeon information into the model? You have a clever idea. Take all surgeries by each surgeon and calculate the infection rate among those surgeons. For each patient in the data, find out who the surgeon was and plug in that surgeon's average infection rate as a feature. Does this pose any target leakage issues? Does it pose any train-test contamination issues? This poses a risk of both target leakage and train-test contamination (though you may be able to avoid both if you are careful). You have target leakage if a given patient's outcome contributes to the infection rate for his surgeon, which is then plugged back into the prediction model for whether that patient becomes infected. You can avoid target leakage if you calculate the surgeon's infection rate by using only the surgeries before the patient we are predicting for. Calculating this for each surgery in your training data may be a little tricky. You also have a train-test contamination problem if you calculate this using all surgeries a surgeon performed, including those from the test-set. The result would be that your model could look very accurate on the test set, even if it wouldn't generalize well to new patients after the model is deployed. This would happen because the surgeon-risk feature accounts for data in the test set. Test sets exist to estimate how the model will do when seeing new data. So this contamination defeats the purpose of the test set. Case 4: Housing Prices You will build a model to predict housing prices. The model will be deployed on an ongoing basis, to predict the price of a new house when a description is added to a website. Here are four features that could be used as predictors. Size of the house (in square meters) Average sales price of homes in the same neighborhood Latitude and longitude of the house Whether the house has a basement You have historic data to train and validate the model. Which of the features is most likely to be a source of leakage? 2 is the source of target leakage. Here is an analysis for each feature: The size of a house is unlikely to be changed after it is sold (though technically it's possible). But typically this will be available when we need to make a prediction, and the data won't be modified after the home is sold. So it is pretty safe. We don't know the rules for when this is updated. If the field is updated in the raw data after a home was sold, and the home's sale is used to calculate the average, this constitutes a case of target leakage. At an extreme, if only one home is sold in the neighborhood, and it is the home we are trying to predict, then the average will be exactly equal to the value we are trying to predict. In general, for neighborhoods with few sales, the model will perform very well on the training data. But when you apply the model, the home you are predicting won't have been sold yet, so this feature won't work the same as it did in the training data. These don't change, and will be available at the time we want to make a prediction. So there's no risk of target leakage here. This also doesn't change, and it is available at the time we want to make a prediction. So there's no risk of target leakage here.","title":"Data Leakage"},{"location":"ML/pyDataLeak/#target-leakage","text":"Target leakage occurs when your predictors include data that will not be available at the time you make predictions. It is important to think about target leakage in terms of the timing or chronological order that data becomes available, not merely whether a feature helps make good predictions. An example will be helpful. Imagine you want to predict who will get sick with pneumonia. The top few rows of your raw data look like this: got_pneumonia age weight male took_antibiotic_medicine ... False 65 100 False False ... False 72 130 True False ... True 58 100 False True ... People take antibiotic medicines after getting pneumonia in order to recover. The raw data shows a strong relationship between those columns, but took_antibiotic_medicine is frequently changed after the value for got_pneumonia is determined. This is target leakage. The model would see that anyone who has a value of False for took_antibiotic_medicine didn't have pneumonia. Since validation data comes from the same source as training data, the pattern will repeat itself in validation, and the model will have great validation (or cross-validation) scores. But the model will be very inaccurate when subsequently deployed in the real world, because even patients who will get pneumonia won't have received antibiotics yet when we need to make predictions about their future health. To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded.","title":"Target Leakage"},{"location":"ML/pyDataLeak/#train-test-contamination","text":"A different type of leak occurs when you aren't careful to distinguish training data from validation data. Recall that validation is meant to be a measure of how the model does on data that it hasn't considered before. You can corrupt this process in subtle ways if the validation data affects the preprocessing behavior. This is sometimes called train-test contamination. For example, imagine you run preprocessing (like fitting an imputer for missing values) before calling train_test_split(). The end result? Your model may get good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions. After all, you incorporated data from the validation or test data into how you make predictions, so the may do well on that particular data even if it can't generalize to new data. This problem becomes even more subtle (and more dangerous) when you do more complex feature engineering. If your validation is based on a simple train-test split, exclude the validation data from any type of fitting, including the fitting of preprocessing steps. This is easier if you use scikit-learn pipelines. When using cross-validation, it's even more critical that you do your preprocessing inside the pipeline! import pandas as pd # Read the data data = pd . read_csv ( 'https://raw.githubusercontent.com/YoshiKitaguchi/Credit-card-verification-project/master/AER_credit_card_data.csv' , true_values = [ 'yes' ], false_values = [ 'no' ]) # Select target y = data . card # Select predictors X = data . drop ([ 'card' ], axis = 1 ) print ( \"Number of rows in the dataset:\" , X . shape [ 0 ]) X . head () Number of rows in the dataset: 1319 reports age income share expenditure owner selfemp dependents months majorcards active 0 0 37.66667 4.5200 0.033270 124.983300 True False 3 54 1 1 0 33.25000 2.4200 0.005217 9.854167 False False 3 34 1 2 0 33.66667 4.5000 0.004156 15.000000 True False 4 58 1 3 0 30.50000 2.5400 0.065214 137.869200 False False 0 25 1 4 0 32.16667 9.7867 0.067051 546.503300 True False 2 64 1 from sklearn.pipeline import make_pipeline from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score # Since there is no preprocessing, we don't need a pipeline (used anyway as best practice!) my_pipeline = make_pipeline ( RandomForestClassifier ( n_estimators = 100 )) cv_scores = cross_val_score ( my_pipeline , X , y , cv = 5 , scoring = 'accuracy' ) print ( \"Cross-validation accuracy: %f \" % cv_scores . mean ()) Cross-validation accuracy: 0.981049 With experience, you'll find that it's very rare to find models that are accurate 98% of the time. It happens, but it's uncommon enough that we should inspect the data more closely for target leakage. Here is a summary of the data, which you can also find under the data tab: card: 1 if credit card application accepted, 0 if not reports: Number of major derogatory reports age: Age n years plus twelfths of a year income: Yearly income (divided by 10,000) share: Ratio of monthly credit card expenditure to yearly income expenditure: Average monthly credit card expenditure owner: 1 if owns home, 0 if rents selfempl: 1 if self-employed, 0 if not dependents: 1 + number of dependents months: Months living at current address majorcards: Number of major credit cards held active: Number of active credit accounts A few variables look suspicious. For example, does expenditure mean expenditure on this card or on cards used before appying? At this point, basic data comparisons can be very helpful: # Drop leaky predictors from dataset potential_leaks = [ 'expenditure' , 'share' , 'active' , 'majorcards' ] X2 = X . drop ( potential_leaks , axis = 1 ) # Evaluate the model with leaky predictors removed cv_scores = cross_val_score ( my_pipeline , X2 , y , cv = 5 , scoring = 'accuracy' ) print ( \"Cross-val accuracy: %f \" % cv_scores . mean ()) Cross-val accuracy: 0.830168 This accuracy is quite a bit lower, which might be disappointing. However, we can expect it to be right about 80% of the time when used on new applications, whereas the leaky model would likely do much worse than that (in spite of its higher apparent score in cross-validation). Data leakage can be multi-million dollar mistake in many data science applications. Careful separation of training and validation data can prevent train-test contamination, and pipelines can help implement this separation. Likewise, a combination of caution, common sense, and data exploration can help identify target leakage.","title":"Train-Test Contamination"},{"location":"ML/pyDataLeak/#case-1-the-data-science-of-shoelaces","text":"Nike has hired you as a data science consultant to help them save money on shoe materials. Your first assignment is to review a model one of their employees built to predict how many shoelaces they'll need each month. The features going into the machine learning model include: The current month (January, February, etc) Advertising expenditures in the previous month Various macroeconomic features (like the unemployment rate) as of the beginning of the current month The amount of leather they ended up using in the current month The results show the model is almost perfectly accurate if you include the feature about how much leather they used. But it is only moderately accurate if you leave that feature out. You realize this is because the amount of leather they use is a perfect indicator of how many shoes they produce, which in turn tells you how many shoelaces they need. This is tricky, and it depends on details of how data is collected (which is common when thinking about leakage). Would you at the beginning of the month decide how much leather will be used that month? If so, this is ok. But if that is determined during the month, you would not have access to it when you make the prediction. If you have a guess at the beginning of the month, and it is subsequently changed during the month, the actual amount used during the month cannot be used as a feature (because it causes leakage). You have a new idea. You could use the amount of leather Nike ordered (rather than the amount they actually used) leading up to a given month as a predictor in your shoelace model. Does this change your answer about whether there is a leakage problem? If you answer \"it depends,\" what does it depend on? This could be fine, but it depends on whether they order shoelaces first or leather first. If they order shoelaces first, you won't know how much leather they've ordered when you predict their shoelace needs. If they order leather first, then you'll have that number available when you place your shoelace order, and you should be ok.","title":"Case 1: The Data Science of Shoelaces"},{"location":"ML/pyDataLeak/#case-2-crypto-rush","text":"You saved Nike so much money that they gave you a bonus. Congratulations. Your friend, who is also a data scientist, says he has built a model that will let you turn your bonus into millions of dollars. Specifically, his model predicts the price of a new cryptocurrency (like Bitcoin, but a newer one) one day ahead of the moment of prediction. His plan is to purchase the cryptocurrency whenever the model says the price of the currency (in dollars) is about to go up. The most important features in his model are: Current price of the currency Amount of the currency sold in the last 24 hours Change in the currency price in the last 24 hours Change in the currency price in the last 1 hour Number of new tweets in the last 24 hours that mention the currency The value of the cryptocurrency in dollars has fluctuated up and down by over \\(\\$\\) 100 in the last year, and yet his model's average error is less than \\(\\$\\) 1. He says this is proof his model is accurate, and you should invest with him, buying the currency whenever the model says it is about to go up. Is he right? If there is a problem with his model, what is it? There is no source of leakage here. These features should be available at the moment you want to make a predition, and they're unlikely to be changed in the training data after the prediction target is determined. But, the way he describes accuracy could be misleading if you aren't careful. If the price moves gradually, today's price will be an accurate predictor of tomorrow's price, but it may not tell you whether it's a good time to invest. For instance, if it is 100 today predicting a price of 100 tomorrow may seem accurate, even if it can't tell you whether the price is going up or down from the current price. A better prediction target would be the change in price over the next day. If you can consistently predict whether the price is about to go up or down (and by how much), you may have a winning investment opportunity.","title":"Case 2: Crypto Rush"},{"location":"ML/pyDataLeak/#case-3-preventing-infections","text":"An agency that provides healthcare wants to predict which patients from a rare surgery are at risk of infection, so it can alert the nurses to be especially careful when following up with those patients. You want to build a model. Each row in the modeling dataset will be a single patient who received the surgery, and the prediction target will be whether they got an infection. Some surgeons may do the procedure in a manner that raises or lowers the risk of infection. But how can you best incorporate the surgeon information into the model? You have a clever idea. Take all surgeries by each surgeon and calculate the infection rate among those surgeons. For each patient in the data, find out who the surgeon was and plug in that surgeon's average infection rate as a feature. Does this pose any target leakage issues? Does it pose any train-test contamination issues? This poses a risk of both target leakage and train-test contamination (though you may be able to avoid both if you are careful). You have target leakage if a given patient's outcome contributes to the infection rate for his surgeon, which is then plugged back into the prediction model for whether that patient becomes infected. You can avoid target leakage if you calculate the surgeon's infection rate by using only the surgeries before the patient we are predicting for. Calculating this for each surgery in your training data may be a little tricky. You also have a train-test contamination problem if you calculate this using all surgeries a surgeon performed, including those from the test-set. The result would be that your model could look very accurate on the test set, even if it wouldn't generalize well to new patients after the model is deployed. This would happen because the surgeon-risk feature accounts for data in the test set. Test sets exist to estimate how the model will do when seeing new data. So this contamination defeats the purpose of the test set.","title":"Case 3: Preventing Infections"},{"location":"ML/pyDataLeak/#case-4-housing-prices","text":"You will build a model to predict housing prices. The model will be deployed on an ongoing basis, to predict the price of a new house when a description is added to a website. Here are four features that could be used as predictors. Size of the house (in square meters) Average sales price of homes in the same neighborhood Latitude and longitude of the house Whether the house has a basement You have historic data to train and validate the model. Which of the features is most likely to be a source of leakage? 2 is the source of target leakage. Here is an analysis for each feature: The size of a house is unlikely to be changed after it is sold (though technically it's possible). But typically this will be available when we need to make a prediction, and the data won't be modified after the home is sold. So it is pretty safe. We don't know the rules for when this is updated. If the field is updated in the raw data after a home was sold, and the home's sale is used to calculate the average, this constitutes a case of target leakage. At an extreme, if only one home is sold in the neighborhood, and it is the home we are trying to predict, then the average will be exactly equal to the value we are trying to predict. In general, for neighborhoods with few sales, the model will perform very well on the training data. But when you apply the model, the home you are predicting won't have been sold yet, so this feature won't work the same as it did in the training data. These don't change, and will be available at the time we want to make a prediction. So there's no risk of target leakage here. This also doesn't change, and it is available at the time we want to make a prediction. So there's no risk of target leakage here.","title":"Case 4: Housing Prices"},{"location":"ML/pyDecTree/","text":"In Logistic Regression, we look at the data graphically and draw a line to separate the data. The model is defined by the coefficients that define the line. These coefficients are called parameters. Since the model is defined by these parameters, Logistic Regression is a parametric machine learning algorithm. Decision Trees, are an example of a nonparametric machine learning algorithm. Decision Trees won\u2019t be defined by a list of parameters. Every machine learning algorithm is either parametric or nonparametric. Decision Trees are often favored if you have a non-technical audience since they can easily interpret the model. The Logistic Regression model performs better, though we may still want to use a Decision Tree for its interpretability. The default impurity criterion in scikit-learn\u2019s Decision Tree algorithm is the Gini Impurity (2p(1-p)). However, they\u2019ve also implemented entropy and you can choose which one you\u2019d like to use when you create the DecisionTreeClassifier object. dt = DecisionTreeClassifer ( criterion = 'entropy' ) from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.tree import export_graphviz import pandas as pd import graphviz from IPython.display import Image df = pd . read_csv ( 'titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values model = DecisionTreeClassifier () X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 22 ) model . fit ( X_train , y_train ) print ( model . predict ([[ 3 , True , 22 , 1 , 0 , 7.25 ]])) feature_names = [ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ] dot_file = export_graphviz ( model , feature_names = feature_names ) graph = graphviz . Source ( dot_file ) #graphviz module to convert it to a png image format graph . render ( filename = 'tree' , format = 'png' , cleanup = True ) [0] tree.png Recall that overfitting is when we do a good job of building a model for the training set, but it doesn\u2019t perform well on the test set. Decision Trees are incredibly prone to overfitting. Since they can keep having additional nodes in the tree that split on features, the model can really dig deep into the specifics of the training set. Depending on the data, this might result in a model that doesn\u2019t capture the true nature of the data and doesn\u2019t generalize. If you let a Decision Tree keep building, it may create a tree that\u2019s overfit and doesn\u2019t capture the essence of the data. In order to solve these issues, we do what\u2019s called pruning the tree. This means we make the tree smaller with the goal of reducing overfitting. There are two types of pruning: pre-pruning & post-pruning. In pre-pruning, we have rules of when to stop building the tree, so we stop building before the tree is too big. In post-pruning we build the whole tree and then we review the tree and decide which leaves to remove to make the tree smaller. Pre-pruning techniques Max depth : Only grow the tree up to a certain depth, or height of the tree. If the max depth is 3, there will be at most 3 splits for each datapoint. Leaf size : Don\u2019t split a node if the number of samples at that node is under a threshold Number of leaf nodes : Limit the total number of leaf nodes allowed in the tree Pruning is a balance. For example, if you set the max depth too small, you won\u2019t have much of a tree and you won\u2019t have any predictive power. This is called underfitting. Similarly if the leaf size is too large, or the number of leaf nodes too small, you\u2019ll have an underfit model. from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split import pandas as pd df = pd . read_csv ( 'https://sololearn.com/uploads/files/titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values ''' Decision Tree with the following properties: \u2022 max depth of 3 \u2022 minimum samples per leaf of 2 \u2022 maximum number of leaf nodes of 10 ''' dt = DecisionTreeClassifier ( max_depth = 3 , min_samples_leaf = 2 , max_leaf_nodes = 10 ) X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 22 ) dt . fit ( X_train , y_train ) print ( dt . predict ([[ 3 , True , 22 , 1 , 0 , 7.25 ]])) [0] We\u2019re not going to be able to intuit best values for the pre-pruning parameters. In order to decide on which to use, we use cross validation and compare metrics. GridSearchCV has four parameters that we\u2019ll use: The model (in this case a DecisionTreeClassifier) Param grid: a dictionary of the parameters names and all the possible values What metric to use (default is accuracy) How many folds for k-fold cross validation from sklearn.model_selection import GridSearchCV from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split import pandas as pd df = pd . read_csv ( 'https://sololearn.com/uploads/files/titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values #Since we have 3 possible values for max_depth, 2 for min_samples_leaf and #4 for max_leaf_nodes, we have 3 * 2 * 4 = 24 different combinations param_grid = { 'max_depth' : [ 5 , 15 , 25 ], 'min_samples_leaf' : [ 1 , 3 ], 'max_leaf_nodes' : [ 10 , 20 , 35 , 50 ]} dt = DecisionTreeClassifier () gs = GridSearchCV ( dt , param_grid , scoring = 'f1' , cv = 5 ) gs . fit ( X , y ) print ( \"Best params: \" , gs . best_params_ ) print ( \"Best score: \" , gs . best_score_ ) Best params: {'max_depth': 25, 'max_leaf_nodes': 35, 'min_samples_leaf': 1} Best score: 0.7745375458302277 Computation When talking about how much computation is required for a machine learning algorithm, we separate it into two questions: how much computation is required to build the model and how much is required to predict. A decision tree is very computationally expensive to build. This is because at every node we are trying every single feature and threshold as a possible split. We have to calculate the information gain of each of these possible splits each time. This is computationally very expensive. Predicting with a decision tree on the other hand, is computational very inexpensive. You just need to ask a series of yes/no questions about the datapoint to get to the prediction. Generally we care much more about the computation time for prediction than training. Predictions often need to happen in real time while a user is waiting for a result. Performance Decision Trees can perform decently well depending on the data, though as we have discussed, they are prone to overfitting. Since a leaf node can have just one datapoint that lands there, it gives too much power to individual datapoints. To remedy the overfitting issues, decision trees generally require some tuning to get the best possible model. Pruning techniques are used to limit the size of the tree and they help mitigate overfitting. Decision Trees often take work to perform on par with how other models perform with no tuning. Interpretability The biggest reason that people like choosing decision trees is because they are easily interpretable. Depending on what you\u2019re building a model for, you might need to give a reason why you made a certain prediction. A non-technical person can interpret a Decision Tree so it\u2019s easy to give an explanation of a prediction. This particularly comes into play in legal situations. Say you are a bank and have a model to predict whether a person should be given a loan or not. It is important to be able to explain why the model made the decision, otherwise you could hide discriminatory practices within the model. ''' Task Given a dataset and a split of the dataset, calculate the information gain using the gini impurity. The first line of the input is a list of the target values in the initial dataset. The second line is the target values of the left split and the third line is the target values of the right split. Round your result to 5 decimal places. You can use round(x, 5). Input Format Three lines of 1's and 0's separated by spaces Output Format Float (rounded to 5 decimal places) ''' import numpy as np S = [ int ( x ) for x in input () . split ()] A = [ int ( x ) for x in input () . split ()] B = [ int ( x ) for x in input () . split ()] sum_s = sum ( S ) / len ( S ) sum_a = sum ( A ) / len ( A ) sum_b = sum ( B ) / len ( B ) #Gini gini_s = 2 * sum_s * ( 1 - sum_s ) #print(round(gini_s,5)) gini_a = 2 * sum_a * ( 1 - sum_a ) #print(round(gini_a,5)) gini_b = 2 * sum_b * ( 1 - sum_b ) #print(round(gini_b,5)) uno = len ( A ) / len ( S ) dos = len ( B ) / len ( S ) #Information Gain res = gini_s - ( uno ) * gini_a - ( dos ) * gini_b print ( round ( res , 5 ))","title":"Decision Trees"},{"location":"ML/pyDecTree/#pre-pruning-techniques","text":"Max depth : Only grow the tree up to a certain depth, or height of the tree. If the max depth is 3, there will be at most 3 splits for each datapoint. Leaf size : Don\u2019t split a node if the number of samples at that node is under a threshold Number of leaf nodes : Limit the total number of leaf nodes allowed in the tree Pruning is a balance. For example, if you set the max depth too small, you won\u2019t have much of a tree and you won\u2019t have any predictive power. This is called underfitting. Similarly if the leaf size is too large, or the number of leaf nodes too small, you\u2019ll have an underfit model. from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split import pandas as pd df = pd . read_csv ( 'https://sololearn.com/uploads/files/titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values ''' Decision Tree with the following properties: \u2022 max depth of 3 \u2022 minimum samples per leaf of 2 \u2022 maximum number of leaf nodes of 10 ''' dt = DecisionTreeClassifier ( max_depth = 3 , min_samples_leaf = 2 , max_leaf_nodes = 10 ) X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 22 ) dt . fit ( X_train , y_train ) print ( dt . predict ([[ 3 , True , 22 , 1 , 0 , 7.25 ]])) [0] We\u2019re not going to be able to intuit best values for the pre-pruning parameters. In order to decide on which to use, we use cross validation and compare metrics. GridSearchCV has four parameters that we\u2019ll use: The model (in this case a DecisionTreeClassifier) Param grid: a dictionary of the parameters names and all the possible values What metric to use (default is accuracy) How many folds for k-fold cross validation from sklearn.model_selection import GridSearchCV from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split import pandas as pd df = pd . read_csv ( 'https://sololearn.com/uploads/files/titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values #Since we have 3 possible values for max_depth, 2 for min_samples_leaf and #4 for max_leaf_nodes, we have 3 * 2 * 4 = 24 different combinations param_grid = { 'max_depth' : [ 5 , 15 , 25 ], 'min_samples_leaf' : [ 1 , 3 ], 'max_leaf_nodes' : [ 10 , 20 , 35 , 50 ]} dt = DecisionTreeClassifier () gs = GridSearchCV ( dt , param_grid , scoring = 'f1' , cv = 5 ) gs . fit ( X , y ) print ( \"Best params: \" , gs . best_params_ ) print ( \"Best score: \" , gs . best_score_ ) Best params: {'max_depth': 25, 'max_leaf_nodes': 35, 'min_samples_leaf': 1} Best score: 0.7745375458302277","title":"Pre-pruning techniques"},{"location":"ML/pyDecTree/#computation","text":"When talking about how much computation is required for a machine learning algorithm, we separate it into two questions: how much computation is required to build the model and how much is required to predict. A decision tree is very computationally expensive to build. This is because at every node we are trying every single feature and threshold as a possible split. We have to calculate the information gain of each of these possible splits each time. This is computationally very expensive. Predicting with a decision tree on the other hand, is computational very inexpensive. You just need to ask a series of yes/no questions about the datapoint to get to the prediction. Generally we care much more about the computation time for prediction than training. Predictions often need to happen in real time while a user is waiting for a result.","title":"Computation"},{"location":"ML/pyDecTree/#performance","text":"Decision Trees can perform decently well depending on the data, though as we have discussed, they are prone to overfitting. Since a leaf node can have just one datapoint that lands there, it gives too much power to individual datapoints. To remedy the overfitting issues, decision trees generally require some tuning to get the best possible model. Pruning techniques are used to limit the size of the tree and they help mitigate overfitting. Decision Trees often take work to perform on par with how other models perform with no tuning.","title":"Performance"},{"location":"ML/pyDecTree/#interpretability","text":"The biggest reason that people like choosing decision trees is because they are easily interpretable. Depending on what you\u2019re building a model for, you might need to give a reason why you made a certain prediction. A non-technical person can interpret a Decision Tree so it\u2019s easy to give an explanation of a prediction. This particularly comes into play in legal situations. Say you are a bank and have a model to predict whether a person should be given a loan or not. It is important to be able to explain why the model made the decision, otherwise you could hide discriminatory practices within the model. ''' Task Given a dataset and a split of the dataset, calculate the information gain using the gini impurity. The first line of the input is a list of the target values in the initial dataset. The second line is the target values of the left split and the third line is the target values of the right split. Round your result to 5 decimal places. You can use round(x, 5). Input Format Three lines of 1's and 0's separated by spaces Output Format Float (rounded to 5 decimal places) ''' import numpy as np S = [ int ( x ) for x in input () . split ()] A = [ int ( x ) for x in input () . split ()] B = [ int ( x ) for x in input () . split ()] sum_s = sum ( S ) / len ( S ) sum_a = sum ( A ) / len ( A ) sum_b = sum ( B ) / len ( B ) #Gini gini_s = 2 * sum_s * ( 1 - sum_s ) #print(round(gini_s,5)) gini_a = 2 * sum_a * ( 1 - sum_a ) #print(round(gini_a,5)) gini_b = 2 * sum_b * ( 1 - sum_b ) #print(round(gini_b,5)) uno = len ( A ) / len ( S ) dos = len ( B ) / len ( S ) #Information Gain res = gini_s - ( uno ) * gini_a - ( dos ) * gini_b print ( round ( res , 5 ))","title":"Interpretability"},{"location":"ML/pyEda/","text":"Basic Functions Example of exploratory data analysis using the Boston Housing Dataset Shape and Columns from sklearn.datasets import load_boston boston_dataset = load_boston () import pandas as pd boston = pd . DataFrame ( boston_dataset . data , columns = boston_dataset . feature_names ) boston [ 'MEDV' ] = boston_dataset . target print ( boston . shape ) #There are 506 records, and 14 columns including 13 features and the target. print ( boston . columns ) (506, 14) Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'], dtype='object') Head and Tail boston [[ 'CHAS' , 'RM' , 'AGE' , 'RAD' , 'MEDV' ]] . head () CHAS RM AGE RAD MEDV 0 0.0 6.575 65.2 1.0 24.0 1 0.0 6.421 78.9 2.0 21.6 2 0.0 7.185 61.1 2.0 34.7 3 0.0 6.998 45.8 3.0 33.4 4 0.0 7.147 54.2 3.0 36.2 Often datasets are loaded from other file formats (e.g., csv, text), it is a good practice to check the first and last few rows of the dataframe and make sure the data is in a consistent format using head and tail, respectively. Describe #To check the summary statistics of the dataset (round to the second decimal place for better display) boston . describe () . round ( 2 ) CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV count 506.00 506.00 506.00 506.00 506.00 506.00 506.00 506.00 506.00 506.00 506.00 506.00 506.00 506.00 mean 3.61 11.36 11.14 0.07 0.55 6.28 68.57 3.80 9.55 408.24 18.46 356.67 12.65 22.53 std 8.60 23.32 6.86 0.25 0.12 0.70 28.15 2.11 8.71 168.54 2.16 91.29 7.14 9.20 min 0.01 0.00 0.46 0.00 0.38 3.56 2.90 1.13 1.00 187.00 12.60 0.32 1.73 5.00 25% 0.08 0.00 5.19 0.00 0.45 5.89 45.02 2.10 4.00 279.00 17.40 375.38 6.95 17.02 50% 0.26 0.00 9.69 0.00 0.54 6.21 77.50 3.21 5.00 330.00 19.05 391.44 11.36 21.20 75% 3.68 12.50 18.10 0.00 0.62 6.62 94.07 5.19 24.00 666.00 20.20 396.22 16.96 25.00 max 88.98 100.00 27.74 1.00 0.87 8.78 100.00 12.13 24.00 711.00 22.00 396.90 37.97 50.00 Plot Hist import matplotlib.pyplot as plt boston . hist ( column = 'CHAS' ) plt . show () boston . hist ( column = 'RM' , bins = 20 ) plt . show () The distribution of RM appears normal and symmetric. The symmetry aligns with what we observed from the output of describe(), as the mean of RM 6.28 is close to its median 6.21. Scatter We specify the type of the plot by passing a string \u2018scatter\u2019 to the argument kind, identify the labels for x and y respectively, and set the size of the figure via a tuple (width, height) in inches. boston . plot ( kind = 'scatter' , x = 'RM' , y = 'MEDV' , figsize = ( 8 , 6 )); boston . plot ( kind = 'scatter' , x = 'LSTAT' , y = 'MEDV' , figsize = ( 8 , 6 )); Correlation Matrix To understand the relationship among features (columns), a correlation matrix is very useful in the exploratory data analysis. Correlation measures linear relationships between variables. corr_matrix = boston . corr () . round ( 2 ) print ( corr_matrix ) CRIM ZN INDUS CHAS NOX ... TAX PTRATIO B LSTAT MEDV CRIM 1.00 -0.20 0.41 -0.06 0.42 ... 0.58 0.29 -0.39 0.46 -0.39 ZN -0.20 1.00 -0.53 -0.04 -0.52 ... -0.31 -0.39 0.18 -0.41 0.36 INDUS 0.41 -0.53 1.00 0.06 0.76 ... 0.72 0.38 -0.36 0.60 -0.48 CHAS -0.06 -0.04 0.06 1.00 0.09 ... -0.04 -0.12 0.05 -0.05 0.18 NOX 0.42 -0.52 0.76 0.09 1.00 ... 0.67 0.19 -0.38 0.59 -0.43 RM -0.22 0.31 -0.39 0.09 -0.30 ... -0.29 -0.36 0.13 -0.61 0.70 AGE 0.35 -0.57 0.64 0.09 0.73 ... 0.51 0.26 -0.27 0.60 -0.38 DIS -0.38 0.66 -0.71 -0.10 -0.77 ... -0.53 -0.23 0.29 -0.50 0.25 RAD 0.63 -0.31 0.60 -0.01 0.61 ... 0.91 0.46 -0.44 0.49 -0.38 TAX 0.58 -0.31 0.72 -0.04 0.67 ... 1.00 0.46 -0.44 0.54 -0.47 PTRATIO 0.29 -0.39 0.38 -0.12 0.19 ... 0.46 1.00 -0.18 0.37 -0.51 B -0.39 0.18 -0.36 0.05 -0.38 ... -0.44 -0.18 1.00 -0.37 0.33 LSTAT 0.46 -0.41 0.60 -0.05 0.59 ... 0.54 0.37 -0.37 1.00 -0.74 MEDV -0.39 0.36 -0.48 0.18 -0.43 ... -0.47 -0.51 0.33 -0.74 1.00 [14 rows x 14 columns]","title":"EDA"},{"location":"ML/pyEda/#basic-functions","text":"Example of exploratory data analysis using the Boston Housing Dataset","title":"Basic Functions"},{"location":"ML/pyEda/#shape-and-columns","text":"from sklearn.datasets import load_boston boston_dataset = load_boston () import pandas as pd boston = pd . DataFrame ( boston_dataset . data , columns = boston_dataset . feature_names ) boston [ 'MEDV' ] = boston_dataset . target print ( boston . shape ) #There are 506 records, and 14 columns including 13 features and the target. print ( boston . columns ) (506, 14) Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'], dtype='object')","title":"Shape and Columns"},{"location":"ML/pyEda/#head-and-tail","text":"boston [[ 'CHAS' , 'RM' , 'AGE' , 'RAD' , 'MEDV' ]] . head () CHAS RM AGE RAD MEDV 0 0.0 6.575 65.2 1.0 24.0 1 0.0 6.421 78.9 2.0 21.6 2 0.0 7.185 61.1 2.0 34.7 3 0.0 6.998 45.8 3.0 33.4 4 0.0 7.147 54.2 3.0 36.2 Often datasets are loaded from other file formats (e.g., csv, text), it is a good practice to check the first and last few rows of the dataframe and make sure the data is in a consistent format using head and tail, respectively.","title":"Head and Tail"},{"location":"ML/pyEda/#describe","text":"#To check the summary statistics of the dataset (round to the second decimal place for better display) boston . describe () . round ( 2 ) CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV count 506.00 506.00 506.00 506.00 506.00 506.00 506.00 506.00 506.00 506.00 506.00 506.00 506.00 506.00 mean 3.61 11.36 11.14 0.07 0.55 6.28 68.57 3.80 9.55 408.24 18.46 356.67 12.65 22.53 std 8.60 23.32 6.86 0.25 0.12 0.70 28.15 2.11 8.71 168.54 2.16 91.29 7.14 9.20 min 0.01 0.00 0.46 0.00 0.38 3.56 2.90 1.13 1.00 187.00 12.60 0.32 1.73 5.00 25% 0.08 0.00 5.19 0.00 0.45 5.89 45.02 2.10 4.00 279.00 17.40 375.38 6.95 17.02 50% 0.26 0.00 9.69 0.00 0.54 6.21 77.50 3.21 5.00 330.00 19.05 391.44 11.36 21.20 75% 3.68 12.50 18.10 0.00 0.62 6.62 94.07 5.19 24.00 666.00 20.20 396.22 16.96 25.00 max 88.98 100.00 27.74 1.00 0.87 8.78 100.00 12.13 24.00 711.00 22.00 396.90 37.97 50.00","title":"Describe"},{"location":"ML/pyEda/#plot","text":"","title":"Plot"},{"location":"ML/pyEda/#hist","text":"import matplotlib.pyplot as plt boston . hist ( column = 'CHAS' ) plt . show () boston . hist ( column = 'RM' , bins = 20 ) plt . show () The distribution of RM appears normal and symmetric. The symmetry aligns with what we observed from the output of describe(), as the mean of RM 6.28 is close to its median 6.21.","title":"Hist"},{"location":"ML/pyEda/#scatter","text":"We specify the type of the plot by passing a string \u2018scatter\u2019 to the argument kind, identify the labels for x and y respectively, and set the size of the figure via a tuple (width, height) in inches. boston . plot ( kind = 'scatter' , x = 'RM' , y = 'MEDV' , figsize = ( 8 , 6 )); boston . plot ( kind = 'scatter' , x = 'LSTAT' , y = 'MEDV' , figsize = ( 8 , 6 ));","title":"Scatter"},{"location":"ML/pyEda/#correlation-matrix","text":"To understand the relationship among features (columns), a correlation matrix is very useful in the exploratory data analysis. Correlation measures linear relationships between variables. corr_matrix = boston . corr () . round ( 2 ) print ( corr_matrix ) CRIM ZN INDUS CHAS NOX ... TAX PTRATIO B LSTAT MEDV CRIM 1.00 -0.20 0.41 -0.06 0.42 ... 0.58 0.29 -0.39 0.46 -0.39 ZN -0.20 1.00 -0.53 -0.04 -0.52 ... -0.31 -0.39 0.18 -0.41 0.36 INDUS 0.41 -0.53 1.00 0.06 0.76 ... 0.72 0.38 -0.36 0.60 -0.48 CHAS -0.06 -0.04 0.06 1.00 0.09 ... -0.04 -0.12 0.05 -0.05 0.18 NOX 0.42 -0.52 0.76 0.09 1.00 ... 0.67 0.19 -0.38 0.59 -0.43 RM -0.22 0.31 -0.39 0.09 -0.30 ... -0.29 -0.36 0.13 -0.61 0.70 AGE 0.35 -0.57 0.64 0.09 0.73 ... 0.51 0.26 -0.27 0.60 -0.38 DIS -0.38 0.66 -0.71 -0.10 -0.77 ... -0.53 -0.23 0.29 -0.50 0.25 RAD 0.63 -0.31 0.60 -0.01 0.61 ... 0.91 0.46 -0.44 0.49 -0.38 TAX 0.58 -0.31 0.72 -0.04 0.67 ... 1.00 0.46 -0.44 0.54 -0.47 PTRATIO 0.29 -0.39 0.38 -0.12 0.19 ... 0.46 1.00 -0.18 0.37 -0.51 B -0.39 0.18 -0.36 0.05 -0.38 ... -0.44 -0.18 1.00 -0.37 0.33 LSTAT 0.46 -0.41 0.60 -0.05 0.59 ... 0.54 0.37 -0.37 1.00 -0.74 MEDV -0.39 0.36 -0.48 0.18 -0.43 ... -0.47 -0.51 0.33 -0.74 1.00 [14 rows x 14 columns]","title":"Correlation Matrix"},{"location":"ML/pyGen/","text":"Useful modules for ML and Data Science Pandas - read and manipulate data Numpy - manipulating lists and tables of numerical data Numba - makes Python code fast Matplotlib - for creating static, animated, and interactive visualizations in Python Scikit-learn - machine learning in Python EDA - exploratory data analysis Linear Regression Logistic Regression K-Nearest Neighbors Decision Trees Random Forest Neural Networks Clustering Pipelines Cross-Validation Gradient Boosting Data Leakage","title":"General"},{"location":"ML/pyGradBoost/","text":"Gradient boosting is a method that goes through cycles to iteratively add models into an ensemble. It begins by initializing the ensemble with a single model, whose predictions can be pretty naive. (Even if its predictions are wildly inaccurate, subsequent additions to the ensemble will address those errors.) Then, we start the cycle: First, we use the current ensemble to generate predictions for each observation in the dataset. To make a prediction, we add the predictions from all models in the ensemble. These predictions are used to calculate a loss function (like mean squared error, for instance). Then, we use the loss function to fit a new model that will be added to the ensemble. Specifically, we determine model parameters so that adding this new model to the ensemble will reduce the loss. (Side note: The \"gradient\" in \"gradient boosting\" refers to the fact that we'll use gradient descent on the loss function to determine the parameters in this new model.) Finally, we add the new model to ensemble, and ... ... repeat! import pandas as pd from sklearn.model_selection import train_test_split # Read the data data = pd . read_csv ( 'https://raw.githubusercontent.com/esabunor/MLWorkspace/master/melb_data.csv' ) # Select subset of predictors cols_to_use = [ 'Rooms' , 'Distance' , 'Landsize' , 'BuildingArea' , 'YearBuilt' ] X = data [ cols_to_use ] # Select target y = data . Price # Separate data into training and validation sets X_train , X_valid , y_train , y_valid = train_test_split ( X , y ) In this example, you'll work with the XGBoost library. XGBoost stands for extreme gradient boosting, which is an implementation of gradient boosting with several additional features focused on performance and speed. (Scikit-learn has another version of gradient boosting, but XGBoost has some technical advantages.) XGBoost is a the leading software library for working with standard tabular data (the type of data you store in Pandas DataFrames, as opposed to more exotic types of data like images and videos). With careful parameter tuning, you can train highly accurate models. In the next code cell, we import the scikit-learn API for XGBoost (xgboost.XGBRegressor). This allows us to build and fit a model just as we would in scikit-learn. As you'll see in the output, the XGBRegressor class has many tunable parameters. from xgboost import XGBRegressor my_model = XGBRegressor () my_model . fit ( X_train , y_train ) We also make predictions and evaluate the model. from sklearn.metrics import mean_absolute_error predictions = my_model . predict ( X_valid ) print ( \"Mean Absolute Error: \" + str ( mean_absolute_error ( predictions , y_valid ))) Mean Absolute Error: 274873.6334291422 Parameter Tuning XGBoost has a few parameters that can dramatically affect accuracy and training speed. The first parameters you should understand are: n_estimators n_estimators specifies how many times to go through the modeling cycle described above. It is equal to the number of models that we include in the ensemble. Too low a value causes underfitting, which leads to inaccurate predictions on both training data and test data. Too high a value causes overfitting, which causes accurate predictions on training data, but inaccurate predictions on test data (which is what we care about). Typical values range from 100-1000, though this depends a lot on the learning_rate parameter discussed below. Here is the code to set the number of models in the ensemble: my_model = XGBRegressor ( n_estimators = 500 ) my_model . fit ( X_train , y_train ) early_stopping_rounds early_stopping_rounds offers a way to automatically find the ideal value for n_estimators. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. It's smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating. Since random chance sometimes causes a single round where validation scores don't improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. Setting early_stopping_rounds=5 is a reasonable choice. In this case, we stop after 5 straight rounds of deteriorating validation scores. When using early_stopping_rounds, you also need to set aside some data for calculating the validation scores - this is done by setting the eval_set parameter. We can modify the example above to include early stopping: my_model = XGBRegressor ( n_estimators = 500 ) my_model . fit ( X_train , y_train , early_stopping_rounds = 5 , eval_set = [( X_valid , y_valid )], verbose = False ) If you later want to fit a model with all of your data, set n_estimators to whatever value you found to be optimal when run with early stopping. learning_rate Instead of getting predictions by simply adding up the predictions from each component model, we can multiply the predictions from each model by a small number (known as the learning rate) before adding them in. This means each tree we add to the ensemble helps us less. So, we can set a higher value for n_estimators without overfitting. If we use early stopping, the appropriate number of trees will be determined automatically. In general, a small learning rate and large number of estimators will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle. As default, XGBoost sets learning_rate=0.1. Modifying the example above to change the learning rate yields the following code: my_model = XGBRegressor ( n_estimators = 1000 , learning_rate = 0.05 ) my_model . fit ( X_train , y_train , early_stopping_rounds = 5 , eval_set = [( X_valid , y_valid )], verbose = False ) n_jobs On larger datasets where runtime is a consideration, you can use parallelism to build your models faster. It's common to set the parameter n_jobs equal to the number of cores on your machine. On smaller datasets, this won't help. The resulting model won't be any better, so micro-optimizing for fitting time is typically nothing but a distraction. But, it's useful in large datasets where you would otherwise spend a long time waiting during the fit command. Here's the modified example: my_model = XGBRegressor ( n_estimators = 1000 , learning_rate = 0.05 , n_jobs = 4 ) my_model . fit ( X_train , y_train , early_stopping_rounds = 5 , eval_set = [( X_valid , y_valid )], verbose = False )","title":"Gradient Boosting"},{"location":"ML/pyGradBoost/#parameter-tuning","text":"XGBoost has a few parameters that can dramatically affect accuracy and training speed. The first parameters you should understand are:","title":"Parameter Tuning"},{"location":"ML/pyGradBoost/#n_estimators","text":"n_estimators specifies how many times to go through the modeling cycle described above. It is equal to the number of models that we include in the ensemble. Too low a value causes underfitting, which leads to inaccurate predictions on both training data and test data. Too high a value causes overfitting, which causes accurate predictions on training data, but inaccurate predictions on test data (which is what we care about). Typical values range from 100-1000, though this depends a lot on the learning_rate parameter discussed below. Here is the code to set the number of models in the ensemble: my_model = XGBRegressor ( n_estimators = 500 ) my_model . fit ( X_train , y_train )","title":"n_estimators"},{"location":"ML/pyGradBoost/#early_stopping_rounds","text":"early_stopping_rounds offers a way to automatically find the ideal value for n_estimators. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. It's smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating. Since random chance sometimes causes a single round where validation scores don't improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. Setting early_stopping_rounds=5 is a reasonable choice. In this case, we stop after 5 straight rounds of deteriorating validation scores. When using early_stopping_rounds, you also need to set aside some data for calculating the validation scores - this is done by setting the eval_set parameter. We can modify the example above to include early stopping: my_model = XGBRegressor ( n_estimators = 500 ) my_model . fit ( X_train , y_train , early_stopping_rounds = 5 , eval_set = [( X_valid , y_valid )], verbose = False ) If you later want to fit a model with all of your data, set n_estimators to whatever value you found to be optimal when run with early stopping.","title":"early_stopping_rounds"},{"location":"ML/pyGradBoost/#learning_rate","text":"Instead of getting predictions by simply adding up the predictions from each component model, we can multiply the predictions from each model by a small number (known as the learning rate) before adding them in. This means each tree we add to the ensemble helps us less. So, we can set a higher value for n_estimators without overfitting. If we use early stopping, the appropriate number of trees will be determined automatically. In general, a small learning rate and large number of estimators will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle. As default, XGBoost sets learning_rate=0.1. Modifying the example above to change the learning rate yields the following code: my_model = XGBRegressor ( n_estimators = 1000 , learning_rate = 0.05 ) my_model . fit ( X_train , y_train , early_stopping_rounds = 5 , eval_set = [( X_valid , y_valid )], verbose = False )","title":"learning_rate"},{"location":"ML/pyGradBoost/#n_jobs","text":"On larger datasets where runtime is a consideration, you can use parallelism to build your models faster. It's common to set the parameter n_jobs equal to the number of cores on your machine. On smaller datasets, this won't help. The resulting model won't be any better, so micro-optimizing for fitting time is typically nothing but a distraction. But, it's useful in large datasets where you would otherwise spend a long time waiting during the fit command. Here's the modified example: my_model = XGBRegressor ( n_estimators = 1000 , learning_rate = 0.05 , n_jobs = 4 ) my_model . fit ( X_train , y_train , early_stopping_rounds = 5 , eval_set = [( X_valid , y_valid )], verbose = False )","title":"n_jobs"},{"location":"ML/pyLogReg/","text":"Titanic Prep Data with Pandas import pandas as pd import matplotlib.pyplot as plt df = pd . read_csv ( 'titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values print ( X ) print ( y ) Build a Logistic Regression Model with Sklearn from sklearn.linear_model import LogisticRegression model = LogisticRegression () X = df [[ 'Fare' , 'Age' ]] . values y = df [ 'Survived' ] . values model . fit ( X , y ) print ( model . coef_ , model . intercept_ ) [[ 0.01615949 -0.01549065]] [-0.51037152] These values mean that the equation is as follows: 0 = 0.0161594x + -0.01549065y + -0.51037152 Make Predictions with the Model (with all the variables) X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values model = LogisticRegression () model . fit ( X , y ) #predict print ( model . predict ([[ 3 , True , 22.0 , 1 , 0 , 7.25 ]])) print ( model . predict ( X [: 5 ])) print ( y [: 5 ]) print ( model . score ( X , y )) #model accuracy [0] [0 1 1 1 0] [0 1 1 1 0] 0.8049605411499436 Breast Cancer Dataset from sklearn.datasets import load_breast_cancer import pandas as pd cancer_data = load_breast_cancer () #print(cancer_data.keys()) #print(cancer_data['DESCR']) df = pd . DataFrame ( cancer_data [ 'data' ], columns = cancer_data [ 'feature_names' ]) df [ 'target' ] = cancer_data [ 'target' ] print ( df . head ()) X = df [ cancer_data . feature_names ] . values y = df [ 'target' ] . values model = LogisticRegression ( solver = 'liblinear' ) #needed to avoid iterations limit model . fit ( X , y ) model . predict ([ X [ 0 ]]) #prediction for datapoint 0 model . score ( X , y ) #Accuracy is the percent of predictions that are correct. mean radius mean texture mean perimeter ... worst symmetry \\ 0 17.99 10.38 122.80 ... 0.4601 1 20.57 17.77 132.90 ... 0.2750 2 19.69 21.25 130.00 ... 0.3613 3 11.42 20.38 77.58 ... 0.6638 4 20.29 14.34 135.10 ... 0.2364 worst fractal dimension target 0 0.11890 0 1 0.08902 0 2 0.08758 0 3 0.17300 0 4 0.07678 0 [5 rows x 31 columns] 0.9595782073813708 Accuracy is a good measure if our classes are evenly split, but is very misleading if we have imbalanced classes. Always use caution with accuracy. You need to know the distribution of the classes to know how to interpret the value. Metrics The Confusion Matrix is a table showing four values: Datapoints we predicted positive that are actually positive: true positive (TP) Datapoints we predicted positive that are actually negative: true negative (TN) Datapoints we predicted negative that are actually positive: false positive (FP) Datapoints we predicted negative that are actually negative: false negative (FN) The confusion matrix fully describes how a model performs on a dataset, though is difficult to use to compare models. Precision is a measure of how precise the model is with its positive predictions. Precision = TP / ( TP + FP) Recall is the percent of positive cases that the model predicts correctly. Recall = TP / (TP + FN) We often will be in a situation of choosing between increasing the recall (while lowering the precision) or increasing the precision (and lowering the recall). There\u2019s no hard and fast rule on what values of precision and recall you\u2019re shooting for. It always depends on the dataset and the application. F1 Score Accuracy was an appealing metric because it was a single number. Precision and recall are two numbers so it\u2019s not always obvious how to choose between two models if one has a higher precision and the other has a higher recall. The F1 score is an average of precision and recall so that we have a single score for our model. The F1 score is the harmonic mean of the precision and recall values. F1 = 2 (precision recall)/(precision + recall) Scikit-learn has a confusion matrix function. Scikit-learn reverses the confusion matrix to show the negative counts first! Since negative target values correspond to 0 and positive to 1, scikit-learn has ordered them in this order. Make sure you double check that you are interpreting the values correctly! import pandas as pd from sklearn.linear_model import LogisticRegression df = pd . read_csv ( 'titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values model = LogisticRegression () model . fit ( X , y ) y_pred = model . predict ( X ) from sklearn.metrics import accuracy_score , precision_score , recall_score , f1_score , confusion_matrix print ( \"Accuracy :\" , accuracy_score ( y , y_pred )) print ( \"Precision :\" , precision_score ( y , y_pred )) print ( \"Recall :\" , recall_score ( y , y_pred )) print ( \"F1 :\" , f1_score ( y , y_pred )) #Confusion matrix print ( confusion_matrix ( y , y_pred )) #Confusion matrix is not in standard view, negative counts are first Accuracy : 0.8049605411499436 Precision : 0.7734627831715211 Recall : 0.6988304093567251 F1 : 0.7342549923195083 [[475 70] [103 239]] Training and Testing Models can suffer from either: Overfitting : capturing spurious patterns that won't recur in the future, leading to less accurate predictions, or Underfitting : failing to capture relevant patterns, again leading to less accurate predictions. We use validation data, which isn't used in model training, to measure a candidate model's accuracy. This lets us try many candidate models and keep the best one. Overfitting is when we perform well on the data the model has already seen, but we don\u2019t perform well on new data. To simulate making predictions on new unseen data, we can break our dataset into a training set and a test set. The training set is used for building the models. The test set is used for evaluating the models. We split our data before building the model, thus the model has no knowledge of the test set and we\u2019ll be giving it a fair assessment. A standard breakdown is to put 70-80% of our data in the training set and 20-30% in the test set. Using less data in the training set means that our model won\u2019t have as much data to learn from, so we want to give it as much as possible while still leaving enough for evaluation. train_test_split function will randomly put each datapoint in either the training set or the test set. By default the training set is 75% of the data and the test set is the remaining 25% of the data. We can change the size of our training set by using the train_size parameter. E.g. train_test_split(X, y, train_size=0.6) would put 60% of the data in the training set and 40% in the test set. Our accuracy, precision, recall and F1 score values are actually very similar to the values when we used the entire dataset. This is a sign our model is not overfit! from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y ) print ( \"whole dataset:\" , X . shape , y . shape ) print ( \"training set:\" , X_train . shape , y_train . shape ) print ( \"test set:\" , X_test . shape , y_test . shape ) whole dataset: (887, 6) (887,) training set: (665, 6) (665,) test set: (222, 6) (222,) import pandas as pd from sklearn.linear_model import LogisticRegression df = pd . read_csv ( 'titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values model = LogisticRegression () model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( model . score ( X_test , y_test )) print ( \"Accuracy :\" , accuracy_score ( y_test , y_pred )) print ( \"Precision :\" , precision_score ( y_test , y_pred )) print ( \"Recall :\" , recall_score ( y_test , y_pred )) print ( \"F1 :\" , f1_score ( y_test , y_pred )) 0.8018018018018018 Accuracy : 0.8018018018018018 Precision : 0.8108108108108109 Recall : 0.6666666666666666 F1 : 0.7317073170731707 To get the same split every time, we can use the random_state attribute. We choose an arbitrary number to give it, and then every time we run the code, we will get the same split. The random state is also called a seed. from sklearn.model_selection import train_test_split X = [[ 1 , 1 ],[ 2 , 2 ],[ 3 , 3 ],[ 4 , 4 ]] y = [ 0 , 0 , 1 , 1 ] #X_train, X_test, y_train, y_test = train_test_split(X, y) #with a seed X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 27 ) print ( \"X_train\" , X_train ) print ( \"X_test\" , X_test ) X_train [[3, 3], [1, 1], [4, 4]] X_test [[2, 2]] An ROC Curve is a graph of the sensitivity vs. the specificity . These values demonstrate the same trade-off that precision and recall demonstrate. The sensitivity is another term for the recall, which is the true positive rate. The specificity is the true negative rate. TN / (TN + FP) The goal is to maximize these two values, though generally making one larger makes the other lower. It will depend on the situation whether we put more emphasis on sensitivity or specificity. import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import recall_score , precision_recall_fscore_support sensitivity_score = recall_score def specificity_score ( y_true , y_pred ): p , r , f , s = precision_recall_fscore_support ( y_true , y_pred ) return r [ 0 ] df = pd . read_csv ( 'titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 5 ) model = LogisticRegression () model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( model . score ( X_test , y_test )) print ( sensitivity_score ( y_test , y_pred )) print ( specificity_score ( y_test , y_pred )) 0.8333333333333334 0.6829268292682927 0.9214285714285714 Adjusting the Logistic Regression Threshold in Sklearn When you use scikit-learn\u2019s predict method, you are given 0 and 1 values of the prediction. However, behind the scenes the Logistic Regression model is getting a probability value between 0 and 1 for each datapoint and then rounding to either 0 or 1. If we want to choose a different threshold besides 0.5, we\u2019ll want those probability values. We can use the predict_proba function to get them. (model.predict_proba(X_test) The result is a numpy array with 2 values for each datapoint (e.g. [0.78, 0.22]). You\u2019ll notice that the two values sum to 1. The first value is the probability that the datapoint is in the 0 class (didn\u2019t survive) and the second is the probability that the datapoint is in the 1 class (survived). We only need the second column of this result, which we can pull with the following numpy syntax. model.predict_proba(X_test)[:, 1] import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import recall_score , precision_recall_fscore_support sensitivity_score = recall_score def specificity_score ( y_true , y_pred ): p , r , f , s = precision_recall_fscore_support ( y_true , y_pred ) return r [ 0 ] df = pd . read_csv ( 'titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 5 ) model = LogisticRegression () model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) #A threshold of 0.75 means we need to be more confident in order to make a positive prediction. #This results in fewer positive predictions and more negative predictions. #Setting the threshold to 0.5 we would get #the original Logistic Regression model. #Any other threshold value yields an alternative model. y_pred = model . predict_proba ( X_test )[:, 1 ] > 0.75 print ( model . score ( X_test , y_test )) print ( sensitivity_score ( y_test , y_pred )) print ( specificity_score ( y_test , y_pred )) 0.8333333333333334 0.43902439024390244 0.9785714285714285 ROC Curve The ROC curve is a graph of the specificity vs the sensitivity. We build a Logistic Regression model and then calculate the specificity and sensitivity for every possible threshold. Every predicted probability is a threshold. Note that we actually plot the sensitivity vs (1-specificity). There is no strong reason for doing it this way besides that it\u2019s the standard. Scikit-learn has a roc_curve function we can use. The function takes the true target values and the predicted probabilities from our model. We first use the predict_proba method on the model to get the probabilities. Then we call the roc_curve function. The roc_curve function returns an array of the false positive rates, an array of the true positive rates and the thresholds. The false positive rate is 1-specificity (x-axis) and the true positive rate is another term for the sensitivity (y-axis). The threshold values won\u2019t be needed in the graph. Here\u2019s the code for plotting the ROC curve in matplotlib. Note that we also have code for plotting a diagonal line. This can help us visually see how far our model is from a model that predicts randomly. As we don\u2019t use the threshold values to build the graph, the graph does not tell us what threshold would yield each of the possible models. import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import recall_score , precision_recall_fscore_support , roc_curve import matplotlib.pyplot as plt sensitivity_score = recall_score def specificity_score ( y_true , y_pred ): p , r , f , s = precision_recall_fscore_support ( y_true , y_pred ) return r [ 0 ] df = pd . read_csv ( 'titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 5 ) model = LogisticRegression () model . fit ( X_train , y_train ) y_pred_proba = model . predict_proba ( X_test ) fpr , tpr , thresholds = roc_curve ( y_test , y_pred_proba [:, 1 ]) plt . plot ( fpr , tpr ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], linestyle = '--' ) plt . xlim ([ 0.0 , 1.0 ]) plt . ylim ([ 0.0 , 1.0 ]) plt . xlabel ( '1 - specificity' ) plt . ylabel ( 'sensitivity' ) plt . show () The ROC curve is showing the performance, not of a single model, but of many models. Each choice of threshold is a different model. Each point A, B & C refers to a model with a different threshold. Model A has a sensitivity of 0.6 and a specificity of 0.9 (recall that the graph is showing 1-specificity). Model B has a sensitivity of 0.8 and a specificity of 0.7. Model C has a sensitivity of 0.9 and a specificity of 0.5. How to choose between these models will depend on the specifics of our situation. The closer the curve gets to the upper left corner, the better the performance. The line should never fall below the diagonal line as that would mean it performs worse than a random model. If we are in a situation where it\u2019s more important that all of our positive predictions are correct than that we catch all the positive cases (meaning that we predict most of the negative cases correctly), we should choose the model with higher specificity (model A). If we are in a situation where it\u2019s important that we catch as many of the positive cases as possible, we should choose the model with the higher sensitivity (model C). If we want a balance between sensitivity and specificity, we should choose model B. The Area Under the Curve, the AUC,is a value between 0 and 1, the higher the better. Since the ROC is a graph of all the different Logistic Regression models with different thresholds, the AUC does not measure the performance of a single model. It gives a general sense of how well the Logistic Regression model is performing. To get a single model, you still need to find the optimal threshold for your problem. It\u2019s important to note that this metric tells us how well in general a Logistic Regression model performs on our data. As an ROC curve shows the performance of multiple models, the AUC is not measuring the performance of a single model. import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score df = pd . read_csv ( 'titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 5 ) model1 = LogisticRegression () model1 . fit ( X_train , y_train ) y_pred_proba1 = model1 . predict_proba ( X_test ) print ( \"Model 1 AUC score: \" , roc_auc_score ( y_test , y_pred_proba1 [:, 1 ])) #model2 with just Pclass and male features. model2 = LogisticRegression () model2 . fit ( X_train [:, 0 : 2 ], y_train ) y_pred_proba2 = model2 . predict_proba ( X_test [:, 0 : 2 ]) print ( \"Model 2 AUC score: \" , roc_auc_score ( y_test , y_pred_proba2 [:, 1 ])) Model 1 AUC score: 0.8572299651567944 Model 2 AUC score: 0.8390679442508711 K-fold Cross Validation Instead of doing a single train/test split, we\u2019ll split our data into a training set and test set multiple times. You will only see values this different when you have a small dataset. With large datasets we often just do a training and test set for simplicity. This process for creating multiple training and test sets is called k-fold cross validation. The k is the number of chunks we split our dataset into. The standard number is 5. Our goal in cross validation is to get accurate measures for our metrics (accuracy, precision, recall). We are building extra models in order to feel confident in the numbers we calculate and report. These 5 models were built just for evaluation purposes, so that we can report the metric values. We don\u2019t actually need these models and want to build the best possible model. The best possible model is going to be a model that uses all of the data. So we keep track of our calculated values for our evaluation metrics and then build a model using all of the data. from sklearn.model_selection import KFold import pandas as pd df = pd . read_csv ( 'titanic.csv' ) X = df [[ 'Age' , 'Fare' ]] . values [: 6 ] y = df [ 'Survived' ] . values [: 6 ] kf = KFold ( n_splits = 3 , shuffle = True ) for train , test in kf . split ( X ): print ( train , test ) #3 training and testing sets as expected [0 2 3 4] [1 5] [1 2 4 5] [0 3] [0 1 3 5] [2 4] from sklearn.model_selection import KFold import pandas as pd df = pd . read_csv ( 'titanic.csv' ) X = df [[ 'Age' , 'Fare' ]] . values [: 6 ] y = df [ 'Survived' ] . values [: 6 ] kf = KFold ( n_splits = 3 , shuffle = True ) splits = list ( kf . split ( X )) first_split = splits [ 0 ] train_indices , test_indices = first_split print ( \"training set indices\" , train_indices ) print ( \"test set indices\" , test_indices ) X_train = X [ train_indices ] X_test = X [ test_indices ] y_train = y [ train_indices ] y_test = y [ test_indices ] print ( \"X train\" ) print ( X_train ) print ( \"y_train\" , y_train ) print ( \"X test\" ) print ( X_test ) print ( \"y_test\" , y_test ) training set indices [0 1 3 5] test set indices [2 4] X train [[22. 7.25 ] [38. 71.2833] [35. 53.1 ] [27. 8.4583]] y_train [0 1 1 0] X test [[26. 7.925] [35. 8.05 ]] y_test [1 0] from sklearn.model_selection import KFold import numpy as np from sklearn.linear_model import LogisticRegression import pandas as pd df = pd . read_csv ( 'titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values scores = [] kf = KFold ( n_splits = 5 , shuffle = True ) for train_index , test_index in kf . split ( X ): X_train , X_test = X [ train_indices ], X [ test_indices ] y_train , y_test = y [ train_indices ], y [ test_indices ] model = LogisticRegression () model . fit ( X_train , y_train ) scores . append ( model . score ( X_test , y_test )) print ( scores ) print ( np . mean ( scores )) [0.7584269662921348, 0.7584269662921348, 0.7584269662921348, 0.7584269662921348, 0.7584269662921348] 0.7584269662921348","title":"Logistic Regression"},{"location":"ML/pyLogReg/#titanic","text":"Prep Data with Pandas import pandas as pd import matplotlib.pyplot as plt df = pd . read_csv ( 'titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values print ( X ) print ( y ) Build a Logistic Regression Model with Sklearn from sklearn.linear_model import LogisticRegression model = LogisticRegression () X = df [[ 'Fare' , 'Age' ]] . values y = df [ 'Survived' ] . values model . fit ( X , y ) print ( model . coef_ , model . intercept_ ) [[ 0.01615949 -0.01549065]] [-0.51037152] These values mean that the equation is as follows: 0 = 0.0161594x + -0.01549065y + -0.51037152 Make Predictions with the Model (with all the variables) X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values model = LogisticRegression () model . fit ( X , y ) #predict print ( model . predict ([[ 3 , True , 22.0 , 1 , 0 , 7.25 ]])) print ( model . predict ( X [: 5 ])) print ( y [: 5 ]) print ( model . score ( X , y )) #model accuracy [0] [0 1 1 1 0] [0 1 1 1 0] 0.8049605411499436","title":"Titanic"},{"location":"ML/pyLogReg/#breast-cancer-dataset","text":"from sklearn.datasets import load_breast_cancer import pandas as pd cancer_data = load_breast_cancer () #print(cancer_data.keys()) #print(cancer_data['DESCR']) df = pd . DataFrame ( cancer_data [ 'data' ], columns = cancer_data [ 'feature_names' ]) df [ 'target' ] = cancer_data [ 'target' ] print ( df . head ()) X = df [ cancer_data . feature_names ] . values y = df [ 'target' ] . values model = LogisticRegression ( solver = 'liblinear' ) #needed to avoid iterations limit model . fit ( X , y ) model . predict ([ X [ 0 ]]) #prediction for datapoint 0 model . score ( X , y ) #Accuracy is the percent of predictions that are correct. mean radius mean texture mean perimeter ... worst symmetry \\ 0 17.99 10.38 122.80 ... 0.4601 1 20.57 17.77 132.90 ... 0.2750 2 19.69 21.25 130.00 ... 0.3613 3 11.42 20.38 77.58 ... 0.6638 4 20.29 14.34 135.10 ... 0.2364 worst fractal dimension target 0 0.11890 0 1 0.08902 0 2 0.08758 0 3 0.17300 0 4 0.07678 0 [5 rows x 31 columns] 0.9595782073813708 Accuracy is a good measure if our classes are evenly split, but is very misleading if we have imbalanced classes. Always use caution with accuracy. You need to know the distribution of the classes to know how to interpret the value.","title":"Breast Cancer Dataset"},{"location":"ML/pyLogReg/#metrics","text":"The Confusion Matrix is a table showing four values: Datapoints we predicted positive that are actually positive: true positive (TP) Datapoints we predicted positive that are actually negative: true negative (TN) Datapoints we predicted negative that are actually positive: false positive (FP) Datapoints we predicted negative that are actually negative: false negative (FN) The confusion matrix fully describes how a model performs on a dataset, though is difficult to use to compare models. Precision is a measure of how precise the model is with its positive predictions. Precision = TP / ( TP + FP) Recall is the percent of positive cases that the model predicts correctly. Recall = TP / (TP + FN) We often will be in a situation of choosing between increasing the recall (while lowering the precision) or increasing the precision (and lowering the recall). There\u2019s no hard and fast rule on what values of precision and recall you\u2019re shooting for. It always depends on the dataset and the application. F1 Score Accuracy was an appealing metric because it was a single number. Precision and recall are two numbers so it\u2019s not always obvious how to choose between two models if one has a higher precision and the other has a higher recall. The F1 score is an average of precision and recall so that we have a single score for our model. The F1 score is the harmonic mean of the precision and recall values. F1 = 2 (precision recall)/(precision + recall) Scikit-learn has a confusion matrix function. Scikit-learn reverses the confusion matrix to show the negative counts first! Since negative target values correspond to 0 and positive to 1, scikit-learn has ordered them in this order. Make sure you double check that you are interpreting the values correctly! import pandas as pd from sklearn.linear_model import LogisticRegression df = pd . read_csv ( 'titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values model = LogisticRegression () model . fit ( X , y ) y_pred = model . predict ( X ) from sklearn.metrics import accuracy_score , precision_score , recall_score , f1_score , confusion_matrix print ( \"Accuracy :\" , accuracy_score ( y , y_pred )) print ( \"Precision :\" , precision_score ( y , y_pred )) print ( \"Recall :\" , recall_score ( y , y_pred )) print ( \"F1 :\" , f1_score ( y , y_pred )) #Confusion matrix print ( confusion_matrix ( y , y_pred )) #Confusion matrix is not in standard view, negative counts are first Accuracy : 0.8049605411499436 Precision : 0.7734627831715211 Recall : 0.6988304093567251 F1 : 0.7342549923195083 [[475 70] [103 239]]","title":"Metrics"},{"location":"ML/pyLogReg/#training-and-testing","text":"Models can suffer from either: Overfitting : capturing spurious patterns that won't recur in the future, leading to less accurate predictions, or Underfitting : failing to capture relevant patterns, again leading to less accurate predictions. We use validation data, which isn't used in model training, to measure a candidate model's accuracy. This lets us try many candidate models and keep the best one. Overfitting is when we perform well on the data the model has already seen, but we don\u2019t perform well on new data. To simulate making predictions on new unseen data, we can break our dataset into a training set and a test set. The training set is used for building the models. The test set is used for evaluating the models. We split our data before building the model, thus the model has no knowledge of the test set and we\u2019ll be giving it a fair assessment. A standard breakdown is to put 70-80% of our data in the training set and 20-30% in the test set. Using less data in the training set means that our model won\u2019t have as much data to learn from, so we want to give it as much as possible while still leaving enough for evaluation. train_test_split function will randomly put each datapoint in either the training set or the test set. By default the training set is 75% of the data and the test set is the remaining 25% of the data. We can change the size of our training set by using the train_size parameter. E.g. train_test_split(X, y, train_size=0.6) would put 60% of the data in the training set and 40% in the test set. Our accuracy, precision, recall and F1 score values are actually very similar to the values when we used the entire dataset. This is a sign our model is not overfit! from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y ) print ( \"whole dataset:\" , X . shape , y . shape ) print ( \"training set:\" , X_train . shape , y_train . shape ) print ( \"test set:\" , X_test . shape , y_test . shape ) whole dataset: (887, 6) (887,) training set: (665, 6) (665,) test set: (222, 6) (222,) import pandas as pd from sklearn.linear_model import LogisticRegression df = pd . read_csv ( 'titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values model = LogisticRegression () model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( model . score ( X_test , y_test )) print ( \"Accuracy :\" , accuracy_score ( y_test , y_pred )) print ( \"Precision :\" , precision_score ( y_test , y_pred )) print ( \"Recall :\" , recall_score ( y_test , y_pred )) print ( \"F1 :\" , f1_score ( y_test , y_pred )) 0.8018018018018018 Accuracy : 0.8018018018018018 Precision : 0.8108108108108109 Recall : 0.6666666666666666 F1 : 0.7317073170731707 To get the same split every time, we can use the random_state attribute. We choose an arbitrary number to give it, and then every time we run the code, we will get the same split. The random state is also called a seed. from sklearn.model_selection import train_test_split X = [[ 1 , 1 ],[ 2 , 2 ],[ 3 , 3 ],[ 4 , 4 ]] y = [ 0 , 0 , 1 , 1 ] #X_train, X_test, y_train, y_test = train_test_split(X, y) #with a seed X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 27 ) print ( \"X_train\" , X_train ) print ( \"X_test\" , X_test ) X_train [[3, 3], [1, 1], [4, 4]] X_test [[2, 2]] An ROC Curve is a graph of the sensitivity vs. the specificity . These values demonstrate the same trade-off that precision and recall demonstrate. The sensitivity is another term for the recall, which is the true positive rate. The specificity is the true negative rate. TN / (TN + FP) The goal is to maximize these two values, though generally making one larger makes the other lower. It will depend on the situation whether we put more emphasis on sensitivity or specificity. import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import recall_score , precision_recall_fscore_support sensitivity_score = recall_score def specificity_score ( y_true , y_pred ): p , r , f , s = precision_recall_fscore_support ( y_true , y_pred ) return r [ 0 ] df = pd . read_csv ( 'titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 5 ) model = LogisticRegression () model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) print ( model . score ( X_test , y_test )) print ( sensitivity_score ( y_test , y_pred )) print ( specificity_score ( y_test , y_pred )) 0.8333333333333334 0.6829268292682927 0.9214285714285714 Adjusting the Logistic Regression Threshold in Sklearn When you use scikit-learn\u2019s predict method, you are given 0 and 1 values of the prediction. However, behind the scenes the Logistic Regression model is getting a probability value between 0 and 1 for each datapoint and then rounding to either 0 or 1. If we want to choose a different threshold besides 0.5, we\u2019ll want those probability values. We can use the predict_proba function to get them. (model.predict_proba(X_test) The result is a numpy array with 2 values for each datapoint (e.g. [0.78, 0.22]). You\u2019ll notice that the two values sum to 1. The first value is the probability that the datapoint is in the 0 class (didn\u2019t survive) and the second is the probability that the datapoint is in the 1 class (survived). We only need the second column of this result, which we can pull with the following numpy syntax. model.predict_proba(X_test)[:, 1] import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import recall_score , precision_recall_fscore_support sensitivity_score = recall_score def specificity_score ( y_true , y_pred ): p , r , f , s = precision_recall_fscore_support ( y_true , y_pred ) return r [ 0 ] df = pd . read_csv ( 'titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 5 ) model = LogisticRegression () model . fit ( X_train , y_train ) y_pred = model . predict ( X_test ) #A threshold of 0.75 means we need to be more confident in order to make a positive prediction. #This results in fewer positive predictions and more negative predictions. #Setting the threshold to 0.5 we would get #the original Logistic Regression model. #Any other threshold value yields an alternative model. y_pred = model . predict_proba ( X_test )[:, 1 ] > 0.75 print ( model . score ( X_test , y_test )) print ( sensitivity_score ( y_test , y_pred )) print ( specificity_score ( y_test , y_pred )) 0.8333333333333334 0.43902439024390244 0.9785714285714285","title":"Training and Testing"},{"location":"ML/pyLogReg/#roc-curve","text":"The ROC curve is a graph of the specificity vs the sensitivity. We build a Logistic Regression model and then calculate the specificity and sensitivity for every possible threshold. Every predicted probability is a threshold. Note that we actually plot the sensitivity vs (1-specificity). There is no strong reason for doing it this way besides that it\u2019s the standard. Scikit-learn has a roc_curve function we can use. The function takes the true target values and the predicted probabilities from our model. We first use the predict_proba method on the model to get the probabilities. Then we call the roc_curve function. The roc_curve function returns an array of the false positive rates, an array of the true positive rates and the thresholds. The false positive rate is 1-specificity (x-axis) and the true positive rate is another term for the sensitivity (y-axis). The threshold values won\u2019t be needed in the graph. Here\u2019s the code for plotting the ROC curve in matplotlib. Note that we also have code for plotting a diagonal line. This can help us visually see how far our model is from a model that predicts randomly. As we don\u2019t use the threshold values to build the graph, the graph does not tell us what threshold would yield each of the possible models. import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import recall_score , precision_recall_fscore_support , roc_curve import matplotlib.pyplot as plt sensitivity_score = recall_score def specificity_score ( y_true , y_pred ): p , r , f , s = precision_recall_fscore_support ( y_true , y_pred ) return r [ 0 ] df = pd . read_csv ( 'titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 5 ) model = LogisticRegression () model . fit ( X_train , y_train ) y_pred_proba = model . predict_proba ( X_test ) fpr , tpr , thresholds = roc_curve ( y_test , y_pred_proba [:, 1 ]) plt . plot ( fpr , tpr ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], linestyle = '--' ) plt . xlim ([ 0.0 , 1.0 ]) plt . ylim ([ 0.0 , 1.0 ]) plt . xlabel ( '1 - specificity' ) plt . ylabel ( 'sensitivity' ) plt . show () The ROC curve is showing the performance, not of a single model, but of many models. Each choice of threshold is a different model. Each point A, B & C refers to a model with a different threshold. Model A has a sensitivity of 0.6 and a specificity of 0.9 (recall that the graph is showing 1-specificity). Model B has a sensitivity of 0.8 and a specificity of 0.7. Model C has a sensitivity of 0.9 and a specificity of 0.5. How to choose between these models will depend on the specifics of our situation. The closer the curve gets to the upper left corner, the better the performance. The line should never fall below the diagonal line as that would mean it performs worse than a random model. If we are in a situation where it\u2019s more important that all of our positive predictions are correct than that we catch all the positive cases (meaning that we predict most of the negative cases correctly), we should choose the model with higher specificity (model A). If we are in a situation where it\u2019s important that we catch as many of the positive cases as possible, we should choose the model with the higher sensitivity (model C). If we want a balance between sensitivity and specificity, we should choose model B. The Area Under the Curve, the AUC,is a value between 0 and 1, the higher the better. Since the ROC is a graph of all the different Logistic Regression models with different thresholds, the AUC does not measure the performance of a single model. It gives a general sense of how well the Logistic Regression model is performing. To get a single model, you still need to find the optimal threshold for your problem. It\u2019s important to note that this metric tells us how well in general a Logistic Regression model performs on our data. As an ROC curve shows the performance of multiple models, the AUC is not measuring the performance of a single model. import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score df = pd . read_csv ( 'titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 5 ) model1 = LogisticRegression () model1 . fit ( X_train , y_train ) y_pred_proba1 = model1 . predict_proba ( X_test ) print ( \"Model 1 AUC score: \" , roc_auc_score ( y_test , y_pred_proba1 [:, 1 ])) #model2 with just Pclass and male features. model2 = LogisticRegression () model2 . fit ( X_train [:, 0 : 2 ], y_train ) y_pred_proba2 = model2 . predict_proba ( X_test [:, 0 : 2 ]) print ( \"Model 2 AUC score: \" , roc_auc_score ( y_test , y_pred_proba2 [:, 1 ])) Model 1 AUC score: 0.8572299651567944 Model 2 AUC score: 0.8390679442508711","title":"ROC Curve"},{"location":"ML/pyLogReg/#k-fold-cross-validation","text":"Instead of doing a single train/test split, we\u2019ll split our data into a training set and test set multiple times. You will only see values this different when you have a small dataset. With large datasets we often just do a training and test set for simplicity. This process for creating multiple training and test sets is called k-fold cross validation. The k is the number of chunks we split our dataset into. The standard number is 5. Our goal in cross validation is to get accurate measures for our metrics (accuracy, precision, recall). We are building extra models in order to feel confident in the numbers we calculate and report. These 5 models were built just for evaluation purposes, so that we can report the metric values. We don\u2019t actually need these models and want to build the best possible model. The best possible model is going to be a model that uses all of the data. So we keep track of our calculated values for our evaluation metrics and then build a model using all of the data. from sklearn.model_selection import KFold import pandas as pd df = pd . read_csv ( 'titanic.csv' ) X = df [[ 'Age' , 'Fare' ]] . values [: 6 ] y = df [ 'Survived' ] . values [: 6 ] kf = KFold ( n_splits = 3 , shuffle = True ) for train , test in kf . split ( X ): print ( train , test ) #3 training and testing sets as expected [0 2 3 4] [1 5] [1 2 4 5] [0 3] [0 1 3 5] [2 4] from sklearn.model_selection import KFold import pandas as pd df = pd . read_csv ( 'titanic.csv' ) X = df [[ 'Age' , 'Fare' ]] . values [: 6 ] y = df [ 'Survived' ] . values [: 6 ] kf = KFold ( n_splits = 3 , shuffle = True ) splits = list ( kf . split ( X )) first_split = splits [ 0 ] train_indices , test_indices = first_split print ( \"training set indices\" , train_indices ) print ( \"test set indices\" , test_indices ) X_train = X [ train_indices ] X_test = X [ test_indices ] y_train = y [ train_indices ] y_test = y [ test_indices ] print ( \"X train\" ) print ( X_train ) print ( \"y_train\" , y_train ) print ( \"X test\" ) print ( X_test ) print ( \"y_test\" , y_test ) training set indices [0 1 3 5] test set indices [2 4] X train [[22. 7.25 ] [38. 71.2833] [35. 53.1 ] [27. 8.4583]] y_train [0 1 1 0] X test [[26. 7.925] [35. 8.05 ]] y_test [1 0] from sklearn.model_selection import KFold import numpy as np from sklearn.linear_model import LogisticRegression import pandas as pd df = pd . read_csv ( 'titanic.csv' ) df [ 'male' ] = df [ 'Sex' ] == 'male' X = df [[ 'Pclass' , 'male' , 'Age' , 'Siblings/Spouses' , 'Parents/Children' , 'Fare' ]] . values y = df [ 'Survived' ] . values scores = [] kf = KFold ( n_splits = 5 , shuffle = True ) for train_index , test_index in kf . split ( X ): X_train , X_test = X [ train_indices ], X [ test_indices ] y_train , y_test = y [ train_indices ], y [ test_indices ] model = LogisticRegression () model . fit ( X_train , y_train ) scores . append ( model . score ( X_test , y_test )) print ( scores ) print ( np . mean ( scores )) [0.7584269662921348, 0.7584269662921348, 0.7584269662921348, 0.7584269662921348, 0.7584269662921348] 0.7584269662921348","title":"K-fold Cross Validation"},{"location":"ML/pyNN/","text":"Neural networks often work well without you needing to use domain knowledge to do any feature engineering. There are three commonly used activation functions: Sigmoid Tanh ReLU Tanh has a similar form to sigmoid, though ranges from -1 to 1 instead of 0 to 1. Tanh is the hyperbolic tan function. ReLU stands for Rectified Linear Unit. It is the identity function for positive numbers and sends negative numbers to 0. Any of these activation functions will work well. Which one to use will depend on specifics of our data. In practice, we figure out which one to use by comparing the performance of different neural networks. Multi-Layer Perceptron Multi-Layer Perceptron (MLP) are feed forward neural networks which means that the neurons only send signals in one direction. A single-layer perceptron is a neural network without any hidden layers. These are rarely used. Most neural networks are multi-layer perceptrons, generally with one or two hidden layers. In order to train a neural network, we need to define a loss function . This is a measure of how far off our neural network is from being perfect. When we train the neural network, we are optimizing a loss function. We will use cross entropy as our loss function. This is the same as the likelihood we used in logistic regression but is called by a different name in this context. Just like we did with the likelihood function in logistic regression, we use the loss function to find the best possible model. Backpropagation A neural network has a lot of parameters that we can control. There are several coefficients for each node and there can be a lot of nodes. The neural network works backwards from the output node iteratively updating the coefficients of the nodes. This process of moving backwards through the neural network is called backpropagation or backprop. Before we create a neural network we fix the number of nodes and number of layers. Then we use backprop to iteratively update all the coefficient values until we converge on an optimal neural network. ''' Scikit-learn has a couple other functions besides make_classification for making classification datasets with different properties. Look at make_circles and make_moons if you want to play around with more artificial datasets. the make_classification function in scikit-learn. It generates a feature matrix X and target array y. We will give it these parameters: \u2022 n_samples: number of datapoints \u2022 n_features: number of features \u2022 n_informative: number of informative features \u2022 n_redundant: number of redundant features \u2022 random_state: random state to guarantee same result every time ''' from sklearn.datasets import make_classification X , y = make_classification ( n_features = 2 , n_redundant = 0 , n_informative = 2 , random_state = 3 ) from matplotlib import pyplot as plt plt . scatter ( X [ y == 0 ][:, 0 ], X [ y == 0 ][:, 1 ], s = 100 , edgecolors = 'k' ) plt . scatter ( X [ y == 1 ][:, 0 ], X [ y == 1 ][:, 1 ], s = 100 , edgecolors = 'k' , marker = '^' ) plt . show () from sklearn.model_selection import train_test_split from sklearn.neural_network import MLPClassifier X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 3 ) mlp = MLPClassifier ( max_iter = 1000 , hidden_layer_sizes = ( 100 , 50 ), alpha = 0.0001 , solver = 'adam' , random_state = 3 ) mlp . fit ( X_train , y_train ) print ( \"Accuracy: \" , mlp . score ( X_test , y_test )) Accuracy: 0.76 MNIST from sklearn.datasets import load_digits X , y = load_digits ( n_class = 2 , return_X_y = True ) print ( X . shape , y . shape ) print ( X [ 0 ] . reshape ( 8 , 8 )) print ( y [ 0 ]) import matplotlib.pyplot as plt from sklearn.datasets import load_digits X , y = load_digits ( n_class = 2 , return_X_y = True ) plt . matshow ( X [ 0 ] . reshape ( 8 , 8 ), cmap = plt . cm . gray ) plt . xticks (()) # remove x tick marks plt . yticks (()) # remove y tick marks plt . show () (360, 64) (360,) [[ 0. 0. 5. 13. 9. 1. 0. 0.] [ 0. 0. 13. 15. 10. 15. 5. 0.] [ 0. 3. 15. 2. 0. 11. 8. 0.] [ 0. 4. 12. 0. 0. 8. 8. 0.] [ 0. 5. 8. 0. 0. 9. 8. 0.] [ 0. 4. 11. 0. 1. 12. 7. 0.] [ 0. 2. 14. 5. 10. 12. 0. 0.] [ 0. 0. 6. 13. 10. 0. 0. 0.]] 0 X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 2 ) mlp = MLPClassifier () mlp . fit ( X_train , y_train ) x = X_test [ 0 ] plt . matshow ( x . reshape ( 8 , 8 ), cmap = plt . cm . gray ) plt . xticks (()) plt . yticks (()) plt . show () print ( mlp . predict ([ x ])) # 0 print ( mlp . score ( X_test , y_test )) #100% accuracy [0] 1.0 All numbers from sklearn.datasets import load_digits X , y = load_digits ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 2 ) mlp = MLPClassifier ( random_state = 2 ) mlp . fit ( X_train , y_train ) print ( mlp . score ( X_test , y_test )) ##The incorrect y_pred = mlp . predict ( X_test ) incorrect = X_test [ y_pred != y_test ] incorrect_true = y_test [ y_pred != y_test ] incorrect_pred = y_pred [ y_pred != y_test ] j = 0 plt . matshow ( incorrect [ j ] . reshape ( 8 , 8 ), cmap = plt . cm . gray ) plt . xticks (()) plt . yticks (()) plt . show () print ( \"true value:\" , incorrect_true [ j ]) print ( \"predicted value:\" , incorrect_pred [ j ]) 0.96 true value: 4 predicted value: 9 OpenML dataset from sklearn.datasets import fetch_openml X , y = fetch_openml ( 'mnist_784' , version = 1 , return_X_y = True ) import numpy as np print ( X . shape , y . shape ) print ( np . min ( X ), np . max ( X )) print ( y [ 0 : 5 ]) #using only the digits 0-3 X5 = X [ y <= '3' ] y5 = y [ y <= '3' ] mlp = MLPClassifier ( hidden_layer_sizes = ( 6 ,), max_iter = 200 , alpha = 1e-4 , solver = 'sgd' , random_state = 2 ) mlp . fit ( X5 , y5 ) (70000, 784) (70000,) 0.0 255.0 ['5' '0' '4' '1' '9'] MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=(6,), learning_rate='constant', learning_rate_init=0.001, max_fun=15000, max_iter=200, momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5, random_state=2, shuffle=True, solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False print ( len ( mlp . coefs_ )) 2 The two elements in the list correspond to the two layers: the hidden layer and the output layer. We have an array of coefficients for each of these layers. Let\u2019s look at the shape of the coefficients for the hidden layer. print ( mlp . coefs_ [ 0 ] . shape ) (784, 6) We see that we have a 2-dimensional array of size 784 x 6. There are 6 nodes and 784 input values feeding into each node, and we have a weight for each of these connections. Visualizing the Hidden Layer fig , axes = plt . subplots ( 2 , 3 , figsize = ( 5 , 4 )) for i , ax in enumerate ( axes . ravel ()): coef = mlp . coefs_ [ 0 ][:, i ] ax . matshow ( coef . reshape ( 28 , 28 ), cmap = plt . cm . gray ) ax . set_xticks (()) ax . set_yticks (()) ax . set_title ( i + 1 ) plt . show () You can see that nodes 4 and 6 are determining if the digit is a 3. Node 1 is determining if the digit is a 0 or a 2 since you can see both of those values in the image. Not every hidden node will have an obvious use Interpretability While we can visualize the nodes in the hidden layer to understand on a high level what the neural network is doing, it is impossible to answer the question \"Why did datapoint x get prediction y?\" Since there are so many nodes, each with their own coefficients, it is not feasible to get a simple explanation of what the neural network is doing. This makes it a difficult model to interpret and use in certain business use cases. Computation Neural networks can take a decent amount of time to train. Each node has its own coefficients and to train they are iteratively updated, so this can be time consuming. However, they are parallelizable, so it is possible to throw computer power at them to make them train faster. Once they are built, neural networks are not slow to make predictions, however, they are not as fast as some of the other models. Performance The main draw to neural networks is their performance. On many problems, their performance simply cannot be beat by other models. They can take some tuning of parameters to find the optimal performance, but they benefit from needing minimal feature engineering prior to building the model. A lot of simpler problems, you can achieve equivalent performance with a simpler model like logistic regression, but with large unstructured datasets, neural networks outperform other models.","title":"Neural Networks"},{"location":"ML/pyNN/#multi-layer-perceptron","text":"Multi-Layer Perceptron (MLP) are feed forward neural networks which means that the neurons only send signals in one direction. A single-layer perceptron is a neural network without any hidden layers. These are rarely used. Most neural networks are multi-layer perceptrons, generally with one or two hidden layers. In order to train a neural network, we need to define a loss function . This is a measure of how far off our neural network is from being perfect. When we train the neural network, we are optimizing a loss function. We will use cross entropy as our loss function. This is the same as the likelihood we used in logistic regression but is called by a different name in this context. Just like we did with the likelihood function in logistic regression, we use the loss function to find the best possible model.","title":"Multi-Layer Perceptron"},{"location":"ML/pyNN/#backpropagation","text":"A neural network has a lot of parameters that we can control. There are several coefficients for each node and there can be a lot of nodes. The neural network works backwards from the output node iteratively updating the coefficients of the nodes. This process of moving backwards through the neural network is called backpropagation or backprop. Before we create a neural network we fix the number of nodes and number of layers. Then we use backprop to iteratively update all the coefficient values until we converge on an optimal neural network. ''' Scikit-learn has a couple other functions besides make_classification for making classification datasets with different properties. Look at make_circles and make_moons if you want to play around with more artificial datasets. the make_classification function in scikit-learn. It generates a feature matrix X and target array y. We will give it these parameters: \u2022 n_samples: number of datapoints \u2022 n_features: number of features \u2022 n_informative: number of informative features \u2022 n_redundant: number of redundant features \u2022 random_state: random state to guarantee same result every time ''' from sklearn.datasets import make_classification X , y = make_classification ( n_features = 2 , n_redundant = 0 , n_informative = 2 , random_state = 3 ) from matplotlib import pyplot as plt plt . scatter ( X [ y == 0 ][:, 0 ], X [ y == 0 ][:, 1 ], s = 100 , edgecolors = 'k' ) plt . scatter ( X [ y == 1 ][:, 0 ], X [ y == 1 ][:, 1 ], s = 100 , edgecolors = 'k' , marker = '^' ) plt . show () from sklearn.model_selection import train_test_split from sklearn.neural_network import MLPClassifier X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 3 ) mlp = MLPClassifier ( max_iter = 1000 , hidden_layer_sizes = ( 100 , 50 ), alpha = 0.0001 , solver = 'adam' , random_state = 3 ) mlp . fit ( X_train , y_train ) print ( \"Accuracy: \" , mlp . score ( X_test , y_test )) Accuracy: 0.76","title":"Backpropagation"},{"location":"ML/pyNN/#mnist","text":"from sklearn.datasets import load_digits X , y = load_digits ( n_class = 2 , return_X_y = True ) print ( X . shape , y . shape ) print ( X [ 0 ] . reshape ( 8 , 8 )) print ( y [ 0 ]) import matplotlib.pyplot as plt from sklearn.datasets import load_digits X , y = load_digits ( n_class = 2 , return_X_y = True ) plt . matshow ( X [ 0 ] . reshape ( 8 , 8 ), cmap = plt . cm . gray ) plt . xticks (()) # remove x tick marks plt . yticks (()) # remove y tick marks plt . show () (360, 64) (360,) [[ 0. 0. 5. 13. 9. 1. 0. 0.] [ 0. 0. 13. 15. 10. 15. 5. 0.] [ 0. 3. 15. 2. 0. 11. 8. 0.] [ 0. 4. 12. 0. 0. 8. 8. 0.] [ 0. 5. 8. 0. 0. 9. 8. 0.] [ 0. 4. 11. 0. 1. 12. 7. 0.] [ 0. 2. 14. 5. 10. 12. 0. 0.] [ 0. 0. 6. 13. 10. 0. 0. 0.]] 0 X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 2 ) mlp = MLPClassifier () mlp . fit ( X_train , y_train ) x = X_test [ 0 ] plt . matshow ( x . reshape ( 8 , 8 ), cmap = plt . cm . gray ) plt . xticks (()) plt . yticks (()) plt . show () print ( mlp . predict ([ x ])) # 0 print ( mlp . score ( X_test , y_test )) #100% accuracy [0] 1.0","title":"MNIST"},{"location":"ML/pyNN/#all-numbers","text":"from sklearn.datasets import load_digits X , y = load_digits ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 2 ) mlp = MLPClassifier ( random_state = 2 ) mlp . fit ( X_train , y_train ) print ( mlp . score ( X_test , y_test )) ##The incorrect y_pred = mlp . predict ( X_test ) incorrect = X_test [ y_pred != y_test ] incorrect_true = y_test [ y_pred != y_test ] incorrect_pred = y_pred [ y_pred != y_test ] j = 0 plt . matshow ( incorrect [ j ] . reshape ( 8 , 8 ), cmap = plt . cm . gray ) plt . xticks (()) plt . yticks (()) plt . show () print ( \"true value:\" , incorrect_true [ j ]) print ( \"predicted value:\" , incorrect_pred [ j ]) 0.96 true value: 4 predicted value: 9","title":"All numbers"},{"location":"ML/pyNN/#openml-dataset","text":"from sklearn.datasets import fetch_openml X , y = fetch_openml ( 'mnist_784' , version = 1 , return_X_y = True ) import numpy as np print ( X . shape , y . shape ) print ( np . min ( X ), np . max ( X )) print ( y [ 0 : 5 ]) #using only the digits 0-3 X5 = X [ y <= '3' ] y5 = y [ y <= '3' ] mlp = MLPClassifier ( hidden_layer_sizes = ( 6 ,), max_iter = 200 , alpha = 1e-4 , solver = 'sgd' , random_state = 2 ) mlp . fit ( X5 , y5 ) (70000, 784) (70000,) 0.0 255.0 ['5' '0' '4' '1' '9'] MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=(6,), learning_rate='constant', learning_rate_init=0.001, max_fun=15000, max_iter=200, momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5, random_state=2, shuffle=True, solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False print ( len ( mlp . coefs_ )) 2 The two elements in the list correspond to the two layers: the hidden layer and the output layer. We have an array of coefficients for each of these layers. Let\u2019s look at the shape of the coefficients for the hidden layer. print ( mlp . coefs_ [ 0 ] . shape ) (784, 6) We see that we have a 2-dimensional array of size 784 x 6. There are 6 nodes and 784 input values feeding into each node, and we have a weight for each of these connections.","title":"OpenML dataset"},{"location":"ML/pyNN/#visualizing-the-hidden-layer","text":"fig , axes = plt . subplots ( 2 , 3 , figsize = ( 5 , 4 )) for i , ax in enumerate ( axes . ravel ()): coef = mlp . coefs_ [ 0 ][:, i ] ax . matshow ( coef . reshape ( 28 , 28 ), cmap = plt . cm . gray ) ax . set_xticks (()) ax . set_yticks (()) ax . set_title ( i + 1 ) plt . show () You can see that nodes 4 and 6 are determining if the digit is a 3. Node 1 is determining if the digit is a 0 or a 2 since you can see both of those values in the image. Not every hidden node will have an obvious use","title":"Visualizing the Hidden Layer"},{"location":"ML/pyNN/#interpretability","text":"While we can visualize the nodes in the hidden layer to understand on a high level what the neural network is doing, it is impossible to answer the question \"Why did datapoint x get prediction y?\" Since there are so many nodes, each with their own coefficients, it is not feasible to get a simple explanation of what the neural network is doing. This makes it a difficult model to interpret and use in certain business use cases.","title":"Interpretability"},{"location":"ML/pyNN/#computation","text":"Neural networks can take a decent amount of time to train. Each node has its own coefficients and to train they are iteratively updated, so this can be time consuming. However, they are parallelizable, so it is possible to throw computer power at them to make them train faster. Once they are built, neural networks are not slow to make predictions, however, they are not as fast as some of the other models.","title":"Computation"},{"location":"ML/pyNN/#performance","text":"The main draw to neural networks is their performance. On many problems, their performance simply cannot be beat by other models. They can take some tuning of parameters to find the optimal performance, but they benefit from needing minimal feature engineering prior to building the model. A lot of simpler problems, you can achieve equivalent performance with a simpler model like logistic regression, but with large unstructured datasets, neural networks outperform other models.","title":"Performance"},{"location":"ML/pyNumba/","text":"Numba is a just-in-time compiler for Python that works best on code that uses NumPy arrays and functions, and loops. The most common way to use Numba is through its collection of decorators that can be applied to your functions to instruct Numba to compile them. When a call is made to a Numba decorated function it is compiled to machine code \u201cjust-in-time\u201d for execution and all or part of your code can subsequently run at native machine code speed! GPU\u2019s have more cores than CPU and hence when it comes to parallel computing of data, GPUs perform exceptionally better than CPU even though GPU has lower clock speed and it lacks several core managements features as compared to the CPU. Thus, running a python script on GPU can prove out to be comparatively faster than CPU, however, it must be noted that for processing a data set with GPU, the data will first be transferred to the GPU\u2019s memory which may require additional time so if data set is small then GPU may perform better than GPU. from numba import jit , cuda import numpy as np # to measure exec time from timeit import default_timer as timer # normal function to run on cpu def func ( a ): for i in range ( 10000000 ): a [ i ] += 1 # function optimized to run on gpu @jit ( target = \"cuda\" ) def func2 ( a ): for i in range ( 10000000 ): a [ i ] += 1 if __name__ == \"__main__\" : n = 10000000 a = np . ones ( n , dtype = np . float64 ) b = np . ones ( n , dtype = np . float32 ) start = timer () func ( a ) print ( \"without GPU:\" , timer () - start ) start = timer () func2 ( a ) print ( \"with GPU:\" , timer () - start )","title":"Numba"},{"location":"ML/pyNumpy/","text":"Numpy (Numerical Python) is a python library that allows fast and easy mathematical operations to be performed on arrays. Numpy is a Python package for manipulating lists and tables of numerical data. We can use it to do a lot of statistical calculations. We call the list or table of data a numpy array. We often will take the data from our pandas DataFrame and put it in numpy arrays. Pandas DataFrames are great because we have the column names and other text data that makes it human readable. A DataFrame, while easy for a human to read, is not the ideal format for doing calculations. The numpy arrays are generally less human readable, but are in a format that enables the necessary computation. Numpy is a Python module for doing calculations on tables of data. Pandas was actually built using Numpy as it\u2019s foundation. Functions and Methods Overview Array Creation arange, array, copy, empty, empty_like, eye, fromfile, fromfunction, identity, linspace, logspace, mgrid, ogrid, ones, ones_like, r_, zeros, zeros_like Conversions ndarray.astype, atleast_1d, atleast_2d, atleast_3d, mat Manipulations array_split, column_stack, concatenate, diagonal, dsplit, dstack, hsplit, hstack, ndarray.item, newaxis, ravel, repeat, reshape, resize, squeeze, swapaxes, take, transpose, vsplit, vstack Questions all, any, nonzero Ordering argmax, argmin, argsort, max, min, ptp, searchsorted, sort, where Operations choose, compress, cumprod, cumsum, inner, ndarray.fill, imag, prod, put, putmask, real, sum Basic Statistics cov, mean, std, var Basic Linear Algebra cross, dot, outer, linalg.svd, vdot import numpy as np data = [ 15 , 16 , 18 , 19 , 22 , 24 , 29 , 30 , 34 ] print ( \"Mean: \" , np . mean ( data )) print ( \"Median: \" , np . median ( data )) print ( \"50th percentile : \" , np . percentile ( data , 50 )) print ( \"75th percentile : \" , np . percentile ( data , 75 )) print ( \"Standard Deviation: \" , np . std ( data )) print ( \"Variance: \" , np . var ( data )) Mean: 23.0 Median: 22.0 50th percentile : 22.0 75th percentile : 29.0 Standard Deviation: 6.342099196813483 Variance: 40.22222222222222 Numpy Array In Python, lists are used to store data. NumPy provides an array structure for performing operations with data. NumPy arrays are faster and more compact than lists. NumPy arrays are homogeneous, meaning they can contain only a single data type, while lists can contain multiple different types of data. To check the data type, use numpy.ndarray.dtype NumPy arrays are ndarrays, which stands for \"N-dimensional array\", because they can have multiple dimensions. However, we can use array as an alias. Array Creation From shape or value np.empty(shape[, dtype, order, like]) returns a new array of given shape and type, without initializing entries. np.ones(shape[, dtype, order, like]) return a new array of given shape and type, filled with ones. np.zeros(shape[, dtype, order, like]) return a new array of given shape and type, filled with zeros. np.full(shape, fill_value[, dtype, order, like]) return a new array of given shape and type, filled with fill_value. From existing data np.array(object[, dtype, copy, order, subok, ...]) Create an array. A NumPy array can be created using the np.array() function, providing it a list as the argument import numpy as np xo = np . array ([ 1 , 2 , 3 , 4 ]) print ( xo [ 0 ]) x = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) print ( x [ 1 ][ 2 ]) 1 6 import numpy as np x = np . array ([ 2 , 4 , 6 ]) # create a rank 1 array A = np . array ([[ 1 , 3 , 5 ], [ 2 , 4 , 6 ]]) # create a rank 2 array B = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) print ( \"Matrix A: \\n \" ) print ( A ) print ( \" \\n Matrix B: \\n \" ) print ( B ) Matrix A: [[1 3 5] [2 4 6]] Matrix B: [[1 2 3] [4 5 6]] np.fromfunction(function, shape, *[, dtype, like]) construct an array by executing a function over each coordinate. def f ( x , y ): return 10 * x + y b = np . fromfunction ( f , ( 5 , 4 ), dtype = int ) print ( b ) [[ 0, 1, 2, 3], [10, 11, 12, 13], [20, 21, 22, 23], [30, 31, 32, 33], [40, 41, 42, 43]] Numerical ranges np.arange([start,] stop[, step,][, dtype, like]) return evenly spaced values within a given interval. np.linspace(start, stop[, num, endpoint, ...]) return evenly spaced numbers over a specified interval. Pandas to Numpy Array We often start with our data in a Pandas DataFrame, but then want to convert it to a numpy array. The values attribute does this for us. However, it is better to use .to_numpy() function. # instead of values attribute df [ 'Fare' ] . values # For series df [[ 'Pclass' , 'Fare' , 'Age' ]] . values # or Dataframes # Use .to_numpy() print ( df [ 'Fare' ] . to_numpy ()) print ( df [[ 'Pclass' , 'Fare' , 'Age' ]] . to_numpy ()) Array Attributes Can be accessed using a dot. ndim returns the number of dimensions of the array. size returns the total number of elements of the array. shape returns a tuple of integers that indicate the number of elements stored along each dimension of the array. Note that once an array is created in numpy, its size cannot be changed. Size tells us how big the array is, shape tells us the dimension. Numpy Shape Attribute We use the numpy shape attribute to determine the size of our numpy array. The size tells us how many rows and columns are in our data. This result means we have 887 rows and 3 columns. Use the shape attribute to find the number of rows and number columns for a Numpy array. You can also use the shape attribute on a pandas DataFrame (df.shape). x = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) print ( x . ndim ) # 2 print ( x . size ) # 9 print ( x . shape ) # (3, 3) 2 9 (3, 3) arr = df [[ 'Pclass' , 'Fare' , 'Age' ]] . values print ( arr . shape ) #(887, 3) (887, 3) Indexing and Slicing Negative indexes count from the end of the array, so, [-3:] will result in the last 3 elements. Numpy slicing syntax follows that of a python list: arr[start:stop:step]. When any of these are unspecified, they default to the values start=0, stop=size of dimension, step=1. NumPy offers more indexing facilities than regular Python sequences. In addition to indexing by integers and slices, as we saw before, arrays can be indexed by arrays of integers and arrays of booleans . # Indexing/Slicing examples print ( A [ 0 , :]) # index the first \"row\" and all columns print ( A [ 1 , 2 ]) # index the second row, third column entry print ( A [:, 1 ]) # index entire second column [1 3 5] 6 [3 4] When fewer indices are provided than the number of axes, the missing indices are considered complete slices. b [ - 1 ] # the last row. Equivalent to b[-1, :] array([40, 41, 42, 43]) NumPy also allows you to write this using dots as b[i, ...]. The dots (...), called ellipsis, represent as many colons as needed to produce a complete indexing tuple. For example, if x is an array with 5 axes, then x[1, 2, ...] is equivalent to x[1, 2, :, :, :], x[..., 3] to x[:, :, :, :, 3] and x[4, ..., 5, :] to x[4, :, :, 5, :]. c = np . array ([[[ 0 , 1 , 2 ], # a 3D array (two stacked 2D arrays) [ 10 , 12 , 13 ]], [[ 100 , 101 , 102 ], [ 110 , 112 , 113 ]]]) #c.shape # (2, 2, 3) c [ 1 , ... ] # same as c[1, :, :] or c[1] array([[100, 101, 102], [110, 112, 113]]) c [ ... , 2 ] # same as c[:, :, 2] array([[ 2, 13], [102, 113]]) You can provide a condition as the index to select the elements that fulfill the given condition. Conditions can be combined using the & (and) and | (or) operators. x = np . arange ( 1 , 10 ) print ( x [ x < 4 ]) [1 2 3] More on slicing with numpy. Mask & Subsetting A mask is a boolean array (True/False values) that tells us which values from the array we\u2019re interested in. Masking is used to extract, modify, count, or otherwise manipulate values in an array based on some criterion. We can create a mask satisfying more than one criteria. We use & to separate the conditions and each condition is encapsulated with parentheses \"()\" mask = arr [:, 2 ] < 18 #all children passengers print ( arr [ mask ]) # or arr[arr[:, 2] < 18] The condition can also be assigned to a variable, which will be an array of boolean values showing whether or not the values in the array fulfill the condition: y = (x>5) & (x%2==0) print ( x [( x > 5 ) & ( x % 2 == 0 )]) y = ( x > 5 ) & ( x % 2 == 0 ) print ( x [ y ]) [6 8] [6 8] We can use operations including \"<\", \">\", \">=\", \"<=\", and \"==\" . To find out how many rows satisfy the condition, use .sum() on the resultant 1d boolean array, e.g., (arr[:, 1] == 10).sum(). True is treated as 1 and False as 0 in the sum. Assigning Values We can use slicing for multiple elements. For example, to replace the first row by 10. arr [ 0 ,:] = 10 We can also combine slicing to change any subset of the array. For example, to reassign 0 to the left upper corner. arr [: 2 ,: 2 ] = 0 Assigning an Array to an Array In addition, a 1darray or a 2darry can be assigned to a subset of another 2darray, as long as their shapes match. arr [:, 0 ] = [ 10 , 1 ] Basic Operations Arithmetic operators on arrays apply elementwise. A new array is created and filled with the result. # Arithmetic Examples C = A * 2 # multiplies every element of A by two - This is called Broadcasting D = A * B # elementwise multiplication rather than matrix multiplication E = np . transpose ( B ) F = np . matmul ( A , E ) # performs matrix multiplication -- could also use np.dot() G = np . matmul ( A , x ) # performs matrix-vector multiplication -- again could also use np.dot() print ( \" \\n Matrix E (the transpose of B): \\n \" ) print ( E ) print ( \" \\n Matrix F (result of matrix multiplication A x E): \\n \" ) print ( F ) print ( \" \\n Matrix G (result of matrix-vector multiplication A*x): \\n \" ) print ( G ) Matrix E (the transpose of B): [[1 4] [2 5] [3 6]] Matrix F (result of matrix multiplication A x E): [[22 49] [28 64]] Matrix G (result of matrix-vector multiplication A*x): [44 56] Unlike in many matrix languages, the product operator * operates elementwise in NumPy arrays. The matrix product can be performed using the @ operator (in python >=3.5) or the dot function or method. A = np . array ([[ 1 , 1 ], [ 0 , 1 ]]) B = np . array ([[ 2 , 0 ], [ 3 , 4 ]]) A * B # elementwise product array([[2, 0], [0, 4]]) A @ B # matrix product, same as A.dot(B) array([[5, 4], [3, 4]]) Some operations, such as += and *=, act in place to modify an existing array rather than create a new one. When operating with arrays of different types, the type of the resulting array corresponds to the more general or precise one (a behavior known as upcasting). Many unary operations, such as computing the sum of all the elements in the array, are implemented as methods of the ndarray class. By default, these operations apply to the array as though it were a list of numbers, regardless of its shape. However, by specifying the axis parameter you can apply an operation along the specified axis of an array. np.min() and np.max() min() and max() can be used to get the smallest and largest elements. # max operation examples X = np . array ([[ 3 , 9 , 4 ], [ 10 , 2 , 7 ], [ 5 , 11 , 8 ]]) all_max = np . max ( X ) # gets the maximum value of matrix X column_max = np . max ( X , axis = 0 ) # gets the maximum in each column -- returns a rank-1 array [10, 11, 8] row_max = np . max ( X , axis = 1 ) # gets the maximum in each row -- returns a rank-1 array [9, 10, 11] # Numpy also has argmax to return indices of maximal values column_argmax = np . argmax ( X , axis = 0 ) # note that the \"index\" here is actually the row the maximum occurs for each column print ( \"Matrix X: \\n \" ) print ( X ) print ( \" \\n Maximum value in X: \\n \" ) print ( all_max ) print ( \" \\n Column-wise max of X: \\n \" ) print ( column_max ) print ( \" \\n Indices of column max: \\n \" ) print ( column_argmax ) print ( \" \\n Row-wise max of X: \\n \" ) print ( row_max ) Matrix X: [[ 3 9 4] [10 2 7] [ 5 11 8]] Maximum value in X: 11 Column-wise max of X: [10 11 8] Indices of column max: [1 2 2] Row-wise max of X: [ 9 10 11] np.sum() & np.mean() These work similarly to the max operations -- use the axis argument to denote if summing over rows or columns # Sum operation examples total_sum = np . sum ( X ) column_sum = np . sum ( X , axis = 0 ) row_sum = np . sum ( X , axis = 1 ) print ( \"Matrix X: \\n \" ) print ( X ) print ( \" \\n Sum over all elements of X: \\n \" ) print ( total_sum ) print ( \" \\n Column-wise sum of X: \\n \" ) print ( column_sum ) print ( \" \\n Row-wise sum of X: \\n \" ) print ( row_sum ) Matrix X: [[ 3 9 4] [10 2 7] [ 5 11 8]] Sum over all elements of X: 59 Column-wise sum of X: [18 22 19] Row-wise sum of X: [16 19 24] Universal Functions NumPy provides familiar mathematical functions such as sin, cos, and exp. In NumPy, these are called \u201cuniversal functions\u201d (ufunc). Within NumPy, these functions operate elementwise on an array, producing an array as output. Methods ufunc.reduce(array[, axis, dtype, out, ...]) Reduces array's dimension by one, by applying ufunc along one axis. np . multiply . reduce ([ 2 , 3 , 5 ]) #30 ufunc.accumulate(array[, axis, dtype, out]) Accumulate the result of applying the operator to all elements. np . add . accumulate ([ 2 , 3 , 5 ]) # array([ 2, 5, 10]) Operations Math: Add, subtract, multiply, divide, matmul, power, mod, absolute, exp, log, sqrt, cbrt, gcd, lcm, etc... Trigonometric functions: sin, cos, tan, arcsin, sinh, deg2rad, etc... Bit-twiddling functions: invert, left_shift, bitwise_and, etc.. Comparison functions: greater, greater_equal, less, not_equal, equal, logical_and, maximum, etc... Floating functions: isfinite, isinf, isnan, floor, ceil, trunc, etc... Array Manipulation Changing Array Shape np.reshape() Gives a new shape to an array without changing its data. np.ravel(a[, order]) Return a contiguous flattened array. np.flatten([order]) Return a copy of the array collapsed into one dimension. When you use the reshape method, the array you want to produce needs to have the same number of elements as the original array. Reshape can also do the opposite: take a 2-dimensional array and make a 1-dimensional array from it. The same result can be achieved using the flatten() function. Numpy can calculate the shape (dimension) for us if we indicate the unknown dimension as -1. For example, given a 2darray arr of shape (3,4), arr.reshape(-1) would output a 1darray of shape (12,), while arr.reshape((-1,2)) would generate a 2darray of shape (6,2). # Matrix reshaping X = np . arange ( 16 ) # makes a rank-1 array of integers from 0 to 15 X_square = np . reshape ( X , ( 4 , 4 )) # reshape X into a 4 x 4 matrix X_rank_3 = np . reshape ( X , ( 2 , 2 , 4 )) # reshape X to be 2 x 2 x 4 --a rank-3 array # consider as two rank-2 arrays with 2 rows and 4 columns print ( \"Rank-1 array X: \\n \" ) print ( X ) print ( \" \\n Reshaped into a square matrix: \\n \" ) print ( X_square ) print ( \" \\n Reshaped into a rank-3 array with dimensions 2 x 2 x 4: \\n \" ) print ( X_rank_3 ) print ( \" \\n Reshaped into Rank 1\" ) print ( X_square . reshape ( 16 )) print ( \"Using Flatten\" ) print ( X_rank_3 . flatten ()) Rank-1 array X: [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Reshaped into a square matrix: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15]] Reshaped into a rank-3 array with dimensions 2 x 2 x 4: [[[ 0 1 2 3] [ 4 5 6 7]] [[ 8 9 10 11] [12 13 14 15]]] Reshaped into Rank 1 [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Using Flatten [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Automatic Reshaping To change the dimensions of an array, you can omit one of the sizes which will then be deduced automatically. a = np . arange ( 30 ) b = a . reshape (( 2 , - 1 , 3 )) # -1 means \"whatever is needed\" b . shape #(2, 5, 3) b array([[[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11], [12, 13, 14]], [[15, 16, 17], [18, 19, 20], [21, 22, 23], [24, 25, 26], [27, 28, 29]]]) Iterating over multidimensional arrays is done with respect to the first axis: for row in b : print ( row ) However, if one wants to perform an operation on each element in the array, one can use the flat attribute which is an iterator over all the elements of the array. for element in b . flat : print ( element ) Transpose np.transpose(a[, axes]) Reverse or permute the axes of an array; returns the modified array. Combining Arrays Oftentime we obtain data stored in different arrays and we need to combine them into one to keep it in one place. We can stack them horizontally (by column) to get a 2darray using 'hstack'. if we want to combine the arrays vertically (by row), we can use 'vstack'. To combine more than two arrays horizontally, simply add the additional arrays into the tuple. arr3 = np . vstack (( arr1 , arr2 )) np.vstack(tup) Stack arrays in sequence vertically (row wise). np.hstack(tup) Stack arrays in sequence horizontally (column wise). np.concatenate([axis, out, dtype, casting]) Join a sequence of arrays along an existing axis. More generally, we can use the function numpy.concatenate. If we want to concatenate, link together, two arrays along rows, then pass 'axis = 1' to achieve the same result as using numpy.hstack; and pass 'axis = 0' if you want to combine arrays vertically. You can use np.hstack to concatenate arrays ONLY if they have the same number of rows. np . concatenate (( arr1 , arr2 ), axis = 1 ) Adding and removing elements np.delete(arr, obj[, axis]) Return a new array with sub-arrays along an axis deleted. np.insert(arr, obj, values[, axis]) Insert values along the given axis before the given indices. np.append(arr, values[, axis]) Append values to the end of an array. We can add, remove and sort an array using the np.append(), np.delete() and np.sort() functions. x = np . array ([ 2 , 1 , 3 ]) #add an element x = np . append ( x , 4 ) #delete an element x = np . delete ( x , 0 ) #sort array x = np . sort ( x ) x = np . arange ( 2 , 8 , 2 ) print ( x ) x = np . append ( x , x . size ) x = np . sort ( x ) print ( x [ 1 ]) [2 4 6] 3 Copies and Views Simple assignments make no copy of objects or their data. a = np . array ([[ 0 , 1 , 2 , 3 ], [ 4 , 5 , 6 , 7 ], [ 8 , 9 , 10 , 11 ]]) b = a # no new object is created b is a # a and b are two names for the same ndarray object True View or Shallow Copy Different array objects can share the same data. The view method creates a new array object that looks at the same data. c = a . view () c is a #False c . base is a # True, c is a view of the data owned by a c . flags . owndata #False c = c . reshape (( 2 , 6 )) # a's shape doesn't change c [ 0 , 4 ] = 1234 # a's data changes Slicing an array returns a view of it. s = a [:, 1 : 3 ] s [:] = 10 # s[:] is a view of s. # Note the difference between s = 10 and s[:] = 10 Deep Copy The copy method makes a complete copy of the array and its data. d = a . copy () # a new array object with new data is created d is a # False d . base is a # False, d doesn't share anything with a","title":"Numpy"},{"location":"ML/pyNumpy/#functions-and-methods-overview","text":"Array Creation arange, array, copy, empty, empty_like, eye, fromfile, fromfunction, identity, linspace, logspace, mgrid, ogrid, ones, ones_like, r_, zeros, zeros_like Conversions ndarray.astype, atleast_1d, atleast_2d, atleast_3d, mat Manipulations array_split, column_stack, concatenate, diagonal, dsplit, dstack, hsplit, hstack, ndarray.item, newaxis, ravel, repeat, reshape, resize, squeeze, swapaxes, take, transpose, vsplit, vstack Questions all, any, nonzero Ordering argmax, argmin, argsort, max, min, ptp, searchsorted, sort, where Operations choose, compress, cumprod, cumsum, inner, ndarray.fill, imag, prod, put, putmask, real, sum Basic Statistics cov, mean, std, var Basic Linear Algebra cross, dot, outer, linalg.svd, vdot import numpy as np data = [ 15 , 16 , 18 , 19 , 22 , 24 , 29 , 30 , 34 ] print ( \"Mean: \" , np . mean ( data )) print ( \"Median: \" , np . median ( data )) print ( \"50th percentile : \" , np . percentile ( data , 50 )) print ( \"75th percentile : \" , np . percentile ( data , 75 )) print ( \"Standard Deviation: \" , np . std ( data )) print ( \"Variance: \" , np . var ( data )) Mean: 23.0 Median: 22.0 50th percentile : 22.0 75th percentile : 29.0 Standard Deviation: 6.342099196813483 Variance: 40.22222222222222","title":"Functions and Methods Overview"},{"location":"ML/pyNumpy/#numpy-array","text":"In Python, lists are used to store data. NumPy provides an array structure for performing operations with data. NumPy arrays are faster and more compact than lists. NumPy arrays are homogeneous, meaning they can contain only a single data type, while lists can contain multiple different types of data. To check the data type, use numpy.ndarray.dtype NumPy arrays are ndarrays, which stands for \"N-dimensional array\", because they can have multiple dimensions. However, we can use array as an alias.","title":"Numpy Array"},{"location":"ML/pyNumpy/#array-creation","text":"","title":"Array Creation"},{"location":"ML/pyNumpy/#from-shape-or-value","text":"np.empty(shape[, dtype, order, like]) returns a new array of given shape and type, without initializing entries. np.ones(shape[, dtype, order, like]) return a new array of given shape and type, filled with ones. np.zeros(shape[, dtype, order, like]) return a new array of given shape and type, filled with zeros. np.full(shape, fill_value[, dtype, order, like]) return a new array of given shape and type, filled with fill_value.","title":"From shape or value"},{"location":"ML/pyNumpy/#from-existing-data","text":"np.array(object[, dtype, copy, order, subok, ...]) Create an array. A NumPy array can be created using the np.array() function, providing it a list as the argument import numpy as np xo = np . array ([ 1 , 2 , 3 , 4 ]) print ( xo [ 0 ]) x = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) print ( x [ 1 ][ 2 ]) 1 6 import numpy as np x = np . array ([ 2 , 4 , 6 ]) # create a rank 1 array A = np . array ([[ 1 , 3 , 5 ], [ 2 , 4 , 6 ]]) # create a rank 2 array B = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) print ( \"Matrix A: \\n \" ) print ( A ) print ( \" \\n Matrix B: \\n \" ) print ( B ) Matrix A: [[1 3 5] [2 4 6]] Matrix B: [[1 2 3] [4 5 6]] np.fromfunction(function, shape, *[, dtype, like]) construct an array by executing a function over each coordinate. def f ( x , y ): return 10 * x + y b = np . fromfunction ( f , ( 5 , 4 ), dtype = int ) print ( b ) [[ 0, 1, 2, 3], [10, 11, 12, 13], [20, 21, 22, 23], [30, 31, 32, 33], [40, 41, 42, 43]]","title":"From existing data"},{"location":"ML/pyNumpy/#numerical-ranges","text":"np.arange([start,] stop[, step,][, dtype, like]) return evenly spaced values within a given interval. np.linspace(start, stop[, num, endpoint, ...]) return evenly spaced numbers over a specified interval.","title":"Numerical ranges"},{"location":"ML/pyNumpy/#pandas-to-numpy-array","text":"We often start with our data in a Pandas DataFrame, but then want to convert it to a numpy array. The values attribute does this for us. However, it is better to use .to_numpy() function. # instead of values attribute df [ 'Fare' ] . values # For series df [[ 'Pclass' , 'Fare' , 'Age' ]] . values # or Dataframes # Use .to_numpy() print ( df [ 'Fare' ] . to_numpy ()) print ( df [[ 'Pclass' , 'Fare' , 'Age' ]] . to_numpy ())","title":"Pandas to Numpy Array"},{"location":"ML/pyNumpy/#array-attributes","text":"Can be accessed using a dot. ndim returns the number of dimensions of the array. size returns the total number of elements of the array. shape returns a tuple of integers that indicate the number of elements stored along each dimension of the array. Note that once an array is created in numpy, its size cannot be changed. Size tells us how big the array is, shape tells us the dimension. Numpy Shape Attribute We use the numpy shape attribute to determine the size of our numpy array. The size tells us how many rows and columns are in our data. This result means we have 887 rows and 3 columns. Use the shape attribute to find the number of rows and number columns for a Numpy array. You can also use the shape attribute on a pandas DataFrame (df.shape). x = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) print ( x . ndim ) # 2 print ( x . size ) # 9 print ( x . shape ) # (3, 3) 2 9 (3, 3) arr = df [[ 'Pclass' , 'Fare' , 'Age' ]] . values print ( arr . shape ) #(887, 3) (887, 3)","title":"Array Attributes"},{"location":"ML/pyNumpy/#indexing-and-slicing","text":"Negative indexes count from the end of the array, so, [-3:] will result in the last 3 elements. Numpy slicing syntax follows that of a python list: arr[start:stop:step]. When any of these are unspecified, they default to the values start=0, stop=size of dimension, step=1. NumPy offers more indexing facilities than regular Python sequences. In addition to indexing by integers and slices, as we saw before, arrays can be indexed by arrays of integers and arrays of booleans . # Indexing/Slicing examples print ( A [ 0 , :]) # index the first \"row\" and all columns print ( A [ 1 , 2 ]) # index the second row, third column entry print ( A [:, 1 ]) # index entire second column [1 3 5] 6 [3 4] When fewer indices are provided than the number of axes, the missing indices are considered complete slices. b [ - 1 ] # the last row. Equivalent to b[-1, :] array([40, 41, 42, 43]) NumPy also allows you to write this using dots as b[i, ...]. The dots (...), called ellipsis, represent as many colons as needed to produce a complete indexing tuple. For example, if x is an array with 5 axes, then x[1, 2, ...] is equivalent to x[1, 2, :, :, :], x[..., 3] to x[:, :, :, :, 3] and x[4, ..., 5, :] to x[4, :, :, 5, :]. c = np . array ([[[ 0 , 1 , 2 ], # a 3D array (two stacked 2D arrays) [ 10 , 12 , 13 ]], [[ 100 , 101 , 102 ], [ 110 , 112 , 113 ]]]) #c.shape # (2, 2, 3) c [ 1 , ... ] # same as c[1, :, :] or c[1] array([[100, 101, 102], [110, 112, 113]]) c [ ... , 2 ] # same as c[:, :, 2] array([[ 2, 13], [102, 113]]) You can provide a condition as the index to select the elements that fulfill the given condition. Conditions can be combined using the & (and) and | (or) operators. x = np . arange ( 1 , 10 ) print ( x [ x < 4 ]) [1 2 3] More on slicing with numpy.","title":"Indexing and Slicing"},{"location":"ML/pyNumpy/#mask-subsetting","text":"A mask is a boolean array (True/False values) that tells us which values from the array we\u2019re interested in. Masking is used to extract, modify, count, or otherwise manipulate values in an array based on some criterion. We can create a mask satisfying more than one criteria. We use & to separate the conditions and each condition is encapsulated with parentheses \"()\" mask = arr [:, 2 ] < 18 #all children passengers print ( arr [ mask ]) # or arr[arr[:, 2] < 18] The condition can also be assigned to a variable, which will be an array of boolean values showing whether or not the values in the array fulfill the condition: y = (x>5) & (x%2==0) print ( x [( x > 5 ) & ( x % 2 == 0 )]) y = ( x > 5 ) & ( x % 2 == 0 ) print ( x [ y ]) [6 8] [6 8] We can use operations including \"<\", \">\", \">=\", \"<=\", and \"==\" . To find out how many rows satisfy the condition, use .sum() on the resultant 1d boolean array, e.g., (arr[:, 1] == 10).sum(). True is treated as 1 and False as 0 in the sum.","title":"Mask &amp; Subsetting"},{"location":"ML/pyNumpy/#assigning-values","text":"We can use slicing for multiple elements. For example, to replace the first row by 10. arr [ 0 ,:] = 10 We can also combine slicing to change any subset of the array. For example, to reassign 0 to the left upper corner. arr [: 2 ,: 2 ] = 0","title":"Assigning Values"},{"location":"ML/pyNumpy/#assigning-an-array-to-an-array","text":"In addition, a 1darray or a 2darry can be assigned to a subset of another 2darray, as long as their shapes match. arr [:, 0 ] = [ 10 , 1 ]","title":"Assigning an Array to an Array"},{"location":"ML/pyNumpy/#basic-operations","text":"Arithmetic operators on arrays apply elementwise. A new array is created and filled with the result. # Arithmetic Examples C = A * 2 # multiplies every element of A by two - This is called Broadcasting D = A * B # elementwise multiplication rather than matrix multiplication E = np . transpose ( B ) F = np . matmul ( A , E ) # performs matrix multiplication -- could also use np.dot() G = np . matmul ( A , x ) # performs matrix-vector multiplication -- again could also use np.dot() print ( \" \\n Matrix E (the transpose of B): \\n \" ) print ( E ) print ( \" \\n Matrix F (result of matrix multiplication A x E): \\n \" ) print ( F ) print ( \" \\n Matrix G (result of matrix-vector multiplication A*x): \\n \" ) print ( G ) Matrix E (the transpose of B): [[1 4] [2 5] [3 6]] Matrix F (result of matrix multiplication A x E): [[22 49] [28 64]] Matrix G (result of matrix-vector multiplication A*x): [44 56] Unlike in many matrix languages, the product operator * operates elementwise in NumPy arrays. The matrix product can be performed using the @ operator (in python >=3.5) or the dot function or method. A = np . array ([[ 1 , 1 ], [ 0 , 1 ]]) B = np . array ([[ 2 , 0 ], [ 3 , 4 ]]) A * B # elementwise product array([[2, 0], [0, 4]]) A @ B # matrix product, same as A.dot(B) array([[5, 4], [3, 4]]) Some operations, such as += and *=, act in place to modify an existing array rather than create a new one. When operating with arrays of different types, the type of the resulting array corresponds to the more general or precise one (a behavior known as upcasting). Many unary operations, such as computing the sum of all the elements in the array, are implemented as methods of the ndarray class. By default, these operations apply to the array as though it were a list of numbers, regardless of its shape. However, by specifying the axis parameter you can apply an operation along the specified axis of an array.","title":"Basic Operations"},{"location":"ML/pyNumpy/#npmin-and-npmax","text":"min() and max() can be used to get the smallest and largest elements. # max operation examples X = np . array ([[ 3 , 9 , 4 ], [ 10 , 2 , 7 ], [ 5 , 11 , 8 ]]) all_max = np . max ( X ) # gets the maximum value of matrix X column_max = np . max ( X , axis = 0 ) # gets the maximum in each column -- returns a rank-1 array [10, 11, 8] row_max = np . max ( X , axis = 1 ) # gets the maximum in each row -- returns a rank-1 array [9, 10, 11] # Numpy also has argmax to return indices of maximal values column_argmax = np . argmax ( X , axis = 0 ) # note that the \"index\" here is actually the row the maximum occurs for each column print ( \"Matrix X: \\n \" ) print ( X ) print ( \" \\n Maximum value in X: \\n \" ) print ( all_max ) print ( \" \\n Column-wise max of X: \\n \" ) print ( column_max ) print ( \" \\n Indices of column max: \\n \" ) print ( column_argmax ) print ( \" \\n Row-wise max of X: \\n \" ) print ( row_max ) Matrix X: [[ 3 9 4] [10 2 7] [ 5 11 8]] Maximum value in X: 11 Column-wise max of X: [10 11 8] Indices of column max: [1 2 2] Row-wise max of X: [ 9 10 11]","title":"np.min() and np.max()"},{"location":"ML/pyNumpy/#npsum-npmean","text":"These work similarly to the max operations -- use the axis argument to denote if summing over rows or columns # Sum operation examples total_sum = np . sum ( X ) column_sum = np . sum ( X , axis = 0 ) row_sum = np . sum ( X , axis = 1 ) print ( \"Matrix X: \\n \" ) print ( X ) print ( \" \\n Sum over all elements of X: \\n \" ) print ( total_sum ) print ( \" \\n Column-wise sum of X: \\n \" ) print ( column_sum ) print ( \" \\n Row-wise sum of X: \\n \" ) print ( row_sum ) Matrix X: [[ 3 9 4] [10 2 7] [ 5 11 8]] Sum over all elements of X: 59 Column-wise sum of X: [18 22 19] Row-wise sum of X: [16 19 24]","title":"np.sum() &amp; np.mean()"},{"location":"ML/pyNumpy/#universal-functions","text":"NumPy provides familiar mathematical functions such as sin, cos, and exp. In NumPy, these are called \u201cuniversal functions\u201d (ufunc). Within NumPy, these functions operate elementwise on an array, producing an array as output.","title":"Universal Functions"},{"location":"ML/pyNumpy/#methods","text":"ufunc.reduce(array[, axis, dtype, out, ...]) Reduces array's dimension by one, by applying ufunc along one axis. np . multiply . reduce ([ 2 , 3 , 5 ]) #30 ufunc.accumulate(array[, axis, dtype, out]) Accumulate the result of applying the operator to all elements. np . add . accumulate ([ 2 , 3 , 5 ]) # array([ 2, 5, 10])","title":"Methods"},{"location":"ML/pyNumpy/#operations","text":"Math: Add, subtract, multiply, divide, matmul, power, mod, absolute, exp, log, sqrt, cbrt, gcd, lcm, etc... Trigonometric functions: sin, cos, tan, arcsin, sinh, deg2rad, etc... Bit-twiddling functions: invert, left_shift, bitwise_and, etc.. Comparison functions: greater, greater_equal, less, not_equal, equal, logical_and, maximum, etc... Floating functions: isfinite, isinf, isnan, floor, ceil, trunc, etc...","title":"Operations"},{"location":"ML/pyNumpy/#array-manipulation","text":"","title":"Array Manipulation"},{"location":"ML/pyNumpy/#changing-array-shape","text":"np.reshape() Gives a new shape to an array without changing its data. np.ravel(a[, order]) Return a contiguous flattened array. np.flatten([order]) Return a copy of the array collapsed into one dimension. When you use the reshape method, the array you want to produce needs to have the same number of elements as the original array. Reshape can also do the opposite: take a 2-dimensional array and make a 1-dimensional array from it. The same result can be achieved using the flatten() function. Numpy can calculate the shape (dimension) for us if we indicate the unknown dimension as -1. For example, given a 2darray arr of shape (3,4), arr.reshape(-1) would output a 1darray of shape (12,), while arr.reshape((-1,2)) would generate a 2darray of shape (6,2). # Matrix reshaping X = np . arange ( 16 ) # makes a rank-1 array of integers from 0 to 15 X_square = np . reshape ( X , ( 4 , 4 )) # reshape X into a 4 x 4 matrix X_rank_3 = np . reshape ( X , ( 2 , 2 , 4 )) # reshape X to be 2 x 2 x 4 --a rank-3 array # consider as two rank-2 arrays with 2 rows and 4 columns print ( \"Rank-1 array X: \\n \" ) print ( X ) print ( \" \\n Reshaped into a square matrix: \\n \" ) print ( X_square ) print ( \" \\n Reshaped into a rank-3 array with dimensions 2 x 2 x 4: \\n \" ) print ( X_rank_3 ) print ( \" \\n Reshaped into Rank 1\" ) print ( X_square . reshape ( 16 )) print ( \"Using Flatten\" ) print ( X_rank_3 . flatten ()) Rank-1 array X: [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Reshaped into a square matrix: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15]] Reshaped into a rank-3 array with dimensions 2 x 2 x 4: [[[ 0 1 2 3] [ 4 5 6 7]] [[ 8 9 10 11] [12 13 14 15]]] Reshaped into Rank 1 [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Using Flatten [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15] Automatic Reshaping To change the dimensions of an array, you can omit one of the sizes which will then be deduced automatically. a = np . arange ( 30 ) b = a . reshape (( 2 , - 1 , 3 )) # -1 means \"whatever is needed\" b . shape #(2, 5, 3) b array([[[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11], [12, 13, 14]], [[15, 16, 17], [18, 19, 20], [21, 22, 23], [24, 25, 26], [27, 28, 29]]]) Iterating over multidimensional arrays is done with respect to the first axis: for row in b : print ( row ) However, if one wants to perform an operation on each element in the array, one can use the flat attribute which is an iterator over all the elements of the array. for element in b . flat : print ( element )","title":"Changing Array Shape"},{"location":"ML/pyNumpy/#transpose","text":"np.transpose(a[, axes]) Reverse or permute the axes of an array; returns the modified array.","title":"Transpose"},{"location":"ML/pyNumpy/#combining-arrays","text":"Oftentime we obtain data stored in different arrays and we need to combine them into one to keep it in one place. We can stack them horizontally (by column) to get a 2darray using 'hstack'. if we want to combine the arrays vertically (by row), we can use 'vstack'. To combine more than two arrays horizontally, simply add the additional arrays into the tuple. arr3 = np . vstack (( arr1 , arr2 )) np.vstack(tup) Stack arrays in sequence vertically (row wise). np.hstack(tup) Stack arrays in sequence horizontally (column wise). np.concatenate([axis, out, dtype, casting]) Join a sequence of arrays along an existing axis. More generally, we can use the function numpy.concatenate. If we want to concatenate, link together, two arrays along rows, then pass 'axis = 1' to achieve the same result as using numpy.hstack; and pass 'axis = 0' if you want to combine arrays vertically. You can use np.hstack to concatenate arrays ONLY if they have the same number of rows. np . concatenate (( arr1 , arr2 ), axis = 1 )","title":"Combining  Arrays"},{"location":"ML/pyNumpy/#adding-and-removing-elements","text":"np.delete(arr, obj[, axis]) Return a new array with sub-arrays along an axis deleted. np.insert(arr, obj, values[, axis]) Insert values along the given axis before the given indices. np.append(arr, values[, axis]) Append values to the end of an array. We can add, remove and sort an array using the np.append(), np.delete() and np.sort() functions. x = np . array ([ 2 , 1 , 3 ]) #add an element x = np . append ( x , 4 ) #delete an element x = np . delete ( x , 0 ) #sort array x = np . sort ( x ) x = np . arange ( 2 , 8 , 2 ) print ( x ) x = np . append ( x , x . size ) x = np . sort ( x ) print ( x [ 1 ]) [2 4 6] 3","title":"Adding and removing elements"},{"location":"ML/pyNumpy/#copies-and-views","text":"Simple assignments make no copy of objects or their data. a = np . array ([[ 0 , 1 , 2 , 3 ], [ 4 , 5 , 6 , 7 ], [ 8 , 9 , 10 , 11 ]]) b = a # no new object is created b is a # a and b are two names for the same ndarray object True","title":"Copies and Views"},{"location":"ML/pyNumpy/#view-or-shallow-copy","text":"Different array objects can share the same data. The view method creates a new array object that looks at the same data. c = a . view () c is a #False c . base is a # True, c is a view of the data owned by a c . flags . owndata #False c = c . reshape (( 2 , 6 )) # a's shape doesn't change c [ 0 , 4 ] = 1234 # a's data changes Slicing an array returns a view of it. s = a [:, 1 : 3 ] s [:] = 10 # s[:] is a view of s. # Note the difference between s = 10 and s[:] = 10","title":"View or Shallow Copy"},{"location":"ML/pyNumpy/#deep-copy","text":"The copy method makes a complete copy of the array and its data. d = a . copy () # a new array object with new data is created d is a # False d . base is a # False, d doesn't share anything with a","title":"Deep Copy"},{"location":"ML/pyPandas/","text":"Pandas is a Python module that helps us read and manipulate data. What's cool about pandas is that you can take in data and view it as a table that's human readable, but it can also be interpreted numerically so that you can do lots of computations with it. Pandas is derived from the term \"panel data\", an econometrics term for data sets that include observations over multiple time periods for the same individuals. We call the table of data a DataFrame. A Series is essentially a column, and a DataFrame is a multi-dimensional table made up of a collection of Series. As numpy ndarrays are homogeneous, pandas relaxes this requirement and allows for various dtypes in its data structures. Asking for Help help(pd.Series.loc) Series The Series is one building block in pandas. Pandas Series is a one-dimensional labeled array that can hold data of any type (integer, string, float, python objects, etc.), similar to a column in an excel spreadsheet. The axis labels are collectively called index. If we are given a bag of letters a, b, and c, and count how many of each we have, we find that there are 1 a, 2 b\u2019s, and 3 c\u2019s. We could create a Series by supplying a list of counts and their corresponding labels. If we don\u2019t specify the index, by default, the index would be the integer positions starting from 0. Accessing the value by its index, rather than the integer position comes in handy when the dataset is of thousands, if not millions, of rows. Series is the building block for the DataFrame we will introduce next. Think of Series as numpy 1d array with index or row names. import pandas as pd import numpy as np s1 = pd . Series ([ 1 , 2 , 3 ], index = [ 'a' , 'b' , 'c' ]) #Alternatively, the values can be a numpy array: s2 = pd . Series ( np . array ([ 1 , 2 , 3 ]), index = [ 'a' , 'b' , 'c' ]) #Or, we could use a dictionary to specify the index with keys s3 = pd . Series ({ 'a' : 1 , 'b' : 2 , 'c' : 3 }) #In a Series, we can access the value by its index directly s3 [ 'a' ] 1 DataFrames In data science, data is usually more than one-dimensional, and of different data types; thus Series is not sufficient. DataFrames are 2darrays with both row and column labels. The easiest way to create a DataFrame is using a dictionary. Each key is a column, while the value is an array representing the data for that column.And then, we can pass this dictionary to the DataFrame constructor. The DataFrame automatically creates a numeric index for each row. We can specify a custom index, when creating the DataFrame. import pandas as pd data = { 'ages' : [ 14 , 18 , 24 , 42 ], 'heights' : [ 165 , 180 , 176 , 184 ] } df = pd . DataFrame ( data ) df = pd . DataFrame ( data , index = [ 'James' , 'Bob' , 'Ana' , 'Susan' ]) Attributes df.index : The index (row labels) of the DataFrame. df . index Index(['Jim', 'Bob', 'Ana', 'Susan'], dtype='object') df.columns : The column labels of the DataFrame. df . columns Index(['ages', 'heights'], dtype='object') df.axes : Both index and columns. df . axes [Index(['Jim', 'Bob', 'Ana', 'Susan'], dtype='object'), Index(['ages', 'heights'], dtype='object')] df.shape : Tuple representing the dimensionality of the DataFrame. (Similar to numpy) df . shape (3, 2) df.size to return an integer representing the number of elements in this object. df . size 6 df.ndim : Return an int representing the number of axes / array dimensions. Return 1 if Series. Otherwise return 2 if DataFrame. df.loc[] : We can access a row using its index and the loc[] attribute. loc uses square brackets to specify the index. import pandas as pd data = { 'ages' : [ 14 , 18 , 24 , 42 ], 'heights' : [ 165 , 180 , 176 , 184 ] } df = pd . DataFrame ( data , index = [ 'James' , 'Bob' , 'Ana' , 'Susan' ]) print ( df . loc [ \"Bob\" ]) ages 18 heights 180 Name: Bob, dtype: int64 To set values with loc: df . loc [[ 'Bob' ],[ 'ages' ]] = 10 df . loc [ 'Bob' ] ages 10 heights 180 Name: Bob, dtype: int64 df.iloc[] : Purely int-location based indexing for selection by position. df . iloc [ 0 ] ages 14 heights 165 Name: Jim, dtype: int64 df . iloc [[ 0 ]] ages heights Jim 14 165 df . iloc [[ 0 , 1 ]] ages heights Jim 14 165 Bob 18 180 df . iloc [: 3 ] ages heights Jim 14 165 Bob 18 180 Ana 24 176 Using Masks: df . iloc [[ True , False , True ]] ages heights Jim 14 165 Ana 24 176 Using both axes: df . iloc [ 0 , 1 ] 165 Methods The head() method returns the first 5 rows. You can instruct it to return the number of rows you would like as an argument (for example, df.head(10) will return the first 10 rows). Similarly, you can get the last rows using the tail() function. The info() function is used to get essential information about your dataset, such as number of rows, columns, data types, etc. Pandas automatically generates an index for the DataFrame, if none is specified. We can set our own index column by using the set_index() function. df.setIndex(\"Sex\", inplace=True) The inplace=True argument specifies that the change will be applied to our DataFrame, without the need to assign it to a new DataFrame variable. The describe() method returns a table of statistics about the columns. We add a line in the code below to force python to display all 6 columns. Without the line, it will abbreviate the results. We can also get the summary stats for a single column. .describe() ignores the null values, such as NaN (Not a Number) and generates the descriptive statistics that summarize the central tendency (i.e., mean), dispersion (i.e., standard deviation), and shape (i.e., min, max, and quantiles) of a dataset\u2019s distribution. Count: This is the number of rows that have a value. In our case, every passenger has a value for each of the columns, so the value is 887 (the total number of passengers). Mean: Recall that the mean is the standard average. Std: This is short for standard deviation. This is a measure of how dispersed the data is. Min: The smallest value 25%: The 25th percentile 50%: The 50th percentile, also known as the median. 75%: The 75th percentile Max: The largest value df.count() : Number of non-NA values Importing and Exporting Data Generally data is stored in CSV (comma-separated values) files, which we can easily read in with panda\u2019s read_csv function. Pandas also supports reading from JSON files, as well as SQL databases. CSV import pandas as pd #read csv df = pd . read_csv ( 'titanic.csv' ) #write csv df . to_csv ( 'titanic.csv' ) print ( df . head ()) pd . options . display . max_columns = 6 Unnamed: 0 Survived Pclass ... Siblings/Spouses Parents/Children \\ 0 0 0 3 ... 1 0 1 1 1 1 ... 1 0 2 2 1 3 ... 0 0 3 3 1 1 ... 1 0 4 4 0 3 ... 0 0 Fare 0 7.2500 1 71.2833 2 7.9250 3 53.1000 4 8.0500 [5 rows x 8 columns] df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 887 entries, 0 to 886 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Survived 887 non-null int64 1 Pclass 887 non-null int64 2 Sex 887 non-null object 3 Age 887 non-null float64 4 Siblings/Spouses 887 non-null int64 5 Parents/Children 887 non-null int64 6 Fare 887 non-null float64 dtypes: float64(2), int64(4), object(1) memory usage: 48.6+ KB print ( df . describe ()) *****Describe**** Survived Pclass Age Siblings/Spouses Parents/Children \\ count 887.000000 887.000000 887.000000 887.000000 887.000000 mean 0.385569 2.305524 29.471443 0.525366 0.383315 std 0.487004 0.836662 14.121908 1.104669 0.807466 min 0.000000 1.000000 0.420000 0.000000 0.000000 25% 0.000000 2.000000 20.250000 0.000000 0.000000 50% 0.000000 3.000000 28.000000 0.000000 0.000000 75% 1.000000 3.000000 38.000000 1.000000 0.000000 max 1.000000 3.000000 80.000000 8.000000 6.000000 Fare count 887.00000 mean 32.30542 std 49.78204 min 0.00000 25% 7.92500 50% 14.45420 75% 31.13750 max 512.32920 print ( df [ 'Survived' ] . describe ()) count 887.000000 mean 0.385569 std 0.487004 min 0.000000 25% 0.000000 50% 0.000000 75% 1.000000 max 1.000000 Name: Survived, dtype: float64 Excel #Read xlsx = pd . ExcelFile ( 'file.xls' ) df = pd . read_excel ( xlsx , 'Sheet1' ) #Write df . to_excel ( 'dir/myDataFrame.xlsx' , sheet_name = 'Sheet1' ) SQL (read_sql()is a convenience wrapper around read_sql_table() and read_sql_query()) from sqlalchemy import create_engine engine = create_engine ( 'sqlite:///:memory:' ) pd . read_sql ( SELECT * FROM my_table ;, engine ) pd . read_sql_table ( 'my_table' , engine ) pd . read_sql_query ( SELECT * FROM my_table ; ', engine) df . to_sql ( 'myDf' , engine ) Selecting Columns We often will only want to deal with some of the columns that we have in our dataset. To select a single column, we use the square brackets and the column name. The result is what we call a Pandas Series. A series is like a DataFrame, but it's just a single column. Selecting Multiple Columns We can also select multiple columns from our original DataFrame, creating a smaller DataFrame. We're going to select just the Age, Sex, and Survived columns from our original DataFrame. When selecting a single column from a Pandas DataFrame, we use single square brackets. When selecting multiple columns, we use double square brackets. col = df [ 'Fare' ] print ( col ) 0 7.2500 1 71.2833 2 7.9250 3 53.1000 4 8.0500 ... 882 13.0000 883 30.0000 884 23.4500 885 30.0000 886 7.7500 Name: Fare, Length: 887, dtype: float64 small_df = df [[ 'Age' , 'Sex' , 'Survived' ]] print ( small_df . head ()) Age Sex Survived 0 22.0 male 0 1 38.0 female 1 2 26.0 female 1 3 35.0 female 1 4 35.0 male 0 Creating a Column We can easily create a new column in our DataFrame that is True if the passenger is male and False if they\u2019re female. To create a new column, we use the same bracket syntax (df['male']) and then assign this new value to it. df [ 'male' ] = df [ 'Sex' ] == 'male' print ( df . head ()) Survived Pclass Sex ... Parents/Children Fare male 0 0 3 male ... 0 7.2500 True 1 1 1 female ... 0 71.2833 False 2 1 3 female ... 0 7.9250 False 3 1 1 female ... 0 53.1000 False 4 0 3 male ... 0 8.0500 True [5 rows x 8 columns] Slicing Pandas uses the iloc function to select data based on its numeric index. It works the same way indexing lists does in Python. iloc follows the same rules as slicing does with Python lists. We can also select the data based on a condition. .loc[ ] allows us to select data by label or by a conditional statement. .loc allows us to access any of the columns. Both .loc[ ] and .iloc[ ] may be used with a boolean array to subset the data. #Third row print ( df . iloc [ 2 ]) #First three rows print ( df . iloc [: 3 ]) #rows 2 to 3 print ( df . iloc [ 1 : 3 ]) ages 24 heights 176 Name: Ana, dtype: int64 ages heights James 14 165 Bob 18 180 Ana 24 176 ages heights Bob 18 180 Ana 24 176 #Conditionals print ( df [ df [ 'ages' ] > 18 ]) #Conditionals print ( df [( df [ 'ages' ] > 18 ) & ( df [ 'heights' ] < 180 )]) ages heights Ana 24 176 Susan 42 184 ages heights Ana 24 176 Droping a Column drop() deletes rows and columns. axis=1 specifies that we want to drop a column. axis=0 will drop a row. df . drop ( 'Sex' , axis = 1 , inplace = True ) Other Functions Value_counts value_counts() returns how many times a value appears in the dataset, also called the frequency of the values. df [ 'Survived' ] . value_counts () 0 545 1 342 Name: Survived, dtype: int64 Groupby The groupby() function is used to group our dataset by the given column. Similarly, we can use min(), max(), mean(), etc. to find the corresponding values for each group. df . groupby ( 'Survived' )[ 'Age' ] . value_counts () Survived Age 0 21.0 28 28.0 27 22.0 24 18.0 23 30.0 23 .. 1 32.5 1 43.0 1 53.0 1 55.0 1 80.0 1 Name: Age, Length: 146, dtype: int64 Aggregation We can also perform multiple operations on the groupby object using .agg() method. It takes a string, a function, or a list thereof. Using groupby and agg provides us the flexibility and therefore the power to look into various perspectives of a variable or column conditioned on categories. df . groupby ( 'Survived' ) . agg ({ 'Age' :[ np . median , np . mean ], 'Fare' :[ min , max ]}) Age Fare median mean min max Survived 0 28.0 30.138532 0.0 263.0000 1 28.0 28.408392 0.0 512.3292 Sort and Rank #Sort by labels along an axis df . sort_index () #Sort by the values along an axis df . sort_values ( by = 'Fare' ) #Assign ranks to entries df . rank () Dealing with Missing Values Three approaches to dealing with missing values. 1) A Simple Option: Drop Columns with Missing Values The simplest option is to drop columns with missing values. Unless most values in the dropped columns are missing, the model loses access to a lot of (potentially useful!) information with this approach. 2) A Better Option: Imputation Imputation fills in the missing values with some number. For instance, we can fill in the mean value along each column. The imputed value won't be exactly right in most cases, but it usually leads to more accurate models than you would get from dropping the column entirely. 3) An Extension To Imputation Imputation is the standard approach, and it usually works well. However, imputed values may be systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing. In this approach, we impute the missing values, as before. And, additionally, for each column with missing entries in the original dataset, we add a new column that shows the location of the imputed entries. In some cases, this will meaningfully improve results. In other cases, it doesn't help at all. import pandas as pd from sklearn.model_selection import train_test_split # Load the data data = pd . read_csv ( 'https://raw.githubusercontent.com/esabunor/MLWorkspace/master/melb_data.csv' ) #print(data.head()) # Select target y = data . Price # To keep things simple, we'll use only numerical predictors melb_predictors = data . drop ([ 'Price' ], axis = 1 ) X = melb_predictors . select_dtypes ( exclude = [ 'object' ]) # Divide data into training and validation subsets X_train , X_valid , y_train , y_valid = train_test_split ( X , y , train_size = 0.8 , test_size = 0.2 , random_state = 0 ) Defining Scoring from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_absolute_error # Function for comparing different approaches def score_dataset ( X_train , X_valid , y_train , y_valid ): model = RandomForestRegressor ( n_estimators = 10 , random_state = 0 ) model . fit ( X_train , y_train ) preds = model . predict ( X_valid ) return mean_absolute_error ( y_valid , preds ) Drop columns with missing values # Get names of columns with missing values cols_with_missing = [ col for col in X_train . columns if X_train [ col ] . isnull () . any ()] # Drop columns in training and validation data reduced_X_train = X_train . drop ( cols_with_missing , axis = 1 ) reduced_X_valid = X_valid . drop ( cols_with_missing , axis = 1 ) print ( \"MAE from Approach 1 (Drop columns with missing values):\" ) print ( score_dataset ( reduced_X_train , reduced_X_valid , y_train , y_valid )) MAE from Approach 1 (Drop columns with missing values): 354257.66157608695 Imputation Next, we use SimpleImputer to replace missing values with the mean value along each column. Although it's simple, filling in the mean value generally performs quite well (but this varies by dataset). While statisticians have experimented with more complex ways to determine imputed values (such as regression imputation, for instance), the complex strategies typically give no additional benefit once you plug the results into sophisticated machine learning models. from sklearn.impute import SimpleImputer # Imputation my_imputer = SimpleImputer () imputed_X_train = pd . DataFrame ( my_imputer . fit_transform ( X_train )) imputed_X_valid = pd . DataFrame ( my_imputer . transform ( X_valid )) # Imputation removed column names; put them back imputed_X_train . columns = X_train . columns imputed_X_valid . columns = X_valid . columns print ( \"MAE from Approach 2 (Imputation):\" ) print ( score_dataset ( imputed_X_train , imputed_X_valid , y_train , y_valid )) MAE from Approach 2 (Imputation): 203078.71828804348 An Extension to Imputation Next, we impute the missing values, while also keeping track of which values were imputed. # Make copy to avoid changing original data (when imputing) X_train_plus = X_train . copy () X_valid_plus = X_valid . copy () # Make new columns indicating what will be imputed for col in cols_with_missing : X_train_plus [ col + '_was_missing' ] = X_train_plus [ col ] . isnull () X_valid_plus [ col + '_was_missing' ] = X_valid_plus [ col ] . isnull () # Imputation my_imputer = SimpleImputer () imputed_X_train_plus = pd . DataFrame ( my_imputer . fit_transform ( X_train_plus )) imputed_X_valid_plus = pd . DataFrame ( my_imputer . transform ( X_valid_plus )) # Imputation removed column names; put them back imputed_X_train_plus . columns = X_train_plus . columns imputed_X_valid_plus . columns = X_valid_plus . columns print ( \"MAE from Approach 3 (An Extension to Imputation):\" ) print ( score_dataset ( imputed_X_train_plus , imputed_X_valid_plus , y_train , y_valid )) MAE from Approach 3 (An Extension to Imputation): 202839.1169021739 Best result is Approach 3, lowest MAE. Categorical Variables A categorical variable takes only a limited number of values. Consider a survey that asks how often you eat breakfast and provides four options: \"Never\", \"Rarely\", \"Most days\", or \"Every day\". In this case, the data is categorical, because responses fall into a fixed set of categories. If people responded to a survey about which what brand of car they owned, the responses would fall into categories like \"Honda\", \"Toyota\", and \"Ford\". In this case, the data is also categorical. You will get an error if you try to plug these variables into most machine learning models in Python without preprocessing them first. 1) Drop Categorical Variables The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information. 2) Ordinal Encoding Ordinal encoding assigns each unique value to a different integer. This approach assumes an ordering of the categories: \"Never\" (0) < \"Rarely\" (1) < \"Most days\" (2) < \"Every day\" (3). This assumption makes sense in this example, because there is an indisputable ranking to the categories. Not all categorical variables have a clear ordering in the values, but we refer to those that do as ordinal variables. For tree-based models (like decision trees and random forests), you can expect ordinal encoding to work well with ordinal variables. 3) One-Hot Encoding One-hot encoding creates new columns indicating the presence (or absence) of each possible value in the original data. To understand this, we'll work through an example. In contrast to ordinal encoding, one-hot encoding does not assume an ordering of the categories. Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data (e.g., \"Red\" is neither more nor less than \"Yellow\"). We refer to categorical variables without an intrinsic ranking as nominal variables. One-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., you generally won't use it for variables taking more than 15 different values). import pandas as pd from sklearn.model_selection import train_test_split # Read the data data = pd . read_csv ( 'https://raw.githubusercontent.com/esabunor/MLWorkspace/master/melb_data.csv' ) # Separate target from predictors y = data . Price X = data . drop ([ 'Price' ], axis = 1 ) # Divide data into training and validation subsets X_train_full , X_valid_full , y_train , y_valid = train_test_split ( X , y , train_size = 0.8 , test_size = 0.2 , random_state = 0 ) # Drop columns with missing values (simplest approach) cols_with_missing = [ col for col in X_train_full . columns if X_train_full [ col ] . isnull () . any ()] X_train_full . drop ( cols_with_missing , axis = 1 , inplace = True ) X_valid_full . drop ( cols_with_missing , axis = 1 , inplace = True ) # \"Cardinality\" means the number of unique values in a column # Select categorical columns with relatively low cardinality (convenient but arbitrary) low_cardinality_cols = [ cname for cname in X_train_full . columns if X_train_full [ cname ] . nunique () < 10 and X_train_full [ cname ] . dtype == \"object\" ] # Select numerical columns numerical_cols = [ cname for cname in X_train_full . columns if X_train_full [ cname ] . dtype in [ 'int64' , 'float64' ]] # Keep selected columns only my_cols = low_cardinality_cols + numerical_cols X_train = X_train_full [ my_cols ] . copy () X_valid = X_valid_full [ my_cols ] . copy () X_train . head () Type Method Unnamed: 0 Rooms 2573 h SP 3349 4 2091 h SP 2686 3 4683 u S 6065 2 8832 h VB 11346 3 10469 u S 13474 2 Next, we obtain a list of all of the categorical variables in the training data. We do this by checking the data type (or dtype) of each column. The object dtype indicates a column has text (there are other things it could theoretically be, but that's unimportant for our purposes). For this dataset, the columns with text indicate categorical variables. # Get list of categorical variables s = ( X_train . dtypes == 'object' ) object_cols = list ( s [ s ] . index ) print ( \"Categorical variables:\" ) print ( object_cols ) Categorical variables: ['Type', 'Method'] Define Function to Measure Quality of Each Approach We define a function score_dataset() to compare the three different approaches to dealing with categorical variables. This function reports the mean absolute error (MAE) from a random forest model. In general, we want the MAE to be as low as possible! from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_absolute_error # Function for comparing different approaches def score_dataset ( X_train , X_valid , y_train , y_valid ): model = RandomForestRegressor ( n_estimators = 100 , random_state = 0 ) model . fit ( X_train , y_train ) preds = model . predict ( X_valid ) return mean_absolute_error ( y_valid , preds ) Score from Approach 1 (Drop Categorical Variables) We drop the object columns with the select_dtypes() method. drop_X_train = X_train . select_dtypes ( exclude = [ 'object' ]) drop_X_valid = X_valid . select_dtypes ( exclude = [ 'object' ]) print ( \"MAE from Approach 1 (Drop categorical variables):\" ) print ( score_dataset ( drop_X_train , drop_X_valid , y_train , y_valid )) MAE from Approach 1 (Drop categorical variables): 345278.21768750006 Score from Approach 2 (Ordinal Encoding) Scikit-learn has a OrdinalEncoder class that can be used to get ordinal encodings. We loop over the categorical variables and apply the ordinal encoder separately to each column. In the code cell below, for each column, we randomly assign each unique value to a different integer. This is a common approach that is simpler than providing custom labels; however, we can expect an additional boost in performance if we provide better-informed labels for all ordinal variables. from sklearn.preprocessing import OrdinalEncoder # Make copy to avoid changing original data label_X_train = X_train . copy () label_X_valid = X_valid . copy () # Apply ordinal encoder to each column with categorical data ordinal_encoder = OrdinalEncoder () label_X_train [ object_cols ] = ordinal_encoder . fit_transform ( X_train [ object_cols ]) label_X_valid [ object_cols ] = ordinal_encoder . transform ( X_valid [ object_cols ]) print ( \"MAE from Approach 2 (Ordinal Encoding):\" ) print ( score_dataset ( label_X_train , label_X_valid , y_train , y_valid )) MAE from Approach 2 (Ordinal Encoding): 314139.64080978255 Fitting an ordinal encoder to a column in the training data creates a corresponding integer-valued label for each unique value that appears in the training data. In the case that the validation data contains values that don't also appear in the training data, the encoder will throw an error, because these values won't have an integer assigned to them. This is a common problem that you'll encounter with real-world data, and there are many approaches to fixing this issue. For instance, you can write a custom ordinal encoder to deal with new categories. The simplest approach, however, is to drop the problematic categorical columns. The code cell below save the problematic columns to a Python list bad_label_cols. Likewise, columns that can be safely ordinal encoded are stored in good_label_cols. # Categorical columns in the training data object_cols = [ col for col in X_train . columns if X_train [ col ] . dtype == \"object\" ] # Columns that can be safely ordinal encoded good_label_cols = [ col for col in object_cols if set ( X_valid [ col ]) . issubset ( set ( X_train [ col ]))] # Problematic columns that will be dropped from the dataset bad_label_cols = list ( set ( object_cols ) - set ( good_label_cols )) print ( 'Categorical columns that will be ordinal encoded:' , good_label_cols ) print ( ' \\n Categorical columns that will be dropped from the dataset:' , bad_label_cols ) Score from Approach 3 (One-Hot Encoding) We use the OneHotEncoder class from scikit-learn to get one-hot encodings. There are a number of parameters that can be used to customize its behavior. We set handle_unknown='ignore' to avoid errors when the validation data contains classes that aren't represented in the training data, and setting sparse=False ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix). To use the encoder, we supply only the categorical columns that we want to be one-hot encoded. For instance, to encode the training data, we supply X_train[object_cols]. (object_cols in the code cell below is a list of the column names with categorical data, and so X_train[object_cols] contains all of the categorical data in the training set.) from sklearn.preprocessing import OneHotEncoder # Apply one-hot encoder to each column with categorical data OH_encoder = OneHotEncoder ( handle_unknown = 'ignore' , sparse = False ) OH_cols_train = pd . DataFrame ( OH_encoder . fit_transform ( X_train [ object_cols ])) OH_cols_valid = pd . DataFrame ( OH_encoder . transform ( X_valid [ object_cols ])) # One-hot encoding removed index; put it back OH_cols_train . index = X_train . index OH_cols_valid . index = X_valid . index # Remove categorical columns (will replace with one-hot encoding) num_X_train = X_train . drop ( object_cols , axis = 1 ) num_X_valid = X_valid . drop ( object_cols , axis = 1 ) # Add one-hot encoded columns to numerical features OH_X_train = pd . concat ([ num_X_train , OH_cols_train ], axis = 1 ) OH_X_valid = pd . concat ([ num_X_valid , OH_cols_valid ], axis = 1 ) print ( \"MAE from Approach 3 (One-Hot Encoding):\" ) print ( score_dataset ( OH_X_train , OH_X_valid , y_train , y_valid )) MAE from Approach 3 (One-Hot Encoding): 314342.2874402174 We refer to the number of unique entries of a categorical variable as the cardinality of that categorical variable. For large datasets with many rows, one-hot encoding can greatly expand the size of the dataset. For this reason, we typically will only one-hot encode columns with relatively low cardinality. Then, high cardinality columns can either be dropped from the dataset, or we can use ordinal encoding. # Get number of unique entries in each column with categorical data object_nunique = list ( map ( lambda col : X_train [ col ] . nunique (), object_cols )) d = dict ( zip ( object_cols , object_nunique )) # Print number of unique entries by column, in ascending order sorted ( d . items (), key = lambda x : x [ 1 ]) to set low_cardinality_cols to a Python list containing the columns that will be one-hot encoded. Likewise, high_cardinality_cols contains a list of categorical columns that will be dropped from the dataset. # Columns that will be one-hot encoded low_cardinality_cols = [ col for col in object_cols if X_train [ col ] . nunique () < 10 ] # Columns that will be dropped from the dataset high_cardinality_cols = list ( set ( object_cols ) - set ( low_cardinality_cols )) print ( 'Categorical columns that will be one-hot encoded:' , low_cardinality_cols ) print ( ' \\n Categorical columns that will be dropped from the dataset:' , high_cardinality_cols ) Which approach is best? In this case, dropping the categorical columns (Approach 1) performed worst, since it had the highest MAE score. As for the other two approaches, since the returned MAE scores are so close in value, there doesn't appear to be any meaningful benefit to one over the other. In general, one-hot encoding (Approach 3) will typically perform best, and dropping the categorical columns (Approach 1) typically performs worst, but it varies on a case-by-case basis.","title":"Pandas"},{"location":"ML/pyPandas/#series","text":"The Series is one building block in pandas. Pandas Series is a one-dimensional labeled array that can hold data of any type (integer, string, float, python objects, etc.), similar to a column in an excel spreadsheet. The axis labels are collectively called index. If we are given a bag of letters a, b, and c, and count how many of each we have, we find that there are 1 a, 2 b\u2019s, and 3 c\u2019s. We could create a Series by supplying a list of counts and their corresponding labels. If we don\u2019t specify the index, by default, the index would be the integer positions starting from 0. Accessing the value by its index, rather than the integer position comes in handy when the dataset is of thousands, if not millions, of rows. Series is the building block for the DataFrame we will introduce next. Think of Series as numpy 1d array with index or row names. import pandas as pd import numpy as np s1 = pd . Series ([ 1 , 2 , 3 ], index = [ 'a' , 'b' , 'c' ]) #Alternatively, the values can be a numpy array: s2 = pd . Series ( np . array ([ 1 , 2 , 3 ]), index = [ 'a' , 'b' , 'c' ]) #Or, we could use a dictionary to specify the index with keys s3 = pd . Series ({ 'a' : 1 , 'b' : 2 , 'c' : 3 }) #In a Series, we can access the value by its index directly s3 [ 'a' ] 1","title":"Series"},{"location":"ML/pyPandas/#dataframes","text":"In data science, data is usually more than one-dimensional, and of different data types; thus Series is not sufficient. DataFrames are 2darrays with both row and column labels. The easiest way to create a DataFrame is using a dictionary. Each key is a column, while the value is an array representing the data for that column.And then, we can pass this dictionary to the DataFrame constructor. The DataFrame automatically creates a numeric index for each row. We can specify a custom index, when creating the DataFrame. import pandas as pd data = { 'ages' : [ 14 , 18 , 24 , 42 ], 'heights' : [ 165 , 180 , 176 , 184 ] } df = pd . DataFrame ( data ) df = pd . DataFrame ( data , index = [ 'James' , 'Bob' , 'Ana' , 'Susan' ])","title":"DataFrames"},{"location":"ML/pyPandas/#attributes","text":"df.index : The index (row labels) of the DataFrame. df . index Index(['Jim', 'Bob', 'Ana', 'Susan'], dtype='object') df.columns : The column labels of the DataFrame. df . columns Index(['ages', 'heights'], dtype='object') df.axes : Both index and columns. df . axes [Index(['Jim', 'Bob', 'Ana', 'Susan'], dtype='object'), Index(['ages', 'heights'], dtype='object')] df.shape : Tuple representing the dimensionality of the DataFrame. (Similar to numpy) df . shape (3, 2) df.size to return an integer representing the number of elements in this object. df . size 6 df.ndim : Return an int representing the number of axes / array dimensions. Return 1 if Series. Otherwise return 2 if DataFrame. df.loc[] : We can access a row using its index and the loc[] attribute. loc uses square brackets to specify the index. import pandas as pd data = { 'ages' : [ 14 , 18 , 24 , 42 ], 'heights' : [ 165 , 180 , 176 , 184 ] } df = pd . DataFrame ( data , index = [ 'James' , 'Bob' , 'Ana' , 'Susan' ]) print ( df . loc [ \"Bob\" ]) ages 18 heights 180 Name: Bob, dtype: int64 To set values with loc: df . loc [[ 'Bob' ],[ 'ages' ]] = 10 df . loc [ 'Bob' ] ages 10 heights 180 Name: Bob, dtype: int64 df.iloc[] : Purely int-location based indexing for selection by position. df . iloc [ 0 ] ages 14 heights 165 Name: Jim, dtype: int64 df . iloc [[ 0 ]] ages heights Jim 14 165 df . iloc [[ 0 , 1 ]] ages heights Jim 14 165 Bob 18 180 df . iloc [: 3 ] ages heights Jim 14 165 Bob 18 180 Ana 24 176 Using Masks: df . iloc [[ True , False , True ]] ages heights Jim 14 165 Ana 24 176 Using both axes: df . iloc [ 0 , 1 ] 165","title":"Attributes"},{"location":"ML/pyPandas/#methods","text":"The head() method returns the first 5 rows. You can instruct it to return the number of rows you would like as an argument (for example, df.head(10) will return the first 10 rows). Similarly, you can get the last rows using the tail() function. The info() function is used to get essential information about your dataset, such as number of rows, columns, data types, etc. Pandas automatically generates an index for the DataFrame, if none is specified. We can set our own index column by using the set_index() function. df.setIndex(\"Sex\", inplace=True) The inplace=True argument specifies that the change will be applied to our DataFrame, without the need to assign it to a new DataFrame variable. The describe() method returns a table of statistics about the columns. We add a line in the code below to force python to display all 6 columns. Without the line, it will abbreviate the results. We can also get the summary stats for a single column. .describe() ignores the null values, such as NaN (Not a Number) and generates the descriptive statistics that summarize the central tendency (i.e., mean), dispersion (i.e., standard deviation), and shape (i.e., min, max, and quantiles) of a dataset\u2019s distribution. Count: This is the number of rows that have a value. In our case, every passenger has a value for each of the columns, so the value is 887 (the total number of passengers). Mean: Recall that the mean is the standard average. Std: This is short for standard deviation. This is a measure of how dispersed the data is. Min: The smallest value 25%: The 25th percentile 50%: The 50th percentile, also known as the median. 75%: The 75th percentile Max: The largest value df.count() : Number of non-NA values","title":"Methods"},{"location":"ML/pyPandas/#importing-and-exporting-data","text":"Generally data is stored in CSV (comma-separated values) files, which we can easily read in with panda\u2019s read_csv function. Pandas also supports reading from JSON files, as well as SQL databases.","title":"Importing and Exporting Data"},{"location":"ML/pyPandas/#csv","text":"import pandas as pd #read csv df = pd . read_csv ( 'titanic.csv' ) #write csv df . to_csv ( 'titanic.csv' ) print ( df . head ()) pd . options . display . max_columns = 6 Unnamed: 0 Survived Pclass ... Siblings/Spouses Parents/Children \\ 0 0 0 3 ... 1 0 1 1 1 1 ... 1 0 2 2 1 3 ... 0 0 3 3 1 1 ... 1 0 4 4 0 3 ... 0 0 Fare 0 7.2500 1 71.2833 2 7.9250 3 53.1000 4 8.0500 [5 rows x 8 columns] df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 887 entries, 0 to 886 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Survived 887 non-null int64 1 Pclass 887 non-null int64 2 Sex 887 non-null object 3 Age 887 non-null float64 4 Siblings/Spouses 887 non-null int64 5 Parents/Children 887 non-null int64 6 Fare 887 non-null float64 dtypes: float64(2), int64(4), object(1) memory usage: 48.6+ KB print ( df . describe ()) *****Describe**** Survived Pclass Age Siblings/Spouses Parents/Children \\ count 887.000000 887.000000 887.000000 887.000000 887.000000 mean 0.385569 2.305524 29.471443 0.525366 0.383315 std 0.487004 0.836662 14.121908 1.104669 0.807466 min 0.000000 1.000000 0.420000 0.000000 0.000000 25% 0.000000 2.000000 20.250000 0.000000 0.000000 50% 0.000000 3.000000 28.000000 0.000000 0.000000 75% 1.000000 3.000000 38.000000 1.000000 0.000000 max 1.000000 3.000000 80.000000 8.000000 6.000000 Fare count 887.00000 mean 32.30542 std 49.78204 min 0.00000 25% 7.92500 50% 14.45420 75% 31.13750 max 512.32920 print ( df [ 'Survived' ] . describe ()) count 887.000000 mean 0.385569 std 0.487004 min 0.000000 25% 0.000000 50% 0.000000 75% 1.000000 max 1.000000 Name: Survived, dtype: float64","title":"CSV"},{"location":"ML/pyPandas/#excel","text":"#Read xlsx = pd . ExcelFile ( 'file.xls' ) df = pd . read_excel ( xlsx , 'Sheet1' ) #Write df . to_excel ( 'dir/myDataFrame.xlsx' , sheet_name = 'Sheet1' )","title":"Excel"},{"location":"ML/pyPandas/#sql","text":"(read_sql()is a convenience wrapper around read_sql_table() and read_sql_query()) from sqlalchemy import create_engine engine = create_engine ( 'sqlite:///:memory:' ) pd . read_sql ( SELECT * FROM my_table ;, engine ) pd . read_sql_table ( 'my_table' , engine ) pd . read_sql_query ( SELECT * FROM my_table ; ', engine) df . to_sql ( 'myDf' , engine )","title":"SQL"},{"location":"ML/pyPandas/#selecting-columns","text":"We often will only want to deal with some of the columns that we have in our dataset. To select a single column, we use the square brackets and the column name. The result is what we call a Pandas Series. A series is like a DataFrame, but it's just a single column. Selecting Multiple Columns We can also select multiple columns from our original DataFrame, creating a smaller DataFrame. We're going to select just the Age, Sex, and Survived columns from our original DataFrame. When selecting a single column from a Pandas DataFrame, we use single square brackets. When selecting multiple columns, we use double square brackets. col = df [ 'Fare' ] print ( col ) 0 7.2500 1 71.2833 2 7.9250 3 53.1000 4 8.0500 ... 882 13.0000 883 30.0000 884 23.4500 885 30.0000 886 7.7500 Name: Fare, Length: 887, dtype: float64 small_df = df [[ 'Age' , 'Sex' , 'Survived' ]] print ( small_df . head ()) Age Sex Survived 0 22.0 male 0 1 38.0 female 1 2 26.0 female 1 3 35.0 female 1 4 35.0 male 0","title":"Selecting Columns"},{"location":"ML/pyPandas/#creating-a-column","text":"We can easily create a new column in our DataFrame that is True if the passenger is male and False if they\u2019re female. To create a new column, we use the same bracket syntax (df['male']) and then assign this new value to it. df [ 'male' ] = df [ 'Sex' ] == 'male' print ( df . head ()) Survived Pclass Sex ... Parents/Children Fare male 0 0 3 male ... 0 7.2500 True 1 1 1 female ... 0 71.2833 False 2 1 3 female ... 0 7.9250 False 3 1 1 female ... 0 53.1000 False 4 0 3 male ... 0 8.0500 True [5 rows x 8 columns]","title":"Creating a Column"},{"location":"ML/pyPandas/#slicing","text":"Pandas uses the iloc function to select data based on its numeric index. It works the same way indexing lists does in Python. iloc follows the same rules as slicing does with Python lists. We can also select the data based on a condition. .loc[ ] allows us to select data by label or by a conditional statement. .loc allows us to access any of the columns. Both .loc[ ] and .iloc[ ] may be used with a boolean array to subset the data. #Third row print ( df . iloc [ 2 ]) #First three rows print ( df . iloc [: 3 ]) #rows 2 to 3 print ( df . iloc [ 1 : 3 ]) ages 24 heights 176 Name: Ana, dtype: int64 ages heights James 14 165 Bob 18 180 Ana 24 176 ages heights Bob 18 180 Ana 24 176 #Conditionals print ( df [ df [ 'ages' ] > 18 ]) #Conditionals print ( df [( df [ 'ages' ] > 18 ) & ( df [ 'heights' ] < 180 )]) ages heights Ana 24 176 Susan 42 184 ages heights Ana 24 176","title":"Slicing"},{"location":"ML/pyPandas/#droping-a-column","text":"drop() deletes rows and columns. axis=1 specifies that we want to drop a column. axis=0 will drop a row. df . drop ( 'Sex' , axis = 1 , inplace = True )","title":"Droping a Column"},{"location":"ML/pyPandas/#other-functions","text":"","title":"Other Functions"},{"location":"ML/pyPandas/#value_counts","text":"value_counts() returns how many times a value appears in the dataset, also called the frequency of the values. df [ 'Survived' ] . value_counts () 0 545 1 342 Name: Survived, dtype: int64","title":"Value_counts"},{"location":"ML/pyPandas/#groupby","text":"The groupby() function is used to group our dataset by the given column. Similarly, we can use min(), max(), mean(), etc. to find the corresponding values for each group. df . groupby ( 'Survived' )[ 'Age' ] . value_counts () Survived Age 0 21.0 28 28.0 27 22.0 24 18.0 23 30.0 23 .. 1 32.5 1 43.0 1 53.0 1 55.0 1 80.0 1 Name: Age, Length: 146, dtype: int64","title":"Groupby"},{"location":"ML/pyPandas/#aggregation","text":"We can also perform multiple operations on the groupby object using .agg() method. It takes a string, a function, or a list thereof. Using groupby and agg provides us the flexibility and therefore the power to look into various perspectives of a variable or column conditioned on categories. df . groupby ( 'Survived' ) . agg ({ 'Age' :[ np . median , np . mean ], 'Fare' :[ min , max ]}) Age Fare median mean min max Survived 0 28.0 30.138532 0.0 263.0000 1 28.0 28.408392 0.0 512.3292","title":"Aggregation"},{"location":"ML/pyPandas/#sort-and-rank","text":"#Sort by labels along an axis df . sort_index () #Sort by the values along an axis df . sort_values ( by = 'Fare' ) #Assign ranks to entries df . rank ()","title":"Sort and Rank"},{"location":"ML/pyPandas/#dealing-with-missing-values","text":"Three approaches to dealing with missing values. 1) A Simple Option: Drop Columns with Missing Values The simplest option is to drop columns with missing values. Unless most values in the dropped columns are missing, the model loses access to a lot of (potentially useful!) information with this approach. 2) A Better Option: Imputation Imputation fills in the missing values with some number. For instance, we can fill in the mean value along each column. The imputed value won't be exactly right in most cases, but it usually leads to more accurate models than you would get from dropping the column entirely. 3) An Extension To Imputation Imputation is the standard approach, and it usually works well. However, imputed values may be systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing. In this approach, we impute the missing values, as before. And, additionally, for each column with missing entries in the original dataset, we add a new column that shows the location of the imputed entries. In some cases, this will meaningfully improve results. In other cases, it doesn't help at all. import pandas as pd from sklearn.model_selection import train_test_split # Load the data data = pd . read_csv ( 'https://raw.githubusercontent.com/esabunor/MLWorkspace/master/melb_data.csv' ) #print(data.head()) # Select target y = data . Price # To keep things simple, we'll use only numerical predictors melb_predictors = data . drop ([ 'Price' ], axis = 1 ) X = melb_predictors . select_dtypes ( exclude = [ 'object' ]) # Divide data into training and validation subsets X_train , X_valid , y_train , y_valid = train_test_split ( X , y , train_size = 0.8 , test_size = 0.2 , random_state = 0 ) Defining Scoring from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_absolute_error # Function for comparing different approaches def score_dataset ( X_train , X_valid , y_train , y_valid ): model = RandomForestRegressor ( n_estimators = 10 , random_state = 0 ) model . fit ( X_train , y_train ) preds = model . predict ( X_valid ) return mean_absolute_error ( y_valid , preds ) Drop columns with missing values # Get names of columns with missing values cols_with_missing = [ col for col in X_train . columns if X_train [ col ] . isnull () . any ()] # Drop columns in training and validation data reduced_X_train = X_train . drop ( cols_with_missing , axis = 1 ) reduced_X_valid = X_valid . drop ( cols_with_missing , axis = 1 ) print ( \"MAE from Approach 1 (Drop columns with missing values):\" ) print ( score_dataset ( reduced_X_train , reduced_X_valid , y_train , y_valid )) MAE from Approach 1 (Drop columns with missing values): 354257.66157608695 Imputation Next, we use SimpleImputer to replace missing values with the mean value along each column. Although it's simple, filling in the mean value generally performs quite well (but this varies by dataset). While statisticians have experimented with more complex ways to determine imputed values (such as regression imputation, for instance), the complex strategies typically give no additional benefit once you plug the results into sophisticated machine learning models. from sklearn.impute import SimpleImputer # Imputation my_imputer = SimpleImputer () imputed_X_train = pd . DataFrame ( my_imputer . fit_transform ( X_train )) imputed_X_valid = pd . DataFrame ( my_imputer . transform ( X_valid )) # Imputation removed column names; put them back imputed_X_train . columns = X_train . columns imputed_X_valid . columns = X_valid . columns print ( \"MAE from Approach 2 (Imputation):\" ) print ( score_dataset ( imputed_X_train , imputed_X_valid , y_train , y_valid )) MAE from Approach 2 (Imputation): 203078.71828804348 An Extension to Imputation Next, we impute the missing values, while also keeping track of which values were imputed. # Make copy to avoid changing original data (when imputing) X_train_plus = X_train . copy () X_valid_plus = X_valid . copy () # Make new columns indicating what will be imputed for col in cols_with_missing : X_train_plus [ col + '_was_missing' ] = X_train_plus [ col ] . isnull () X_valid_plus [ col + '_was_missing' ] = X_valid_plus [ col ] . isnull () # Imputation my_imputer = SimpleImputer () imputed_X_train_plus = pd . DataFrame ( my_imputer . fit_transform ( X_train_plus )) imputed_X_valid_plus = pd . DataFrame ( my_imputer . transform ( X_valid_plus )) # Imputation removed column names; put them back imputed_X_train_plus . columns = X_train_plus . columns imputed_X_valid_plus . columns = X_valid_plus . columns print ( \"MAE from Approach 3 (An Extension to Imputation):\" ) print ( score_dataset ( imputed_X_train_plus , imputed_X_valid_plus , y_train , y_valid )) MAE from Approach 3 (An Extension to Imputation): 202839.1169021739 Best result is Approach 3, lowest MAE.","title":"Dealing with Missing Values"},{"location":"ML/pyPandas/#categorical-variables","text":"A categorical variable takes only a limited number of values. Consider a survey that asks how often you eat breakfast and provides four options: \"Never\", \"Rarely\", \"Most days\", or \"Every day\". In this case, the data is categorical, because responses fall into a fixed set of categories. If people responded to a survey about which what brand of car they owned, the responses would fall into categories like \"Honda\", \"Toyota\", and \"Ford\". In this case, the data is also categorical. You will get an error if you try to plug these variables into most machine learning models in Python without preprocessing them first. 1) Drop Categorical Variables The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information. 2) Ordinal Encoding Ordinal encoding assigns each unique value to a different integer. This approach assumes an ordering of the categories: \"Never\" (0) < \"Rarely\" (1) < \"Most days\" (2) < \"Every day\" (3). This assumption makes sense in this example, because there is an indisputable ranking to the categories. Not all categorical variables have a clear ordering in the values, but we refer to those that do as ordinal variables. For tree-based models (like decision trees and random forests), you can expect ordinal encoding to work well with ordinal variables. 3) One-Hot Encoding One-hot encoding creates new columns indicating the presence (or absence) of each possible value in the original data. To understand this, we'll work through an example. In contrast to ordinal encoding, one-hot encoding does not assume an ordering of the categories. Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data (e.g., \"Red\" is neither more nor less than \"Yellow\"). We refer to categorical variables without an intrinsic ranking as nominal variables. One-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., you generally won't use it for variables taking more than 15 different values). import pandas as pd from sklearn.model_selection import train_test_split # Read the data data = pd . read_csv ( 'https://raw.githubusercontent.com/esabunor/MLWorkspace/master/melb_data.csv' ) # Separate target from predictors y = data . Price X = data . drop ([ 'Price' ], axis = 1 ) # Divide data into training and validation subsets X_train_full , X_valid_full , y_train , y_valid = train_test_split ( X , y , train_size = 0.8 , test_size = 0.2 , random_state = 0 ) # Drop columns with missing values (simplest approach) cols_with_missing = [ col for col in X_train_full . columns if X_train_full [ col ] . isnull () . any ()] X_train_full . drop ( cols_with_missing , axis = 1 , inplace = True ) X_valid_full . drop ( cols_with_missing , axis = 1 , inplace = True ) # \"Cardinality\" means the number of unique values in a column # Select categorical columns with relatively low cardinality (convenient but arbitrary) low_cardinality_cols = [ cname for cname in X_train_full . columns if X_train_full [ cname ] . nunique () < 10 and X_train_full [ cname ] . dtype == \"object\" ] # Select numerical columns numerical_cols = [ cname for cname in X_train_full . columns if X_train_full [ cname ] . dtype in [ 'int64' , 'float64' ]] # Keep selected columns only my_cols = low_cardinality_cols + numerical_cols X_train = X_train_full [ my_cols ] . copy () X_valid = X_valid_full [ my_cols ] . copy () X_train . head () Type Method Unnamed: 0 Rooms 2573 h SP 3349 4 2091 h SP 2686 3 4683 u S 6065 2 8832 h VB 11346 3 10469 u S 13474 2 Next, we obtain a list of all of the categorical variables in the training data. We do this by checking the data type (or dtype) of each column. The object dtype indicates a column has text (there are other things it could theoretically be, but that's unimportant for our purposes). For this dataset, the columns with text indicate categorical variables. # Get list of categorical variables s = ( X_train . dtypes == 'object' ) object_cols = list ( s [ s ] . index ) print ( \"Categorical variables:\" ) print ( object_cols ) Categorical variables: ['Type', 'Method'] Define Function to Measure Quality of Each Approach We define a function score_dataset() to compare the three different approaches to dealing with categorical variables. This function reports the mean absolute error (MAE) from a random forest model. In general, we want the MAE to be as low as possible! from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_absolute_error # Function for comparing different approaches def score_dataset ( X_train , X_valid , y_train , y_valid ): model = RandomForestRegressor ( n_estimators = 100 , random_state = 0 ) model . fit ( X_train , y_train ) preds = model . predict ( X_valid ) return mean_absolute_error ( y_valid , preds ) Score from Approach 1 (Drop Categorical Variables) We drop the object columns with the select_dtypes() method. drop_X_train = X_train . select_dtypes ( exclude = [ 'object' ]) drop_X_valid = X_valid . select_dtypes ( exclude = [ 'object' ]) print ( \"MAE from Approach 1 (Drop categorical variables):\" ) print ( score_dataset ( drop_X_train , drop_X_valid , y_train , y_valid )) MAE from Approach 1 (Drop categorical variables): 345278.21768750006 Score from Approach 2 (Ordinal Encoding) Scikit-learn has a OrdinalEncoder class that can be used to get ordinal encodings. We loop over the categorical variables and apply the ordinal encoder separately to each column. In the code cell below, for each column, we randomly assign each unique value to a different integer. This is a common approach that is simpler than providing custom labels; however, we can expect an additional boost in performance if we provide better-informed labels for all ordinal variables. from sklearn.preprocessing import OrdinalEncoder # Make copy to avoid changing original data label_X_train = X_train . copy () label_X_valid = X_valid . copy () # Apply ordinal encoder to each column with categorical data ordinal_encoder = OrdinalEncoder () label_X_train [ object_cols ] = ordinal_encoder . fit_transform ( X_train [ object_cols ]) label_X_valid [ object_cols ] = ordinal_encoder . transform ( X_valid [ object_cols ]) print ( \"MAE from Approach 2 (Ordinal Encoding):\" ) print ( score_dataset ( label_X_train , label_X_valid , y_train , y_valid )) MAE from Approach 2 (Ordinal Encoding): 314139.64080978255 Fitting an ordinal encoder to a column in the training data creates a corresponding integer-valued label for each unique value that appears in the training data. In the case that the validation data contains values that don't also appear in the training data, the encoder will throw an error, because these values won't have an integer assigned to them. This is a common problem that you'll encounter with real-world data, and there are many approaches to fixing this issue. For instance, you can write a custom ordinal encoder to deal with new categories. The simplest approach, however, is to drop the problematic categorical columns. The code cell below save the problematic columns to a Python list bad_label_cols. Likewise, columns that can be safely ordinal encoded are stored in good_label_cols. # Categorical columns in the training data object_cols = [ col for col in X_train . columns if X_train [ col ] . dtype == \"object\" ] # Columns that can be safely ordinal encoded good_label_cols = [ col for col in object_cols if set ( X_valid [ col ]) . issubset ( set ( X_train [ col ]))] # Problematic columns that will be dropped from the dataset bad_label_cols = list ( set ( object_cols ) - set ( good_label_cols )) print ( 'Categorical columns that will be ordinal encoded:' , good_label_cols ) print ( ' \\n Categorical columns that will be dropped from the dataset:' , bad_label_cols ) Score from Approach 3 (One-Hot Encoding) We use the OneHotEncoder class from scikit-learn to get one-hot encodings. There are a number of parameters that can be used to customize its behavior. We set handle_unknown='ignore' to avoid errors when the validation data contains classes that aren't represented in the training data, and setting sparse=False ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix). To use the encoder, we supply only the categorical columns that we want to be one-hot encoded. For instance, to encode the training data, we supply X_train[object_cols]. (object_cols in the code cell below is a list of the column names with categorical data, and so X_train[object_cols] contains all of the categorical data in the training set.) from sklearn.preprocessing import OneHotEncoder # Apply one-hot encoder to each column with categorical data OH_encoder = OneHotEncoder ( handle_unknown = 'ignore' , sparse = False ) OH_cols_train = pd . DataFrame ( OH_encoder . fit_transform ( X_train [ object_cols ])) OH_cols_valid = pd . DataFrame ( OH_encoder . transform ( X_valid [ object_cols ])) # One-hot encoding removed index; put it back OH_cols_train . index = X_train . index OH_cols_valid . index = X_valid . index # Remove categorical columns (will replace with one-hot encoding) num_X_train = X_train . drop ( object_cols , axis = 1 ) num_X_valid = X_valid . drop ( object_cols , axis = 1 ) # Add one-hot encoded columns to numerical features OH_X_train = pd . concat ([ num_X_train , OH_cols_train ], axis = 1 ) OH_X_valid = pd . concat ([ num_X_valid , OH_cols_valid ], axis = 1 ) print ( \"MAE from Approach 3 (One-Hot Encoding):\" ) print ( score_dataset ( OH_X_train , OH_X_valid , y_train , y_valid )) MAE from Approach 3 (One-Hot Encoding): 314342.2874402174 We refer to the number of unique entries of a categorical variable as the cardinality of that categorical variable. For large datasets with many rows, one-hot encoding can greatly expand the size of the dataset. For this reason, we typically will only one-hot encode columns with relatively low cardinality. Then, high cardinality columns can either be dropped from the dataset, or we can use ordinal encoding. # Get number of unique entries in each column with categorical data object_nunique = list ( map ( lambda col : X_train [ col ] . nunique (), object_cols )) d = dict ( zip ( object_cols , object_nunique )) # Print number of unique entries by column, in ascending order sorted ( d . items (), key = lambda x : x [ 1 ]) to set low_cardinality_cols to a Python list containing the columns that will be one-hot encoded. Likewise, high_cardinality_cols contains a list of categorical columns that will be dropped from the dataset. # Columns that will be one-hot encoded low_cardinality_cols = [ col for col in object_cols if X_train [ col ] . nunique () < 10 ] # Columns that will be dropped from the dataset high_cardinality_cols = list ( set ( object_cols ) - set ( low_cardinality_cols )) print ( 'Categorical columns that will be one-hot encoded:' , low_cardinality_cols ) print ( ' \\n Categorical columns that will be dropped from the dataset:' , high_cardinality_cols ) Which approach is best? In this case, dropping the categorical columns (Approach 1) performed worst, since it had the highest MAE score. As for the other two approaches, since the returned MAE scores are so close in value, there doesn't appear to be any meaningful benefit to one over the other. In general, one-hot encoding (Approach 3) will typically perform best, and dropping the categorical columns (Approach 1) typically performs worst, but it varies on a case-by-case basis.","title":"Categorical Variables"},{"location":"ML/pyPipe/","text":"Pipelines are a simple way to keep your data preprocessing and modeling code organized. Specifically, a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step. Many data scientists hack together models without pipelines, but pipelines have some important benefits. Those include: Cleaner Code: Accounting for data at each step of preprocessing can get messy. With a pipeline, you won't need to manually keep track of your training and validation data at each step. Fewer Bugs: There are fewer opportunities to misapply a step or forget a preprocessing step. Easier to Productionize: It can be surprisingly hard to transition a model from a prototype to something deployable at scale. . More Options for Model Validation: cross-validation. import pandas as pd from sklearn.model_selection import train_test_split # Read the data data = pd . read_csv ( 'https://raw.githubusercontent.com/esabunor/MLWorkspace/master/melb_data.csv' ) # Separate target from predictors y = data . Price X = data . drop ([ 'Price' ], axis = 1 ) # Divide data into training and validation subsets X_train_full , X_valid_full , y_train , y_valid = train_test_split ( X , y , train_size = 0.8 , test_size = 0.2 , random_state = 0 ) # \"Cardinality\" means the number of unique values in a column # Select categorical columns with relatively low cardinality (convenient but arbitrary) categorical_cols = [ cname for cname in X_train_full . columns if X_train_full [ cname ] . nunique () < 10 and X_train_full [ cname ] . dtype == \"object\" ] # Select numerical columns numerical_cols = [ cname for cname in X_train_full . columns if X_train_full [ cname ] . dtype in [ 'int64' , 'float64' ]] # Keep selected columns only my_cols = categorical_cols + numerical_cols X_train = X_train_full [ my_cols ] . copy () X_valid = X_valid_full [ my_cols ] . copy () X_train . head () Type Method Regionname Unnamed: 0 Rooms Distance Postcode Bedroom2 Bathroom Car Landsize BuildingArea YearBuilt Lattitude Longtitude Propertycount 2573 h SP Northern Metropolitan 3349 4 7.8 3058.0 4.0 2.0 1.0 381.0 NaN 1938.0 -37.7337 144.9548 11204.0 2091 h SP Southern Metropolitan 2686 3 7.8 3124.0 3.0 1.0 1.0 544.0 160.0 1930.0 -37.8436 145.0581 8920.0 4683 u S Southern Metropolitan 6065 2 5.6 3101.0 2.0 1.0 1.0 121.0 NaN NaN -37.8126 145.0534 10331.0 8832 h VB Southern Metropolitan 11346 3 7.5 3123.0 3.0 2.0 2.0 200.0 NaN NaN -37.8396 145.0514 6482.0 10469 u S Southern Metropolitan 13474 2 4.5 3181.0 2.0 1.0 1.0 2842.0 84.0 1920.0 -37.8513 144.9943 7717.0 We construct the full pipeline in three steps. Step 1: Define Preprocessing Steps Similar to how a pipeline bundles together preprocessing and modeling steps, we use the ColumnTransformer class to bundle together different preprocessing steps. The code below: imputes missing values in numerical data, and imputes missing values and applies a one-hot encoding to categorical data. from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import OneHotEncoder # Preprocessing for numerical data numerical_transformer = SimpleImputer ( strategy = 'constant' ) # Preprocessing for categorical data categorical_transformer = Pipeline ( steps = [ ( 'imputer' , SimpleImputer ( strategy = 'most_frequent' )), ( 'onehot' , OneHotEncoder ( handle_unknown = 'ignore' )) ]) # Bundle preprocessing for numerical and categorical data preprocessor = ColumnTransformer ( transformers = [ ( 'num' , numerical_transformer , numerical_cols ), ( 'cat' , categorical_transformer , categorical_cols ) ]) Step 2: Define the Model Next, we define a random forest model with the familiar RandomForestRegressor class. from sklearn.ensemble import RandomForestRegressor model = RandomForestRegressor ( n_estimators = 100 , random_state = 0 ) Step 3: Create and Evaluate the Pipeline Finally, we use the Pipeline class to define a pipeline that bundles the preprocessing and modeling steps. There are a few important things to notice: With the pipeline, we preprocess the training data and fit the model in a single line of code. (In contrast, without a pipeline, we have to do imputation, one-hot encoding, and model training in separate steps. This becomes especially messy if we have to deal with both numerical and categorical variables!) With the pipeline, we supply the unprocessed features in X_valid to the predict() command, and the pipeline automatically preprocesses the features before generating predictions. (However, without a pipeline, we have to remember to preprocess the validation data before making predictions.) from sklearn.metrics import mean_absolute_error # Bundle preprocessing and modeling code in a pipeline my_pipeline = Pipeline ( steps = [( 'preprocessor' , preprocessor ), ( 'model' , model ) ]) # Preprocessing of training data, fit model my_pipeline . fit ( X_train , y_train ) # Preprocessing of validation data, get predictions preds = my_pipeline . predict ( X_valid ) # Evaluate the model score = mean_absolute_error ( y_valid , preds ) print ( 'MAE:' , score ) MAE: 177453.58753804347 Now, you'll use your trained model to generate predictions with the test data. # Preprocessing of test data, fit model preds_test = my_pipeline . predict ( X_test ) # Save test predictions to file output = pd . DataFrame ({ 'Id' : X_test . index , 'SalePrice' : preds_test }) output . to_csv ( 'submission.csv' , index = False )","title":"Pipelines"},{"location":"ML/pyPipe/#step-1-define-preprocessing-steps","text":"Similar to how a pipeline bundles together preprocessing and modeling steps, we use the ColumnTransformer class to bundle together different preprocessing steps. The code below: imputes missing values in numerical data, and imputes missing values and applies a one-hot encoding to categorical data. from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import OneHotEncoder # Preprocessing for numerical data numerical_transformer = SimpleImputer ( strategy = 'constant' ) # Preprocessing for categorical data categorical_transformer = Pipeline ( steps = [ ( 'imputer' , SimpleImputer ( strategy = 'most_frequent' )), ( 'onehot' , OneHotEncoder ( handle_unknown = 'ignore' )) ]) # Bundle preprocessing for numerical and categorical data preprocessor = ColumnTransformer ( transformers = [ ( 'num' , numerical_transformer , numerical_cols ), ( 'cat' , categorical_transformer , categorical_cols ) ])","title":"Step 1: Define Preprocessing Steps"},{"location":"ML/pyPipe/#step-2-define-the-model","text":"Next, we define a random forest model with the familiar RandomForestRegressor class. from sklearn.ensemble import RandomForestRegressor model = RandomForestRegressor ( n_estimators = 100 , random_state = 0 )","title":"Step 2: Define the Model"},{"location":"ML/pyPipe/#step-3-create-and-evaluate-the-pipeline","text":"Finally, we use the Pipeline class to define a pipeline that bundles the preprocessing and modeling steps. There are a few important things to notice: With the pipeline, we preprocess the training data and fit the model in a single line of code. (In contrast, without a pipeline, we have to do imputation, one-hot encoding, and model training in separate steps. This becomes especially messy if we have to deal with both numerical and categorical variables!) With the pipeline, we supply the unprocessed features in X_valid to the predict() command, and the pipeline automatically preprocesses the features before generating predictions. (However, without a pipeline, we have to remember to preprocess the validation data before making predictions.) from sklearn.metrics import mean_absolute_error # Bundle preprocessing and modeling code in a pipeline my_pipeline = Pipeline ( steps = [( 'preprocessor' , preprocessor ), ( 'model' , model ) ]) # Preprocessing of training data, fit model my_pipeline . fit ( X_train , y_train ) # Preprocessing of validation data, get predictions preds = my_pipeline . predict ( X_valid ) # Evaluate the model score = mean_absolute_error ( y_valid , preds ) print ( 'MAE:' , score ) MAE: 177453.58753804347 Now, you'll use your trained model to generate predictions with the test data. # Preprocessing of test data, fit model preds_test = my_pipeline . predict ( X_test ) # Save test predictions to file output = pd . DataFrame ({ 'Id' : X_test . index , 'SalePrice' : preds_test }) output . to_csv ( 'submission.csv' , index = False )","title":"Step 3: Create and Evaluate the Pipeline"},{"location":"ML/pyPlot/","text":"Much of plotting you'll do will be through the Matplotlib library, specifically within the Pyplot module. matplotlib.pyplot is a collection of functions that make plotting in python work like MATLAB. Each function makes some change to a figure, e.g., creates a figure, creates a plotting area in a figure, plots lines, annotates the plots with labels, etc. The style can be changed from classic to ggplot, mimicking the aesthetic style used in the R package ggplot2. plt.style.use('ggplot') For all matplotlib plots, first create a figure and an axes object; to show the plot, call \u201cplt.show()\u201d. The figure contains all the objects, including axes, graphics, texts, and labels. The axes is a bounding box with ticks and labels. Think of axes as an individual plot. import matplotlib.pyplot as plt fig = plt . figure () ax = plt . axes () plt . style . use ( 'ggplot' ) plt . show () We can specify the x and y axis labels and a title using plt.xlabel(), plt.ylabel() and plt.title() One can also set the limits of x- and y-axis using plt.xlim() and plt.ylim(), respectively. To create a plot legend labeling each line. We can use the method plt.legend(), in conjunction with specifying labels in the plt.plot(). The color, line style (e.g., solid, dashed, etc.), and position, size, and style of labels can be modified using optional arguments to the function. For more details check the docstring of each function and the matplotlib documentation . Aptly named, the plot function is used to plot 2-D data, as shown below: import numpy as np import matplotlib.pyplot as plt # We'll start with a parabola # Compute the parabola's x and y coordinates x = np . arange ( - 5 , 5 , 0.1 ) y = np . square ( x ) # Use matplotlib for the plot plt . plot ( x , y , 'b' ) # specify the color blue for the line plt . xlabel ( 'X-Axis Values' ) plt . ylabel ( 'Y-Axis Values' ) plt . title ( 'First Plot: A Parabola' ) plt . show () # required to actually display the plot Imshow Another Matplotlib function you'll encounter is imshow which is used to display images. Recall that an image may be considered as an array, with array elements indicating image pixel values. As a simple example, here is the identity matrix: import numpy as np import matplotlib.pyplot as plt X = np . identity ( 10 ) identity_matrix_image = plt . imshow ( X , cmap = \"Greys_r\" ) plt . show () # Now plot a random matrix, with a different colormap A = np . random . randn ( 10 , 10 ) random_matrix_image = plt . imshow ( A ) plt . show () Scatter Plot A scatter plot is used to show all the values from your data on a graph. In order to get a visual representation of our data, we have to limit our data to two features. The purple dots are first class, the green dots are second class, and the yellow dots are third class. import pandas as pd import matplotlib.pyplot as plt df = pd . read_csv ( 'titanic.csv' ) plt . xlabel ( 'Age' ) plt . ylabel ( 'Fare' ) plt . scatter ( df [ 'Age' ], df [ 'Fare' ]) plt . scatter ( df [ 'Age' ], df [ 'Fare' ], c = df [ 'Pclass' ]) Line Plot Now that we can put individual datapoints on a plot, let's see how to draw the line. The plot function does just that. The following draws a line to approximately separate the 1st class from the 2nd and 3rd class. From eyeballing, we\u2019ll put the line from (0, 85) to (80, 5). Our syntax below has a list of the x values and a list of the y values. plt . plot ([ 0 , 80 ], [ 85 , 5 ]) Bar Plot The plot() function can take a kind argument, specifying the type of the plot we want to produce. For bar plots, provide kind=\"bar\". kind=\"barh\" can be used to create a horizontal bar chart. The stacked property can be used to specify if the bars should be stacked on top of each other. import pandas as pd import matplotlib.pyplot as plt df = pd . read_csv ( 'titanic.csv' ) ( df . groupby ( 'Survived' )[ 'Fare' ] . mean ()) . plot ( kind = 'bar' ) Box Plot A box plot is used to visualize the distribution of values in a column, basically visualizing the result of the describe() function. The green line shows the median value. The box shows the upper and lower quartiles (25% of the data is greater or less than these values). The circles show the outliers, while the black lines show the min/max values excluding the outliers. df [ df [ 'Survived' ] == 1 ][ \"Fare\" ] . plot ( kind = \"box\" ) Histogram Similar to box plots, histograms show the distribution of data. Visually histograms are similar to bar charts, however, histograms display frequencies for a group of data rather than an individual data point; therefore, no spaces are present between the bars. Typically, a histogram groups data into chunks (or bins). You can manually specify the number of bins to use using the bins attribute. df [ df [ 'Survived' ] == 1 ][ \"Fare\" ] . plot ( kind = \"hist\" , bins = 10 ) Area Plot df [ df [ 'Survived' ] == 1 ][ \"Fare\" ] . plot ( kind = \"area\" ) Pie Chart Create a pie chart using kind=\"pie\". Pie charts are generally used to show percentage or proportional data. Pie charts are usually used when you have up to 6 categories. df . groupby ( \"Survived\" )[ \"Fare\" ] . sum () . plot ( kind = \"pie\" )","title":"MatplotLib"},{"location":"ML/pyPlot/#imshow","text":"Another Matplotlib function you'll encounter is imshow which is used to display images. Recall that an image may be considered as an array, with array elements indicating image pixel values. As a simple example, here is the identity matrix: import numpy as np import matplotlib.pyplot as plt X = np . identity ( 10 ) identity_matrix_image = plt . imshow ( X , cmap = \"Greys_r\" ) plt . show () # Now plot a random matrix, with a different colormap A = np . random . randn ( 10 , 10 ) random_matrix_image = plt . imshow ( A ) plt . show ()","title":"Imshow"},{"location":"ML/pyPlot/#scatter-plot","text":"A scatter plot is used to show all the values from your data on a graph. In order to get a visual representation of our data, we have to limit our data to two features. The purple dots are first class, the green dots are second class, and the yellow dots are third class. import pandas as pd import matplotlib.pyplot as plt df = pd . read_csv ( 'titanic.csv' ) plt . xlabel ( 'Age' ) plt . ylabel ( 'Fare' ) plt . scatter ( df [ 'Age' ], df [ 'Fare' ]) plt . scatter ( df [ 'Age' ], df [ 'Fare' ], c = df [ 'Pclass' ])","title":"Scatter Plot"},{"location":"ML/pyPlot/#line-plot","text":"Now that we can put individual datapoints on a plot, let's see how to draw the line. The plot function does just that. The following draws a line to approximately separate the 1st class from the 2nd and 3rd class. From eyeballing, we\u2019ll put the line from (0, 85) to (80, 5). Our syntax below has a list of the x values and a list of the y values. plt . plot ([ 0 , 80 ], [ 85 , 5 ])","title":"Line Plot"},{"location":"ML/pyPlot/#bar-plot","text":"The plot() function can take a kind argument, specifying the type of the plot we want to produce. For bar plots, provide kind=\"bar\". kind=\"barh\" can be used to create a horizontal bar chart. The stacked property can be used to specify if the bars should be stacked on top of each other. import pandas as pd import matplotlib.pyplot as plt df = pd . read_csv ( 'titanic.csv' ) ( df . groupby ( 'Survived' )[ 'Fare' ] . mean ()) . plot ( kind = 'bar' )","title":"Bar Plot"},{"location":"ML/pyPlot/#box-plot","text":"A box plot is used to visualize the distribution of values in a column, basically visualizing the result of the describe() function. The green line shows the median value. The box shows the upper and lower quartiles (25% of the data is greater or less than these values). The circles show the outliers, while the black lines show the min/max values excluding the outliers. df [ df [ 'Survived' ] == 1 ][ \"Fare\" ] . plot ( kind = \"box\" )","title":"Box Plot"},{"location":"ML/pyPlot/#histogram","text":"Similar to box plots, histograms show the distribution of data. Visually histograms are similar to bar charts, however, histograms display frequencies for a group of data rather than an individual data point; therefore, no spaces are present between the bars. Typically, a histogram groups data into chunks (or bins). You can manually specify the number of bins to use using the bins attribute. df [ df [ 'Survived' ] == 1 ][ \"Fare\" ] . plot ( kind = \"hist\" , bins = 10 )","title":"Histogram"},{"location":"ML/pyPlot/#area-plot","text":"df [ df [ 'Survived' ] == 1 ][ \"Fare\" ] . plot ( kind = \"area\" )","title":"Area Plot"},{"location":"ML/pyPlot/#pie-chart","text":"Create a pie chart using kind=\"pie\". Pie charts are generally used to show percentage or proportional data. Pie charts are usually used when you have up to 6 categories. df . groupby ( \"Survived\" )[ \"Fare\" ] . sum () . plot ( kind = \"pie\" )","title":"Pie Chart"},{"location":"ML/pyRandomFor/","text":"Decision Trees are very susceptible to random idiosyncrasies in the training dataset. We say that Decision Trees have high variance since if you randomly change the training dataset, you may end up with a very different looking tree. One of the advantages of decision trees over a model like logistic regression is that they make no assumptions about how the data is structured. In logistic regression, we assume that we can draw a line to split the data. Sometimes our data just isn\u2019t structured like that. A decision tree has the potential to get at the essence of the data no matter how it is structured. The goal of random forests is to take the advantages of decision trees while mitigating the variance issues. A random forest is an example of an ensemble because it uses multiple machine learning models to create a single model. Bootstrapping A bootstrapped sample is a random sample of datapoints where we randomly select with replacement datapoints from our original dataset to create a dataset of the same size. Randomly selecting with replacement means that we can choose the same datapoint multiple times. This means that in a bootstrapped sample, some datapoints from the original dataset will appear multiple times and some will not appear at all. For example if we have four datapoints A, B, C, D, these could be 3 resamples: A, A, B, C B, B, B, D A, A, C, C We would rather be able to get more samples of data from the population, but as all we have is our training set, we use that to generate additional datasets. We use bootstrapping to mimic creating multiple samples. Bagging Bootstrap Aggregation (or Bagging) is a technique for reducing the variance in an individual model by creating an ensemble from multiple models built on bootstrapped samples. To bag decision trees, we create multiple (say 10) bootstrapped resamples of our training dataset. So if we have 100 datapoints in our training set, each of the resamples will have 100 datapoints randomly chosen from our training set. Recall that we randomly select with replacement, meaning that some datapoints will appear multiple times and some not at all. We create a decision tree with each of these 10 resamples. To make a prediction, we make a prediction with each of the 10 decision trees and then each decision tree gets a vote. The prediction with the most votes is the final prediction. When we bootstrap the training set, we're trying to wash out the variance of the decision tree. The average of several trees that have different training sets will create a model that more accurately gets at the essence of the data. With bagged decision trees, the trees may still be too similar to have fully created the ideal model. They are built on different resamples, but they all have access to the same features. Thus we will add some restrictions to the model when building each decision tree so the trees have more variation. We call this decorrelating the trees. If you recall, when building a decision tree, at every node, we compare all the split thresholds for each feature to find the single best feature & split threshold. In a decision tree for a random forest, at each node, we randomly select a subset of the features to consider. This will result in us choosing a good, but not the best, feature to split on at each step. It\u2019s important to note that the random selection of features happens at each node. So maybe at the first node we consider the Sex and Fare features and then at the second node, the Fare and Age features. A standard choice for the number of features to consider at each split is the square root of the number of features. So if we have 9 features, we will consider 3 of them at each node (randomly chosen). If we bag these decision trees, we get a random forest. Each decision tree within a random forest is probably worse than a standard decision tree. But when we average them we get a very strong model! import pandas as pd from sklearn.datasets import load_breast_cancer from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split cancer_data = load_breast_cancer () df = pd . DataFrame ( cancer_data [ 'data' ], columns = cancer_data [ 'feature_names' ]) df [ 'target' ] = cancer_data [ 'target' ] X = df [ cancer_data . feature_names ] . values y = df [ 'target' ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 101 ) print ( 'data dimensions' , X . shape ) #569 datapoints, 30 features rf = RandomForestClassifier () rf . fit ( X_train , y_train ) first_row = X_test [ 0 ] print ( \"Prediction: \" , rf . predict ([ first_row ])) print ( \"True value: \" , y_test [ 0 ]) print ( \"RF accuracy:\" , rf . score ( X_test , y_test )) data dimensions (569, 30) Prediction: [1] True value: 1 RF accuracy: 0.972027972027972 Random Forest Parameters One of the big advantages of Random Forests is that they rarely require much tuning. The default values will work well on most datasets. Tuning parameters for prepruning as we did for decision trees: max_depth, min_samples_leaf, and max_leaf_nodes. With random forests, it is generally not important to tune these as overfitting is generally not an issue. Two new tuning parameters: n_estimators (the number of trees) max_features (the number of features to consider at each split). The default for the max features is the square root of p, where p is the number of features (or predictors). The default is generally a good choice for max features and we usually will not need to change it, but you can set it to a fixed number with the following code. rf = RandomForestClassifier ( max_features = 5 ) The default number of estimators (decision trees) is 10. This often works well but may in some cases be too small. You can set it to another number as follows. We will see in the next parts how to choose the best value. rf = RandomForestClassifier ( n_estimators = 15 ) You can add additional parameters, e.g. max_features, and parameter values to the param_grid dictionary to compare more decision trees. from sklearn.model_selection import GridSearchCV param_grid = { 'n_estimators' : [ 10 , 25 , 50 , 75 , 100 ], } rf = RandomForestClassifier () gs = GridSearchCV ( rf , param_grid , cv = 5 ) gs . fit ( X , y ) print ( \"best params:\" , gs . best_params_ ) best params: {'n_estimators': 25} Elbow Graph With a parameter like the number of trees in a random forest, increasing the number of trees will never hurt performance. Increasing the number trees will increase performance until a point where it levels out. The more trees, however, the more complicated the algorithm. A more complicated algorithm is more resource intensive to use. Generally it is worth adding complexity to the model if it improves performance but we do not want to unnecessarily add complexity. We can use what is called an Elbow Graph to find the sweet spot. Elbow Graph is a model that optimizes performance without adding unnecessary complexity. To find the optimal value, let\u2019s do a Grid Search trying all the values from 1 to 100 for n_estimators. n_estimators = list ( range ( 1 , 101 )) param_grid = { 'n_estimators' : n_estimators , } rf = RandomForestClassifier () gs = GridSearchCV ( rf , param_grid , cv = 5 ) gs . fit ( X , y ) Instead of just looking at the best params like we did before, we are going to use the entire result from the grid search. The values are located in the cv_results_ attribute. This is a dictionary with a lot of data, however, we will only need one of the keys: mean_test_score. Let\u2019s pull out these values and store them as a variable. import matplotlib.pyplot as plt scores = gs . cv_results_ [ 'mean_test_score' ] plt . plot ( n_estimators , scores ) plt . xlabel ( \"n_estimators\" ) plt . ylabel ( \"accuracy\" ) plt . xlim ( 0 , 100 ) plt . ylim ( 0.9 , 1 ) plt . show () Now we can build our random forest model with the optimal number of trees. rf = RandomForestClassifier ( n_estimators = 10 ) rf . fit ( X , y ) You\u2019ll see elbow graphs pop up in lots of different situations when we are adding complexity to a model and want to determine the minimal amount of complexity that will yield optimal performance. Feature Selection Random forests provide a straightforward method for feature selection: mean decrease impurity. Recall that a random forest consists of many decision trees, and that for each tree, the node is chosen to split the dataset based on maximum decrease in impurity, typically either Gini impurity or entropy in classification. Thus for a tree, it can be computed how much impurity each feature decreases in a tree. And then for a forest, the impurity decrease from each feature can be averaged. Consider this measure a metric of importance of each feature, we then can rank and select the features according to feature importance. Scikit-learn provides a feature_importances_ variable with the model, which shows the relative importance of each feature. The scores are scaled down so that the sum of all scores is 1. In regression, we calculate the feature importance using variance instead. There is no best feature selection method, at least not universally. Instead, we must discover what works best for the specific problem and leverage the domain expertise to build a good model. Scikit-learn provides an easy way to discover the feature importances. it enables us to train a model faster; it reduces the complexity of a model thus makes it easier to interpret. And if the right subset is chosen, it can improve the accuracy of a model. Choosing the right subset often relies on domain knowledge, some art, and a bit of luck. In our dataset, we happen to notice that features with \"worst\" seem to have higher importances. As a result we are going to build a new model with the selected features and see if it improves accuracy. Here we are able to improve the accuracy using a subset of features, a third of the total features to be exact. This is because we removed some noise and highly correlated features, resulting in an increased accuracy. The advantage of building a better model using less features will be more pronounced when the sample size is large. import pandas as pd from sklearn.datasets import load_breast_cancer from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split cancer_data = load_breast_cancer () df = pd . DataFrame ( cancer_data [ 'data' ], columns = cancer_data [ 'feature_names' ]) df [ 'target' ] = cancer_data [ 'target' ] X = df [ cancer_data . feature_names ] . values y = df [ 'target' ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 101 ) print ( 'data dimensions' , X . shape ) #569 datapoints, 30 features rf = RandomForestClassifier ( n_estimators = 10 , random_state = 111 ) rf . fit ( X_train , y_train ) print ( \"Score 1:\" , rf . score ( X_test , y_test )) #0,96 worst_cols = [ col for col in df . columns if 'worst' in col ] print ( worst_cols ) X_worst = df [ worst_cols ] X_train , X_test , y_train , y_test = train_test_split ( X_worst , y , random_state = 101 ) rf . fit ( X_train , y_train ) print ( \"Score 2:\" , rf . score ( X_test , y_test )) #0,97 data dimensions (569, 30) Score 1: 0.965034965034965 ['worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension'] Score 2: 0.972027972027972 Performance Probably the biggest advantage of Random Forests is that they generally perform well without any tuning. They will also perform decently well on almost every dataset. A linear model, for example, cannot perform well on a dataset that cannot be split with a line. It is not possible to split the following dataset with a line without manipulating the features. However, a random forest will perform just fine on this dataset. When looking to get a benchmark for a new classification problem, it is common practice to start by building a Logistic Regression model and a Random Forest model as these two models both have potential to perform well without any tuning. This will give you values for your metrics to try to beat. Oftentimes it is almost impossible to do better than these benchmarks. Interpretability A random forest has several decision trees, each of which is not a very good model, but when averaged, create an excellent model. Thus Random Forests are not a good choice when looking for interpretability. Computation Random Forests can be a little slow to build, especially if you have a lot of trees in the random forest. Building a random forest involves building 10-100 (usually) decision trees. Each of the decision trees is faster to build than a standard decision tree because of how we do not compare every feature at every split, however given the quantity of decision trees it is often slow to build. Similarly, predicting with a Random Forest will be slower than a Decision Tree since we have to do a prediction with each of the 10-100 decision trees in order to get our final prediction. Random Forests are not the fastest model, but generally this is not a problem since the computational power of computers is a lot.","title":"Random Forest"},{"location":"ML/pyRandomFor/#bootstrapping","text":"A bootstrapped sample is a random sample of datapoints where we randomly select with replacement datapoints from our original dataset to create a dataset of the same size. Randomly selecting with replacement means that we can choose the same datapoint multiple times. This means that in a bootstrapped sample, some datapoints from the original dataset will appear multiple times and some will not appear at all. For example if we have four datapoints A, B, C, D, these could be 3 resamples: A, A, B, C B, B, B, D A, A, C, C We would rather be able to get more samples of data from the population, but as all we have is our training set, we use that to generate additional datasets. We use bootstrapping to mimic creating multiple samples.","title":"Bootstrapping"},{"location":"ML/pyRandomFor/#bagging","text":"Bootstrap Aggregation (or Bagging) is a technique for reducing the variance in an individual model by creating an ensemble from multiple models built on bootstrapped samples. To bag decision trees, we create multiple (say 10) bootstrapped resamples of our training dataset. So if we have 100 datapoints in our training set, each of the resamples will have 100 datapoints randomly chosen from our training set. Recall that we randomly select with replacement, meaning that some datapoints will appear multiple times and some not at all. We create a decision tree with each of these 10 resamples. To make a prediction, we make a prediction with each of the 10 decision trees and then each decision tree gets a vote. The prediction with the most votes is the final prediction. When we bootstrap the training set, we're trying to wash out the variance of the decision tree. The average of several trees that have different training sets will create a model that more accurately gets at the essence of the data. With bagged decision trees, the trees may still be too similar to have fully created the ideal model. They are built on different resamples, but they all have access to the same features. Thus we will add some restrictions to the model when building each decision tree so the trees have more variation. We call this decorrelating the trees. If you recall, when building a decision tree, at every node, we compare all the split thresholds for each feature to find the single best feature & split threshold. In a decision tree for a random forest, at each node, we randomly select a subset of the features to consider. This will result in us choosing a good, but not the best, feature to split on at each step. It\u2019s important to note that the random selection of features happens at each node. So maybe at the first node we consider the Sex and Fare features and then at the second node, the Fare and Age features. A standard choice for the number of features to consider at each split is the square root of the number of features. So if we have 9 features, we will consider 3 of them at each node (randomly chosen). If we bag these decision trees, we get a random forest. Each decision tree within a random forest is probably worse than a standard decision tree. But when we average them we get a very strong model! import pandas as pd from sklearn.datasets import load_breast_cancer from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split cancer_data = load_breast_cancer () df = pd . DataFrame ( cancer_data [ 'data' ], columns = cancer_data [ 'feature_names' ]) df [ 'target' ] = cancer_data [ 'target' ] X = df [ cancer_data . feature_names ] . values y = df [ 'target' ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 101 ) print ( 'data dimensions' , X . shape ) #569 datapoints, 30 features rf = RandomForestClassifier () rf . fit ( X_train , y_train ) first_row = X_test [ 0 ] print ( \"Prediction: \" , rf . predict ([ first_row ])) print ( \"True value: \" , y_test [ 0 ]) print ( \"RF accuracy:\" , rf . score ( X_test , y_test )) data dimensions (569, 30) Prediction: [1] True value: 1 RF accuracy: 0.972027972027972","title":"Bagging"},{"location":"ML/pyRandomFor/#random-forest-parameters","text":"One of the big advantages of Random Forests is that they rarely require much tuning. The default values will work well on most datasets. Tuning parameters for prepruning as we did for decision trees: max_depth, min_samples_leaf, and max_leaf_nodes. With random forests, it is generally not important to tune these as overfitting is generally not an issue. Two new tuning parameters: n_estimators (the number of trees) max_features (the number of features to consider at each split). The default for the max features is the square root of p, where p is the number of features (or predictors). The default is generally a good choice for max features and we usually will not need to change it, but you can set it to a fixed number with the following code. rf = RandomForestClassifier ( max_features = 5 ) The default number of estimators (decision trees) is 10. This often works well but may in some cases be too small. You can set it to another number as follows. We will see in the next parts how to choose the best value. rf = RandomForestClassifier ( n_estimators = 15 ) You can add additional parameters, e.g. max_features, and parameter values to the param_grid dictionary to compare more decision trees. from sklearn.model_selection import GridSearchCV param_grid = { 'n_estimators' : [ 10 , 25 , 50 , 75 , 100 ], } rf = RandomForestClassifier () gs = GridSearchCV ( rf , param_grid , cv = 5 ) gs . fit ( X , y ) print ( \"best params:\" , gs . best_params_ ) best params: {'n_estimators': 25}","title":"Random Forest Parameters"},{"location":"ML/pyRandomFor/#elbow-graph","text":"With a parameter like the number of trees in a random forest, increasing the number of trees will never hurt performance. Increasing the number trees will increase performance until a point where it levels out. The more trees, however, the more complicated the algorithm. A more complicated algorithm is more resource intensive to use. Generally it is worth adding complexity to the model if it improves performance but we do not want to unnecessarily add complexity. We can use what is called an Elbow Graph to find the sweet spot. Elbow Graph is a model that optimizes performance without adding unnecessary complexity. To find the optimal value, let\u2019s do a Grid Search trying all the values from 1 to 100 for n_estimators. n_estimators = list ( range ( 1 , 101 )) param_grid = { 'n_estimators' : n_estimators , } rf = RandomForestClassifier () gs = GridSearchCV ( rf , param_grid , cv = 5 ) gs . fit ( X , y ) Instead of just looking at the best params like we did before, we are going to use the entire result from the grid search. The values are located in the cv_results_ attribute. This is a dictionary with a lot of data, however, we will only need one of the keys: mean_test_score. Let\u2019s pull out these values and store them as a variable. import matplotlib.pyplot as plt scores = gs . cv_results_ [ 'mean_test_score' ] plt . plot ( n_estimators , scores ) plt . xlabel ( \"n_estimators\" ) plt . ylabel ( \"accuracy\" ) plt . xlim ( 0 , 100 ) plt . ylim ( 0.9 , 1 ) plt . show () Now we can build our random forest model with the optimal number of trees. rf = RandomForestClassifier ( n_estimators = 10 ) rf . fit ( X , y ) You\u2019ll see elbow graphs pop up in lots of different situations when we are adding complexity to a model and want to determine the minimal amount of complexity that will yield optimal performance.","title":"Elbow Graph"},{"location":"ML/pyRandomFor/#feature-selection","text":"Random forests provide a straightforward method for feature selection: mean decrease impurity. Recall that a random forest consists of many decision trees, and that for each tree, the node is chosen to split the dataset based on maximum decrease in impurity, typically either Gini impurity or entropy in classification. Thus for a tree, it can be computed how much impurity each feature decreases in a tree. And then for a forest, the impurity decrease from each feature can be averaged. Consider this measure a metric of importance of each feature, we then can rank and select the features according to feature importance. Scikit-learn provides a feature_importances_ variable with the model, which shows the relative importance of each feature. The scores are scaled down so that the sum of all scores is 1. In regression, we calculate the feature importance using variance instead. There is no best feature selection method, at least not universally. Instead, we must discover what works best for the specific problem and leverage the domain expertise to build a good model. Scikit-learn provides an easy way to discover the feature importances. it enables us to train a model faster; it reduces the complexity of a model thus makes it easier to interpret. And if the right subset is chosen, it can improve the accuracy of a model. Choosing the right subset often relies on domain knowledge, some art, and a bit of luck. In our dataset, we happen to notice that features with \"worst\" seem to have higher importances. As a result we are going to build a new model with the selected features and see if it improves accuracy. Here we are able to improve the accuracy using a subset of features, a third of the total features to be exact. This is because we removed some noise and highly correlated features, resulting in an increased accuracy. The advantage of building a better model using less features will be more pronounced when the sample size is large. import pandas as pd from sklearn.datasets import load_breast_cancer from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split cancer_data = load_breast_cancer () df = pd . DataFrame ( cancer_data [ 'data' ], columns = cancer_data [ 'feature_names' ]) df [ 'target' ] = cancer_data [ 'target' ] X = df [ cancer_data . feature_names ] . values y = df [ 'target' ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 101 ) print ( 'data dimensions' , X . shape ) #569 datapoints, 30 features rf = RandomForestClassifier ( n_estimators = 10 , random_state = 111 ) rf . fit ( X_train , y_train ) print ( \"Score 1:\" , rf . score ( X_test , y_test )) #0,96 worst_cols = [ col for col in df . columns if 'worst' in col ] print ( worst_cols ) X_worst = df [ worst_cols ] X_train , X_test , y_train , y_test = train_test_split ( X_worst , y , random_state = 101 ) rf . fit ( X_train , y_train ) print ( \"Score 2:\" , rf . score ( X_test , y_test )) #0,97 data dimensions (569, 30) Score 1: 0.965034965034965 ['worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension'] Score 2: 0.972027972027972","title":"Feature Selection"},{"location":"ML/pyRandomFor/#performance","text":"Probably the biggest advantage of Random Forests is that they generally perform well without any tuning. They will also perform decently well on almost every dataset. A linear model, for example, cannot perform well on a dataset that cannot be split with a line. It is not possible to split the following dataset with a line without manipulating the features. However, a random forest will perform just fine on this dataset. When looking to get a benchmark for a new classification problem, it is common practice to start by building a Logistic Regression model and a Random Forest model as these two models both have potential to perform well without any tuning. This will give you values for your metrics to try to beat. Oftentimes it is almost impossible to do better than these benchmarks.","title":"Performance"},{"location":"ML/pyRandomFor/#interpretability","text":"A random forest has several decision trees, each of which is not a very good model, but when averaged, create an excellent model. Thus Random Forests are not a good choice when looking for interpretability.","title":"Interpretability"},{"location":"ML/pyRandomFor/#computation","text":"Random Forests can be a little slow to build, especially if you have a lot of trees in the random forest. Building a random forest involves building 10-100 (usually) decision trees. Each of the decision trees is faster to build than a standard decision tree because of how we do not compare every feature at every split, however given the quantity of decision trees it is often slow to build. Similarly, predicting with a Random Forest will be slower than a Decision Tree since we have to do a prediction with each of the 10-100 decision trees in order to get our final prediction. Random Forests are not the fastest model, but generally this is not a problem since the computational power of computers is a lot.","title":"Computation"},{"location":"ML/pyknn/","text":"K nearest neighbors (knn) is a supervised machine learning model that takes a data point, looks at its 'k' closest labeled data points, and assigns the label by a majority vote. Here we see that changing k could affect the output of the model. In knn, k is a hyperparameter. A hyperparameter in machine learning is a parameter whose value is set before the learning process begins. K nearest neighbors can also be used for regression problems. The difference lies in prediction. Instead of a majority vote, knn for regression makes a prediction using the mean labels of the k closest data points. The famous iris database, first used by Sir R. A. Fisher, is perhaps the best known dataset to be found in pattern recognition literature. There are 150 iris plants, each with 4 numeric attributes: sepal length in cm, sepal width in cm, petal length in cm, and petal width in cm. import pandas as pd from sklearn import datasets #iris = pd.read_csv('./data/iris.csv') # import some data to play with iris = datasets . load_iris () df_iris = pd . DataFrame ( iris . data , columns = iris . feature_names ) df_iris . shape (150, 4) df_iris . head () sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 df_iris . describe () sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 std 0.828066 0.435866 1.765298 0.762238 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 print ( df_iris . columns ) Index(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], dtype='object') import matplotlib.pyplot as plt df_iris . hist () plt . show () In classifications, stratified sampling is often chosen to ensure that the train and test sets have approximately the same percentage of samples of each target class as the complete set. from sklearn.neighbors import KNeighborsClassifier X = df_iris [[ 'petal length (cm)' , 'petal width (cm)' ]] y = iris . target from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.30 , random_state = 1 , stratify = y ) knn = KNeighborsClassifier ( n_neighbors = 5 ) knn . fit ( X_train , y_train ) KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights='uniform') pred = knn . predict ( X_test ) #First five predictions pred [: 5 ] array([2, 0, 0, 1, 1]) Probablility Prediction Of all classification algorithms implemented in scikit learn, there is an additional method 'predict_prob'. Instead of splitting the label, it outputs the probability for the target in array form. y_pred_prob = knn . predict_proba ( X_test ) y_pred_prob [ 10 : 12 ] array([[1. , 0. , 0. ], [0. , 0.2, 0.8]]) For example, the probability of the 11th flower being predicted an iris-setosa is 1, an iris-versicolor and an iris-virginica are both 0. For the next flower, there is a 20% chance that it would be classified as iris-versicolor but 80% chance to be iris-virginica. What it tells us is that of the five nearest neighbours of the 12th flower in the testing set, 1 is an iris-versicolor, the rest 4 are iris-virginica. Accuracy knn . score ( X_test , y_test ) 0.9777777777777777 Under the module sklearn.metrics, function accuracy_score(y_true, y_pred) does the same calculation. Confusion Matrix from sklearn.metrics import confusion_matrix from sklearn.metrics import plot_confusion_matrix confusion_matrix ( y_test , pred ) plot_confusion_matrix ( knn , X_test , y_test ) <sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fd70c24bcd0> K-fold Cross Validation Previously we made train-test split before fitting the model so that we can report the model performance on the test data. This is a simple kind of cross validation technique, also known as the holdout method. However, the split is random, as a result, model performance can be sensitive to how the data is split. To overcome this, we introduce k-fold cross validation. In k fold cross validation, the data is divided into k subsets. Then the holdout method is repeated k times, such that each time, one of the k subsets is used as the test set and the other k-1 subsets are combined to train the model. Then the accuracy is averaged over k trials to provide total effectiveness of the model. In this way, all data points are used; and there are more metrics so we don\u2019t rely on one test data for model performance evaluation. The simplest way to use k-fold cross-validation in scikit-learn is to call the cross_val_score function on the model and the dataset. from sklearn.model_selection import cross_val_score # create a new KNN model knn_cv = KNeighborsClassifier ( n_neighbors = 3 ) # train model with 5-fold cv cv_scores = cross_val_score ( knn_cv , X , y , cv = 5 ) #print each cv score (accuracy) print ( cv_scores ) [0.96666667 0.96666667 0.9 0.93333333 1. #then, average cv_scores . mean () 0.9533333333333334 As a general rule, 5-fold or 10-fold cross validation is preferred; but there is no formal rule. As k gets larger, the difference in size between the training set and the resampling subsets gets smaller. As this difference decreases, the bias of the technique becomes smaller. Grid Search When we built our first knn model, we set the hyperparameter k to 5, and then to 3 later in k-fold cross validation; random choices really. What is the best k? Finding the optimal k is called tuning the hyperparameter. A handy tool is grid search. In scikit-learn, we use GridSearchCV, which trains our model multiple times on a range of values specified with the param_grid parameter and computes cross validation score, so that we can check which of our values for the tested hyperparameter performed the best. from sklearn.model_selection import GridSearchCV import numpy as np # create new a knn model knn2 = KNeighborsClassifier () # create a dict of all values we want to test for n_neighbors param_grid = { 'n_neighbors' : np . arange ( 2 , 10 )} # use gridsearch to test all values for n_neighbors knn_gscv = GridSearchCV ( knn2 , param_grid , cv = 5 ) #fit model to data knn_gscv . fit ( X , y ) #To check the top performing n_neighbors value knn_gscv . best_params_ {'n_neighbors': 4} #We can see that 4 is the best value for n_neighbors. #What is the accuracy of the model when k is 4 knn_gscv . best_score_ 0.9666666666666668 By using grid search to find the optimal hyperparameter for our model, it improves the model accuracy by over 1%. knn_final = KNeighborsClassifier ( n_neighbors = knn_gscv . best_params_ [ 'n_neighbors' ]) knn_final . fit ( X , y ) y_pred = knn_final . predict ( X ) knn_final . score ( X , y ) 0.9733333333333334 We can report that our final model, 4nn, has an accuracy of 97.3% in predicting the species of iris! The techniques of k-fold cross validation and tuning parameters with grid search is applicable to both classification and regression problems. Label Prediction with New Data new_data = np . array ([ 3.76 , 1.20 ]) new_data = new_data . reshape ( 1 , - 1 ) knn_final . predict ( np . array ( new_data )) array([1]) Our model predicts that this iris is a versicolor.","title":"K-Nearest Neighbors"},{"location":"ML/pyknn/#probablility-prediction","text":"Of all classification algorithms implemented in scikit learn, there is an additional method 'predict_prob'. Instead of splitting the label, it outputs the probability for the target in array form. y_pred_prob = knn . predict_proba ( X_test ) y_pred_prob [ 10 : 12 ] array([[1. , 0. , 0. ], [0. , 0.2, 0.8]]) For example, the probability of the 11th flower being predicted an iris-setosa is 1, an iris-versicolor and an iris-virginica are both 0. For the next flower, there is a 20% chance that it would be classified as iris-versicolor but 80% chance to be iris-virginica. What it tells us is that of the five nearest neighbours of the 12th flower in the testing set, 1 is an iris-versicolor, the rest 4 are iris-virginica.","title":"Probablility Prediction"},{"location":"ML/pyknn/#accuracy","text":"knn . score ( X_test , y_test ) 0.9777777777777777 Under the module sklearn.metrics, function accuracy_score(y_true, y_pred) does the same calculation.","title":"Accuracy"},{"location":"ML/pyknn/#confusion-matrix","text":"from sklearn.metrics import confusion_matrix from sklearn.metrics import plot_confusion_matrix confusion_matrix ( y_test , pred ) plot_confusion_matrix ( knn , X_test , y_test ) <sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fd70c24bcd0>","title":"Confusion Matrix"},{"location":"ML/pyknn/#k-fold-cross-validation","text":"Previously we made train-test split before fitting the model so that we can report the model performance on the test data. This is a simple kind of cross validation technique, also known as the holdout method. However, the split is random, as a result, model performance can be sensitive to how the data is split. To overcome this, we introduce k-fold cross validation. In k fold cross validation, the data is divided into k subsets. Then the holdout method is repeated k times, such that each time, one of the k subsets is used as the test set and the other k-1 subsets are combined to train the model. Then the accuracy is averaged over k trials to provide total effectiveness of the model. In this way, all data points are used; and there are more metrics so we don\u2019t rely on one test data for model performance evaluation. The simplest way to use k-fold cross-validation in scikit-learn is to call the cross_val_score function on the model and the dataset. from sklearn.model_selection import cross_val_score # create a new KNN model knn_cv = KNeighborsClassifier ( n_neighbors = 3 ) # train model with 5-fold cv cv_scores = cross_val_score ( knn_cv , X , y , cv = 5 ) #print each cv score (accuracy) print ( cv_scores ) [0.96666667 0.96666667 0.9 0.93333333 1. #then, average cv_scores . mean () 0.9533333333333334 As a general rule, 5-fold or 10-fold cross validation is preferred; but there is no formal rule. As k gets larger, the difference in size between the training set and the resampling subsets gets smaller. As this difference decreases, the bias of the technique becomes smaller.","title":"K-fold Cross Validation"},{"location":"ML/pyknn/#grid-search","text":"When we built our first knn model, we set the hyperparameter k to 5, and then to 3 later in k-fold cross validation; random choices really. What is the best k? Finding the optimal k is called tuning the hyperparameter. A handy tool is grid search. In scikit-learn, we use GridSearchCV, which trains our model multiple times on a range of values specified with the param_grid parameter and computes cross validation score, so that we can check which of our values for the tested hyperparameter performed the best. from sklearn.model_selection import GridSearchCV import numpy as np # create new a knn model knn2 = KNeighborsClassifier () # create a dict of all values we want to test for n_neighbors param_grid = { 'n_neighbors' : np . arange ( 2 , 10 )} # use gridsearch to test all values for n_neighbors knn_gscv = GridSearchCV ( knn2 , param_grid , cv = 5 ) #fit model to data knn_gscv . fit ( X , y ) #To check the top performing n_neighbors value knn_gscv . best_params_ {'n_neighbors': 4} #We can see that 4 is the best value for n_neighbors. #What is the accuracy of the model when k is 4 knn_gscv . best_score_ 0.9666666666666668 By using grid search to find the optimal hyperparameter for our model, it improves the model accuracy by over 1%. knn_final = KNeighborsClassifier ( n_neighbors = knn_gscv . best_params_ [ 'n_neighbors' ]) knn_final . fit ( X , y ) y_pred = knn_final . predict ( X ) knn_final . score ( X , y ) 0.9733333333333334 We can report that our final model, 4nn, has an accuracy of 97.3% in predicting the species of iris! The techniques of k-fold cross validation and tuning parameters with grid search is applicable to both classification and regression problems.","title":"Grid Search"},{"location":"ML/pyknn/#label-prediction-with-new-data","text":"new_data = np . array ([ 3.76 , 1.20 ]) new_data = new_data . reshape ( 1 , - 1 ) knn_final . predict ( np . array ( new_data )) array([1]) Our model predicts that this iris is a versicolor.","title":"Label Prediction with New Data"},{"location":"ML/pylinReg/","text":"In scikit-learn, models require a two-dimensional feature matrix (X, 2darray or a pandas DataFrame) and a one-dimensional target array (Y). Here we define the feature matrix as the column RM in boston and assign it to X. Note the double brackets around 'RM' in the code below, it is to ensure the result remains a DataFrame, a 2-dimensional data structure Similarly, we define our target to be the column MEDV in boston and assign it in a variable called Y. X = boston [[ 'RM' ]] print ( X . shape ) (506, 1) Y = boston [ 'MEDV' ] print ( Y . shape ) (506,) Recall that the single bracket outputs a Pandas Series, while a double bracket outputs a Pandas DataFrame, and the model expects the feature matrix X to be a 2darray. Univariate Linear Regression from sklearn.linear_model import LinearRegression model = LinearRegression () from sklearn.model_selection import train_test_split X_train , X_test , Y_train , Y_test = train_test_split ( X , Y , test_size = 0.3 , random_state = 1 ) print ( X_train . shape ) print ( X_test . shape ) print ( Y_train . shape ) print ( Y_test . shape ) (354, 1) (152, 1) (354,) (152,) To get an objective assessment on model\u2019s predictive power, it\u2019s important to keep the testing data unseen to the built model. model . fit ( X_train , Y_train ) LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) Fitting is equal to training. It fits the model to the training data and finds the coefficients specified in the linear regression model, i.e., intercept and slope. After it is trained, the model can be used to make predictions. Parameter Estimates The linear regression model has been fitted, what it means is that both parameters, the intercept and the slope, have been learned. What are they? In Scikit-learn, by convention all model parameters have trailing underscores, for example to access the estimated intercept from the model, rounded to the 2nd decimal place for better display. print ( model . intercept_ . round ( 2 )) print ( model . coef_ . round ( 2 )) -30.57 [8.46] The two parameters represent the intercept and slope of the line fit to the data. Our fitted model is MEDV = -30.57 + 8.46 * RM. For one unit increase in RM, the median home price would go up by $8460. Prediction The predict() method estimates the median home value by computing model.intercept_ + model.coef_*RM. Note that the input has to be 2-dimensional, either a 2darray or DataFrame will work in this case. import numpy as np new_RM = np . array ([ 6.5 ]) . reshape ( - 1 , 1 ) model . predict ( new_RM ) array([24.42606323]) In addition, we can feed the testing set and get predictions for all homes. y_test_predicted = model . predict ( X_test ) print ( y_test_predicted . shape ) type ( y_test_predicted ) (152,) numpy.ndarray Evaluating the Model plt . scatter ( X_test , Y_test , label = 'testing data' ) plt . plot ( X_test , y_test_predicted , label = 'prediction' , linewidth = 3 ) plt . xlabel = ( 'RM' ) plt . ylabel ( 'MEDV' ) plt . legend ( loc = 'upper left' ) plt . show () We can measure the distance between a point to the line along the vertical line, and this distance is referred to as residual or error. A residual is the difference between the observed value of the target and the predicted value. The closer the residual is to 0, the better job our model is doing. Residual plots can reveal bias from the model and statistical measures indicate goodness-of-fit. import matplotlib.pyplot as plt residuals = Y_test - y_test_predicted #plot residuals plt . scatter ( X_test , residuals ) #plot horizontal line at 0 plt . hlines ( y = 0 , xmin = X_test . min (), xmax = X_test . max (), linestyles = '--' ) #set xlim plt . xlim (( 4 , 9 )) plt . ylabel ( 'Residuals' ) plt . show () We can also use the mean_squared_error() method under scikit-learn metrics from sklearn.metrics import mean_squared_error mse = mean_squared_error ( Y_test , y_test_predicted ) print ( mse ) 36.517214730838624 In general, the smaller the MSE, the better, yet there is no absolute good or bad threshold. We can define it based on the dependent variable, i.e., MEDV in the test set. Y_test ranges from 6.3 to 50 with a variance 92.26. Compared to the total variance, a MSE of 36.52 is not bad. To make the scale of errors to be the same as the scale of targets, root mean squared error (RMSE) is often used. It is the square root of MSE. R-squared It is the proportion of total variation explained by the model. Here, around 60% of variability in the testing data is explained by our model. Evaluating R-squared values in conjunction with residual plots quantifies model performance. model . score ( X_test , Y_test ) 0.6015774471545622 Multivariate Linear Regression The extension from univariate to multivariate linear regression is straightforward in scikit-learn. The model instantiation, fitting, and predictions are identical, the only difference being the data preparation. #Data preparation X2 = boston [[ 'RM' , 'LSTAT' ]] Y = boston [ 'MEDV' ] #Trian test split #Same random split to ensure The same splits X2_train , X2_test , Y_train , Y_test = train_test_split ( X2 , Y , test_size = 0.3 , random_state = 1 ) model2 = LinearRegression () model2 . fit ( X2_train , Y_train ) print ( model2 . intercept_ ) print ( model2 . coef_ ) y_test_predicted2 = model2 . predict ( X2_test ) mean_squared_error ( Y_test , y_test_predicted2 ) 5.316818471096063 [ 4.12674118 -0.67762654] 28.93449134108657 In general, the more features the model includes the lower the MSE would be. Yet be careful about including too many features. Some features could be random noise, thus hurt the interpretability of the model.","title":"Linear Regression"},{"location":"ML/pylinReg/#univariate-linear-regression","text":"from sklearn.linear_model import LinearRegression model = LinearRegression () from sklearn.model_selection import train_test_split X_train , X_test , Y_train , Y_test = train_test_split ( X , Y , test_size = 0.3 , random_state = 1 ) print ( X_train . shape ) print ( X_test . shape ) print ( Y_train . shape ) print ( Y_test . shape ) (354, 1) (152, 1) (354,) (152,) To get an objective assessment on model\u2019s predictive power, it\u2019s important to keep the testing data unseen to the built model. model . fit ( X_train , Y_train ) LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) Fitting is equal to training. It fits the model to the training data and finds the coefficients specified in the linear regression model, i.e., intercept and slope. After it is trained, the model can be used to make predictions.","title":"Univariate Linear Regression"},{"location":"ML/pylinReg/#parameter-estimates","text":"The linear regression model has been fitted, what it means is that both parameters, the intercept and the slope, have been learned. What are they? In Scikit-learn, by convention all model parameters have trailing underscores, for example to access the estimated intercept from the model, rounded to the 2nd decimal place for better display. print ( model . intercept_ . round ( 2 )) print ( model . coef_ . round ( 2 )) -30.57 [8.46] The two parameters represent the intercept and slope of the line fit to the data. Our fitted model is MEDV = -30.57 + 8.46 * RM. For one unit increase in RM, the median home price would go up by $8460.","title":"Parameter Estimates"},{"location":"ML/pylinReg/#prediction","text":"The predict() method estimates the median home value by computing model.intercept_ + model.coef_*RM. Note that the input has to be 2-dimensional, either a 2darray or DataFrame will work in this case. import numpy as np new_RM = np . array ([ 6.5 ]) . reshape ( - 1 , 1 ) model . predict ( new_RM ) array([24.42606323]) In addition, we can feed the testing set and get predictions for all homes. y_test_predicted = model . predict ( X_test ) print ( y_test_predicted . shape ) type ( y_test_predicted ) (152,) numpy.ndarray","title":"Prediction"},{"location":"ML/pylinReg/#evaluating-the-model","text":"plt . scatter ( X_test , Y_test , label = 'testing data' ) plt . plot ( X_test , y_test_predicted , label = 'prediction' , linewidth = 3 ) plt . xlabel = ( 'RM' ) plt . ylabel ( 'MEDV' ) plt . legend ( loc = 'upper left' ) plt . show () We can measure the distance between a point to the line along the vertical line, and this distance is referred to as residual or error. A residual is the difference between the observed value of the target and the predicted value. The closer the residual is to 0, the better job our model is doing. Residual plots can reveal bias from the model and statistical measures indicate goodness-of-fit. import matplotlib.pyplot as plt residuals = Y_test - y_test_predicted #plot residuals plt . scatter ( X_test , residuals ) #plot horizontal line at 0 plt . hlines ( y = 0 , xmin = X_test . min (), xmax = X_test . max (), linestyles = '--' ) #set xlim plt . xlim (( 4 , 9 )) plt . ylabel ( 'Residuals' ) plt . show () We can also use the mean_squared_error() method under scikit-learn metrics from sklearn.metrics import mean_squared_error mse = mean_squared_error ( Y_test , y_test_predicted ) print ( mse ) 36.517214730838624 In general, the smaller the MSE, the better, yet there is no absolute good or bad threshold. We can define it based on the dependent variable, i.e., MEDV in the test set. Y_test ranges from 6.3 to 50 with a variance 92.26. Compared to the total variance, a MSE of 36.52 is not bad. To make the scale of errors to be the same as the scale of targets, root mean squared error (RMSE) is often used. It is the square root of MSE. R-squared It is the proportion of total variation explained by the model. Here, around 60% of variability in the testing data is explained by our model. Evaluating R-squared values in conjunction with residual plots quantifies model performance. model . score ( X_test , Y_test ) 0.6015774471545622","title":"Evaluating the Model"},{"location":"ML/pylinReg/#multivariate-linear-regression","text":"The extension from univariate to multivariate linear regression is straightforward in scikit-learn. The model instantiation, fitting, and predictions are identical, the only difference being the data preparation. #Data preparation X2 = boston [[ 'RM' , 'LSTAT' ]] Y = boston [ 'MEDV' ] #Trian test split #Same random split to ensure The same splits X2_train , X2_test , Y_train , Y_test = train_test_split ( X2 , Y , test_size = 0.3 , random_state = 1 ) model2 = LinearRegression () model2 . fit ( X2_train , Y_train ) print ( model2 . intercept_ ) print ( model2 . coef_ ) y_test_predicted2 = model2 . predict ( X2_test ) mean_squared_error ( Y_test , y_test_predicted2 ) 5.316818471096063 [ 4.12674118 -0.67762654] 28.93449134108657 In general, the more features the model includes the lower the MSE would be. Yet be careful about including too many features. Some features could be random noise, thus hurt the interpretability of the model.","title":"Multivariate Linear Regression"},{"location":"PR/pyAsser/","text":"An assertion is a sanity-check that you can turn on or turn off when you have finished testing the program. An expression is tested, and if the result comes up false, an exception is raised. Assertions are carried out through use of the assert statement. Programmers often place assertions at the start of a function to check for valid input, and after a function call to check for valid output. print ( 1 ) assert 2 + 2 == 4 print ( 2 ) assert 1 + 1 == 3 print ( 3 ) 1 2 --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-77-c678df5c3c33> in <module>() 2 assert 2 + 2 == 4 3 print (2) ----> 4 assert 1 + 1 == 3 5 print (3) AssertionError: The assert can take a second argument that is passed to the AssertionError raised if the assertion fails. AssertionError exceptions can be caught and handled like any other exception using the try-except statement, but if not handled, this type of exception will terminate the program. temp =- 10 assert ( temp >= 0 ), \"Colder than absolute zero!\" --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-78-f713a352d793> in <module>() 1 temp=-10 ----> 2 assert (temp>=0), \"Colder than absolute zero!\" AssertionError: Colder than absolute zero!","title":"Assertions"},{"location":"PR/pyCompr/","text":"Comprehensions are a handy way to run a loop within a single line of code and to collect the results of the loop in a collection such as a list List Comprehension List comprehensions are a useful way of quickly creating lists whose contents obey a simple rule. Python allows for list comprehension in which the elements of a list are iterated over all in one line of code. List comprehensions are inspired by set-builder notation in mathematics. even_list = [ 2 , 4 , 6 , 8 ] odd_list = [ even + 1 for even in even_list ] print ( odd_list ) cubes = [ i ** 3 for i in range ( 5 )] print ( cubes ) [3, 5, 7, 9] [0, 1, 8, 27, 64] Note from above the similarities between list comprehension and a for-loop; Python has list comprehension as a compact, \"pythonic\" way of performing operations that could be done within a for-loop. A list comprehension can also contain an if statement to enforce a condition on values in the list. a = [ i ** 2 for i in range ( 10 )] evens = [ i ** 2 for i in range ( 10 ) if i ** 2 % 2 == 0 ] print ( a ) print ( evens ) [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] [0, 4, 16, 36, 64] Trying to create a list in a very extensive range will result in a MemoryError. This code shows an example where the list comprehension runs out of memory. This issue is solved by generators. results = [] for city , year in zip ( cities , years ): #int () needed because year is a string if int ( year ) > 1945 : results . append ( city + ': ' + year ) To do it better and more Pythonic Use List comprehensions #List comprehension: # [f(element) for element in iterator if condition(element)] results = [( city + ': ' + year ) for city , year in zip ( cities , years ) if int ( year ) > 1945 ] results = [( city + ': ' + year ) for city , year in zip ( cities , years ) if int ( year ) > 1945 ] Dictionary comprehension cities_by_year = { year : city for city , year in zip ( cities , years )} # Demonstrate how to use dictionary comprehensions def main (): # define a list of temperature values ctemps = [ 0 , 12 , 34 , 100 ] # Use a comprehension to build a dictionary tempDict = { t : ( t * 9 / 5 ) + 32 for t in ctemps if t < 100 } print ( tempDict ) print ( tempDict [ 12 ]) # Merge two dictionaries with a comprehension team1 = { \"Jones\" : 24 , \"Jameson\" : 18 , \"Smith\" : 58 , \"Burns\" : 7 } team2 = { \"White\" : 12 , \"Macke\" : 88 , \"Perce\" : 4 } newTeam = { k : v for team in ( team1 , team2 ) for k , v in team . items ()} print ( newTeam ) if __name__ == \"__main__\" : main () {0: 32.0, 12: 53.6, 34: 93.2} 53.6 {'Jones': 24, 'Jameson': 18, 'Smith': 58, 'Burns': 7, 'White': 12, 'Macke': 88, 'Perce': 4} Set Comprehension cities_after_1930 = { city for year , city in cities_by_year . items () if int ( year ) > 1930 } def main (): # define a list of temperature data points ctemps = [ 5 , 10 , 12 , 14 , 10 , 23 , 41 , 30 , 12 , 24 , 12 , 18 , 29 ] # build a set of unique Fahrenheit temperatures ftemps1 = [( t * 9 / 5 ) + 32 for t in ctemps ] ftemps2 = {( t * 9 / 5 ) + 32 for t in ctemps } print ( ftemps1 ) print ( ftemps2 ) # build a set from an input source sTemp = \"The quick brown fox jumped over the lazy dog\" chars = { c . upper () for c in sTemp if not c . isspace ()} print ( chars ) if __name__ == \"__main__\" : main () [41.0, 50.0, 53.6, 57.2, 50.0, 73.4, 105.8, 86.0, 53.6, 75.2, 53.6, 64.4, 84.2] {64.4, 73.4, 41.0, 105.8, 75.2, 50.0, 84.2, 53.6, 86.0, 57.2} {'G', 'M', 'E', 'B', 'J', 'U', 'R', 'N', 'W', 'P', 'Y', 'T', 'O', 'H', 'Z', 'Q', 'V', 'X', 'F', 'K', 'L', 'D', 'I', 'A', 'C'}","title":"Comprehensions"},{"location":"PR/pyCompr/#list-comprehension","text":"List comprehensions are a useful way of quickly creating lists whose contents obey a simple rule. Python allows for list comprehension in which the elements of a list are iterated over all in one line of code. List comprehensions are inspired by set-builder notation in mathematics. even_list = [ 2 , 4 , 6 , 8 ] odd_list = [ even + 1 for even in even_list ] print ( odd_list ) cubes = [ i ** 3 for i in range ( 5 )] print ( cubes ) [3, 5, 7, 9] [0, 1, 8, 27, 64] Note from above the similarities between list comprehension and a for-loop; Python has list comprehension as a compact, \"pythonic\" way of performing operations that could be done within a for-loop. A list comprehension can also contain an if statement to enforce a condition on values in the list. a = [ i ** 2 for i in range ( 10 )] evens = [ i ** 2 for i in range ( 10 ) if i ** 2 % 2 == 0 ] print ( a ) print ( evens ) [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] [0, 4, 16, 36, 64] Trying to create a list in a very extensive range will result in a MemoryError. This code shows an example where the list comprehension runs out of memory. This issue is solved by generators. results = [] for city , year in zip ( cities , years ): #int () needed because year is a string if int ( year ) > 1945 : results . append ( city + ': ' + year ) To do it better and more Pythonic Use List comprehensions #List comprehension: # [f(element) for element in iterator if condition(element)] results = [( city + ': ' + year ) for city , year in zip ( cities , years ) if int ( year ) > 1945 ] results = [( city + ': ' + year ) for city , year in zip ( cities , years ) if int ( year ) > 1945 ]","title":"List Comprehension"},{"location":"PR/pyCompr/#dictionary-comprehension","text":"cities_by_year = { year : city for city , year in zip ( cities , years )} # Demonstrate how to use dictionary comprehensions def main (): # define a list of temperature values ctemps = [ 0 , 12 , 34 , 100 ] # Use a comprehension to build a dictionary tempDict = { t : ( t * 9 / 5 ) + 32 for t in ctemps if t < 100 } print ( tempDict ) print ( tempDict [ 12 ]) # Merge two dictionaries with a comprehension team1 = { \"Jones\" : 24 , \"Jameson\" : 18 , \"Smith\" : 58 , \"Burns\" : 7 } team2 = { \"White\" : 12 , \"Macke\" : 88 , \"Perce\" : 4 } newTeam = { k : v for team in ( team1 , team2 ) for k , v in team . items ()} print ( newTeam ) if __name__ == \"__main__\" : main () {0: 32.0, 12: 53.6, 34: 93.2} 53.6 {'Jones': 24, 'Jameson': 18, 'Smith': 58, 'Burns': 7, 'White': 12, 'Macke': 88, 'Perce': 4}","title":"Dictionary comprehension"},{"location":"PR/pyCompr/#set-comprehension","text":"cities_after_1930 = { city for year , city in cities_by_year . items () if int ( year ) > 1930 } def main (): # define a list of temperature data points ctemps = [ 5 , 10 , 12 , 14 , 10 , 23 , 41 , 30 , 12 , 24 , 12 , 18 , 29 ] # build a set of unique Fahrenheit temperatures ftemps1 = [( t * 9 / 5 ) + 32 for t in ctemps ] ftemps2 = {( t * 9 / 5 ) + 32 for t in ctemps } print ( ftemps1 ) print ( ftemps2 ) # build a set from an input source sTemp = \"The quick brown fox jumped over the lazy dog\" chars = { c . upper () for c in sTemp if not c . isspace ()} print ( chars ) if __name__ == \"__main__\" : main () [41.0, 50.0, 53.6, 57.2, 50.0, 73.4, 105.8, 86.0, 53.6, 75.2, 53.6, 64.4, 84.2] {64.4, 73.4, 41.0, 105.8, 75.2, 50.0, 84.2, 53.6, 86.0, 57.2} {'G', 'M', 'E', 'B', 'J', 'U', 'R', 'N', 'W', 'P', 'Y', 'T', 'O', 'H', 'Z', 'Q', 'V', 'X', 'F', 'K', 'L', 'D', 'I', 'A', 'C'}","title":"Set Comprehension"},{"location":"PR/pyCond/","text":"Oftentimes while programming, one will want to only execute portions of code when certain conditions are met, for instance, when a variable has a certain value. This is accomplished using conditional statements: if, elif, and else. Indentation is used to define the level of nesting, for nested conditionals. Multiple if/else statements make the code long and not very readable. The elif statement is equivalent to an else/if statement. It is used to make the code shorter, more readable, and avoid indentation increase. for i in range ( 10 ): if i % 2 == 0 : # % -- modulus operator -- returns the remainder after division print ( \" {} is even\" . format ( i )) else : print ( \" {} is odd\" . format ( i )) 0 is even 1 is odd 2 is even 3 is odd 4 is even 5 is odd 6 is even 7 is odd 8 is even 9 is odd # Example using elif as well # Print the meteorological season for each month (loosely, of course, and in the Northern Hemisphere) print ( \"In the Northern Hemisphere: \\n \" ) month_integer = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ] # i.e., January is 1, February is 2, etc... for month in month_integer : if month < 3 : print ( \"Month {} is in Winter\" . format ( month )) elif month < 6 : print ( \"Month {} is in Spring\" . format ( month )) elif month < 9 : print ( \"Month {} is in Summer\" . format ( month )) elif month < 12 : print ( \"Month {} is in Fall\" . format ( month )) else : # This will put 12 (i.e., December) into Winter print ( \"Month {} is in Winter\" . format ( month )) In the Northern Hemisphere: Month 1 is in Winter Month 2 is in Winter Month 3 is in Spring Month 4 is in Spring Month 5 is in Spring Month 6 is in Summer Month 7 is in Summer Month 8 is in Summer Month 9 is in Fall Month 10 is in Fall Month 11 is in Fall Month 12 is in Winter Ternary operator Conditional expressions provide the functionality of if statements while using less code. They shouldn't be overused, as they can easily reduce readability, but they are often useful when assigning variables. Conditional expressions are also known as applications of the ternary operator. The ternary operator checks the condition and returns the corresponding value. The ternary operator is so called because, unlike most operators, it takes three arguments. a = 3 b = 1 if a >= 5 else 42 print ( b ) #42 Match case (Switch) ##For 3.10 up value = \"one\" match value : case \"one\" : result = 1 case \"two\" : result = 2 case \"three\" | \"four\" : result = ( 3 , 4 ) case _ : result =- 1 print ( result )","title":"Conditionals"},{"location":"PR/pyCond/#ternary-operator","text":"Conditional expressions provide the functionality of if statements while using less code. They shouldn't be overused, as they can easily reduce readability, but they are often useful when assigning variables. Conditional expressions are also known as applications of the ternary operator. The ternary operator checks the condition and returns the corresponding value. The ternary operator is so called because, unlike most operators, it takes three arguments. a = 3 b = 1 if a >= 5 else 42 print ( b ) #42","title":"Ternary operator"},{"location":"PR/pyCond/#match-case-switch","text":"##For 3.10 up value = \"one\" match value : case \"one\" : result = 1 case \"two\" : result = 2 case \"three\" | \"four\" : result = ( 3 , 4 ) case _ : result =- 1 print ( result )","title":"Match case (Switch)"},{"location":"PR/pyElse/","text":"The else statement is most commonly used along with the if statement, but it can also follow a for or while loop, which gives it a different meaning. With the for or while loop, the code within it is called if the loop finishes normally (when a break statement does not cause an exit from the loop). for i in range ( 10 ): if i == 999 : break else : print ( \"Unbroken 1\" ) for i in range ( 10 ): if i == 5 : break else : print ( \"Unbroken 2\" ) Unbroken 1 The else statement can also be used with try/except statements. In this case, the code within it is only executed if no error occurs in the try statement. try : print ( 1 ) except ZeroDivisionError : print ( 2 ) else : print ( 3 ) print ( \"*********\" ) try : print ( 1 / 0 ) except ZeroDivisionError : print ( 2 ) else : print ( 3 ) 1 3 ********* 2","title":"Else-4loopsExcep"},{"location":"PR/pyExp/","text":"Exceptions ImportError: an import fails; IndexError: a list is indexed with an out-of-range number; NameError: an unknown variable is used; SyntaxError: the code can't be parsed properly; TypeError: a function is called on a value of an inappropriate type; ValueError: a function is called on a value of the correct type, but with an inappropriate value. Exception Handling To handle exceptions, and to call code when an exception occurs, you can use a try/except statement. Multiple exceptions can also be put into a single except block using parentheses, to have the except block handle all of them. An except statement without any exception specified will catch all errors. Finally To ensure some code runs no matter what errors occur, you can use a finally statement. The finally statement is placed at the bottom of a try/except statement. try : a = 0 b = 1 print ( a / b ) #print(b/a) print ( a + \"a\" ) except ZeroDivisionError : print ( \"Error!\" ) except ( ValueError , TypeError ): print ( \"Error 2!\" ) except : print ( f 'Unknown error: { sys . exc_info () } ' ) else : print ( 'No errors' ) finally : print ( \"Bye bye\" ) 0.0 Error 2! Bye bye Raising Exceptions Use raise statement Exceptions can be raised with arguments that give detail about them. print ( 1 ) raise ValueError print ( 2 ) 1 --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-75-990863ff3a0f> in <module>() 1 print(1) ----> 2 raise ValueError 3 print(2) ValueError: try : name = \"123\" raise NameError ( \"Invalid Name!\" ) except NameError as e : print ( f 'Name error: { e } ' ) print ( 2 ) Name error: Invalid Name! 2","title":"Exceptions"},{"location":"PR/pyExp/#exceptions","text":"ImportError: an import fails; IndexError: a list is indexed with an out-of-range number; NameError: an unknown variable is used; SyntaxError: the code can't be parsed properly; TypeError: a function is called on a value of an inappropriate type; ValueError: a function is called on a value of the correct type, but with an inappropriate value.","title":"Exceptions"},{"location":"PR/pyExp/#exception-handling","text":"To handle exceptions, and to call code when an exception occurs, you can use a try/except statement. Multiple exceptions can also be put into a single except block using parentheses, to have the except block handle all of them. An except statement without any exception specified will catch all errors.","title":"Exception Handling"},{"location":"PR/pyExp/#finally","text":"To ensure some code runs no matter what errors occur, you can use a finally statement. The finally statement is placed at the bottom of a try/except statement. try : a = 0 b = 1 print ( a / b ) #print(b/a) print ( a + \"a\" ) except ZeroDivisionError : print ( \"Error!\" ) except ( ValueError , TypeError ): print ( \"Error 2!\" ) except : print ( f 'Unknown error: { sys . exc_info () } ' ) else : print ( 'No errors' ) finally : print ( \"Bye bye\" ) 0.0 Error 2! Bye bye","title":"Finally"},{"location":"PR/pyExp/#raising-exceptions","text":"Use raise statement Exceptions can be raised with arguments that give detail about them. print ( 1 ) raise ValueError print ( 2 ) 1 --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-75-990863ff3a0f> in <module>() 1 print(1) ----> 2 raise ValueError 3 print(2) ValueError: try : name = \"123\" raise NameError ( \"Invalid Name!\" ) except NameError as e : print ( f 'Name error: { e } ' ) print ( 2 ) Name error: Invalid Name! 2","title":"Raising Exceptions"},{"location":"PR/pyOp1/","text":"Operator Name Description a + b Addition Sum of a and b a - b Subtraction Difference of a and b a * b Multiplication Product of a and b a / b True division Quotient of a and b a // b Floor division Quotient of a and b , removing fractional parts a % b Modulus Integer remainder after division of a by b a ** b Exponentiation a raised to the power of b -a Negation The negative of a a = 3 ** 2 b = 2 ** 2 ** 3 print ( a ) print ( b ) print ( 9 ** ( 1 / 2 )) #result is float print ( 20 // 6 ) # Quotient print ( 20 % 6 ) # Modulo or remainder 9 256 3.0 3 2 Decimal Decimal fixed point and floating point arithmetic For enhanced precision, the decimal module provides support for fast correctly-rounded decimal floating point arithmetic. from decimal import * getcontext () . prec = 6 Decimal ( 1 ) / Decimal ( 7 ) Decimal('0.142857')","title":"Arithmethic"},{"location":"PR/pyOp1/#decimal","text":"Decimal fixed point and floating point arithmetic For enhanced precision, the decimal module provides support for fast correctly-rounded decimal floating point arithmetic. from decimal import * getcontext () . prec = 6 Decimal ( 1 ) / Decimal ( 7 ) Decimal('0.142857')","title":"Decimal"},{"location":"PR/pyOp2/","text":"In Python, bitwise operators are used to performing bitwise calculations on integers. The integers are first converted into binary and then operations are performed on bit by bit, hence the name bitwise operators. Then the result is returned in decimal format. Note: Python bitwise operators work only on integers. Operator Name Description a & b Bitwise AND Returns 1 if both the bits are 1 else 0 a | b Bitwise OR Returns 1 if either of the bit is 1 else 0 a ~ b Bitwise NOT Returns one\u2019s complement of the number a ^ b Bitwise XOR Returns 1 if one of the bits is 1 and the other is 0 else returns false a >> b Bitwise right shift Returns 1 if both the bits are 1 else 0 a << b Bitwise left shift Returns 1 if both the bits are 1 else 0 Shift Operators These operators are used to shift the bits of a number left or right thereby multiplying or dividing the number by two respectively. They can be used when we have to multiply or divide a number by two. Bitwise right shift Shifts the bits of the number to the right and fills 0 on voids left as a result. Similar effect as of dividing the number with some power of two. Bitwise left shift Shifts the bits of the number to the left and fills 0 on voids left as a result. Similar effect as of multiplying the number with some power of two. a = 10 #1010(Binary) b = 4 #0100(Binary) # Print bitwise AND operation print ( \"a & b =\" , a & b ) #0000(Binary) # Print bitwise OR operation print ( \"a | b =\" , a | b ) #1110(Binary)=14(Decimal) # Print bitwise NOT operation print ( \"~a =\" , ~ a ) #-(1010+1)(Binary)=-(1011)=-11(Decimal) # print bitwise XOR operation print ( \"a ^ b =\" , a ^ b ) # 1110(Binary)=14(Decimal) a = 10 b = - 10 # print bitwise right shift operator print ( \"a >> 1 =\" , a >> 1 ) print ( \"b >> 1 =\" , b >> 1 ) a = 5 b = - 10 # print bitwise left shift operator print ( \"a << 1 =\" , a << 1 ) print ( \"b << 1 =\" , b << 1 ) #Hexidecimal x = 0x0a y = 0x02 z = x & y print ( f '(hex) x is { x : 02x } , y is { y : 02x } , z is { z : 02x } ' ) print ( f '(bin) x is { x : 08b } , y is { y : 08b } , z is { z : 08b } ' ) z = x << y print ( f '(bin) x is { x : 08b } , y is { y : 08b } , z is { z : 08b } ' ) a & b = 0 a | b = 14 ~a = -11 a ^ b = 14 a >> 1 = 5 b >> 1 = -5 a << 1 = 10 b << 1 = -20 (hex) x is 0a, y is 02, z is 02 (bin) x is 00001010, y is 00000010, z is 00000010 (bin) x is 00001010, y is 00000010, z is 00101000","title":"Bitwise"},{"location":"PR/pyOp2/#shift-operators","text":"These operators are used to shift the bits of a number left or right thereby multiplying or dividing the number by two respectively. They can be used when we have to multiply or divide a number by two.","title":"Shift Operators"},{"location":"PR/pyOp2/#bitwise-right-shift","text":"Shifts the bits of the number to the right and fills 0 on voids left as a result. Similar effect as of dividing the number with some power of two.","title":"Bitwise right shift"},{"location":"PR/pyOp2/#bitwise-left-shift","text":"Shifts the bits of the number to the left and fills 0 on voids left as a result. Similar effect as of multiplying the number with some power of two. a = 10 #1010(Binary) b = 4 #0100(Binary) # Print bitwise AND operation print ( \"a & b =\" , a & b ) #0000(Binary) # Print bitwise OR operation print ( \"a | b =\" , a | b ) #1110(Binary)=14(Decimal) # Print bitwise NOT operation print ( \"~a =\" , ~ a ) #-(1010+1)(Binary)=-(1011)=-11(Decimal) # print bitwise XOR operation print ( \"a ^ b =\" , a ^ b ) # 1110(Binary)=14(Decimal) a = 10 b = - 10 # print bitwise right shift operator print ( \"a >> 1 =\" , a >> 1 ) print ( \"b >> 1 =\" , b >> 1 ) a = 5 b = - 10 # print bitwise left shift operator print ( \"a << 1 =\" , a << 1 ) print ( \"b << 1 =\" , b << 1 ) #Hexidecimal x = 0x0a y = 0x02 z = x & y print ( f '(hex) x is { x : 02x } , y is { y : 02x } , z is { z : 02x } ' ) print ( f '(bin) x is { x : 08b } , y is { y : 08b } , z is { z : 08b } ' ) z = x << y print ( f '(bin) x is { x : 08b } , y is { y : 08b } , z is { z : 08b } ' ) a & b = 0 a | b = 14 ~a = -11 a ^ b = 14 a >> 1 = 5 b >> 1 = -5 a << 1 = 10 b << 1 = -20 (hex) x is 0a, y is 02, z is 02 (bin) x is 00001010, y is 00000010, z is 00000010 (bin) x is 00001010, y is 00000010, z is 00101000","title":"Bitwise left shift"},{"location":"PR/pyOp3/","text":"In-place operators allow you to write code like 'x = x + 3' more concisely, as 'x += 3'. In-place operators can be used for any numerical operation (+, -, , /, %, *, //). These operators can be used on types other than numbers, as well, such as strings. x = 2 x += 3 print ( x ) x *= 7 print ( x ) y = \"spam \" y *= 3 print ( y ) 5 35 spam spam spam","title":"In-Place"},{"location":"PR/pyOp4/","text":"Walrus operator := allows you to assign values to variables within an expression, including variables that do not exist yet. Let's suppose we want to take an integer from the user, assign it to a variable num and output it. The walrus operator accomplishes these operations at once. The walrus operator makes code more readable and can be useful in many situations. num = int ( input ()) print ( num ) #The same but using Walrus operator #Python 3.8 up print ( num := int ( input ()))","title":"Walrus"},{"location":"PR/pyOp5/","text":"Boolean type . There are two Boolean values: True and False. They can be created by comparing values, for instance by using the equal operator ==. a = True print ( a ) b = ( 1 == 3 ) print ( b ) True False -1 Comparison operators are also called Relational operators . There are != , <, <=, >, >=. Used to compare strings lexicographically print ( 3 != 2 ) print ( 8 <= 9.0 ) #different types, no problem! print ( 'Carlos' > 'Ana' ) True True True Boolean Operators Boolean logic is used to make more complicated conditions for if statements that rely on more than one condition. Python's Boolean operators are and , or , not , in , not in , is and, is not . print ( 1 == 1 and 3 > 4 ) print ( 1 == 1 or 3 > 4 ) print ( 1 == 2 and not 3 > 4 ) x = ( 'bear' , 'bunny' , 'tree' ) y = 'bear' print ( y in x ) print ( y not in x ) print ( y is x [ 0 ]) #same id print ( id ( y )) print ( id ( x [ 0 ])) False True False True False True 140097849021808 140097849021808","title":"Comparison"},{"location":"PR/pyOp5/#boolean-operators","text":"Boolean logic is used to make more complicated conditions for if statements that rely on more than one condition. Python's Boolean operators are and , or , not , in , not in , is and, is not . print ( 1 == 1 and 3 > 4 ) print ( 1 == 1 or 3 > 4 ) print ( 1 == 2 and not 3 > 4 ) x = ( 'bear' , 'bunny' , 'tree' ) y = 'bear' print ( y in x ) print ( y not in x ) print ( y is x [ 0 ]) #same id print ( id ( y )) print ( id ( x [ 0 ])) False True False True False True 140097849021808 140097849021808","title":"Boolean Operators"},{"location":"PR/pyOp6/","text":"The Operator, @ In the context of matrix multiplication, a @ b invokes a.__matmul__(b) - making this syntax: a @ b equivalent to dot(a, b) and a @= b equivalent to a = dot(a, b) where dot is, for example, the numpy matrix multiplication function and a and b are matrices.","title":"/@"},{"location":"PR/pyOp7/","text":"(Star) * Operator The single star * unpacks the sequence/collection into positional arguments, so you can do this: def sum ( a , b ): return a + b values = ( 1 , 2 ) s = sum ( * values ) print ( s ) s = sum ( 1 , 2 ) #The same print ( s ) 3 3 The double star ** does the same, only using a dictionary and thus named arguments: values = { 'a' : 1 , 'b' : 2 } s = sum ( ** values ) print ( s ) 3 Combining the two: def sum ( a , b , c , d ): return a + b + c + d values1 = ( 1 , 2 ) values2 = { 'c' : 10 , 'd' : 15 } s = sum ( * values1 , ** values2 ) print ( s ) # the same as s = sum ( 1 , 2 , c = 10 , d = 15 ) print ( s ) 28 28 Additionally you can define functions to take x and *y arguments, this allows a function to accept any number of positional and/or named arguments that aren't specifically named in the declaration. def sum ( * values ): s = 0 for v in values : s = s + v return s s = sum ( 1 , 2 , 3 , 4 , 5 ) print ( s ) 15 or with ** def get_a ( ** values ): return values [ 'a' ] s = get_a ( a = 1 , b = 2 ) # returns 1 print ( s ) 1 this can allow you to specify a large number of optional parameters without having to declare them. And again, you can combine: def sum ( * values , ** options ): s = 0 for i in values : s = s + i if \"neg\" in options : if options [ \"neg\" ]: s = - s return s s = sum ( 1 , 2 , 3 , 4 , 5 ) # returns 15 s = sum ( 1 , 2 , 3 , 4 , 5 , neg = True ) # returns -15 s = sum ( 1 , 2 , 3 , 4 , 5 , neg = False ) # returns 15","title":"(star) *"},{"location":"PR/pyOp7/#star-operator","text":"The single star * unpacks the sequence/collection into positional arguments, so you can do this: def sum ( a , b ): return a + b values = ( 1 , 2 ) s = sum ( * values ) print ( s ) s = sum ( 1 , 2 ) #The same print ( s ) 3 3 The double star ** does the same, only using a dictionary and thus named arguments: values = { 'a' : 1 , 'b' : 2 } s = sum ( ** values ) print ( s ) 3 Combining the two: def sum ( a , b , c , d ): return a + b + c + d values1 = ( 1 , 2 ) values2 = { 'c' : 10 , 'd' : 15 } s = sum ( * values1 , ** values2 ) print ( s ) # the same as s = sum ( 1 , 2 , c = 10 , d = 15 ) print ( s ) 28 28 Additionally you can define functions to take x and *y arguments, this allows a function to accept any number of positional and/or named arguments that aren't specifically named in the declaration. def sum ( * values ): s = 0 for v in values : s = s + v return s s = sum ( 1 , 2 , 3 , 4 , 5 ) print ( s ) 15 or with ** def get_a ( ** values ): return values [ 'a' ] s = get_a ( a = 1 , b = 2 ) # returns 1 print ( s ) 1 this can allow you to specify a large number of optional parameters without having to declare them. And again, you can combine: def sum ( * values , ** options ): s = 0 for i in values : s = s + i if \"neg\" in options : if options [ \"neg\" ]: s = - s return s s = sum ( 1 , 2 , 3 , 4 , 5 ) # returns 15 s = sum ( 1 , 2 , 3 , 4 , 5 , neg = True ) # returns -15 s = sum ( 1 , 2 , 3 , 4 , 5 , neg = False ) # returns 15","title":"(Star) * Operator"},{"location":"PR/pyStr1/","text":"Backslashes can also be used to escape tabs or arbitrary Unicode characters. \\n represents a new line. Similarly, \\t represents a tab. Newlines will be automatically added for strings that are created using three quotes. This makes it easier to format long, multi-line texts without the need to explicitly put \\n for line breaks. print ( \"Hugo \\' s house \\n Jane \\' s mother \\t Diana \\' s dog\" ) print ( \"\"\"This is multline text\"\"\" ) print ( \"That's \\\" cool \\\" \" ) print ( \"Look, a mountain: / \\\\ \" ) print ( \"1 \\n 2 3\" ) Hugo's house Jane's mother Diana's dog This is multline text That's \"cool\" Look, a mountain: /\\ 1 2 3","title":"Backslashes"},{"location":"PR/pyStr2/","text":"String Operations : (+) concatenation (*) multiplication - Strings can also be multiplied by integers. This produces a repeated version of the original string. User Input : The input function prompts the user for input, and returns what they enter as a string print ( \"spam\" + \" eggs\" ) print ( \"spam \" * 3 ) name = input ( \"your name is?\" ) print ( \"Hello \" + name ) spam eggs spam spam spam your name is?Carlos Hello Carlos","title":"String operations"},{"location":"PR/pyStr3/","text":"str.format() String formatting uses a string's format method to substitute a number of arguments in the string. Each argument of the format function is placed in the string at the corresponding position, which is determined using the curly braces { }. # String formatting nums = [ 1 , 2 , 3 ] msg = \"Numbers {0} {1} {2} \" . format ( nums [ 1 ], nums [ 0 ], nums [ 2 ]) print ( msg ) Numbers 2 1 3 String formatting can also be done with named arguments. a = \" {x} , {y} \" . format ( y = 12 , x = 4 ) print ( a ) 4 , 12 x = 42 print ( 'The number is {:b} ' . format ( x )) The number is 101010 count(str) returns how many times the str substring appears in the given string. upper() converts the string to uppercase. lower() converts the string to lowercase. replace(old, new) replaces all occurrences of old with new. len(str) returns the length of the string (number of characters). Note, that these functions return a new string with the corresponding manipulation Fstrings (Literal String Interpolation) After Python 3.6, to create an f-string, prefix the string with the letter \u201c f \u201d. The string itself can be formatted in much the same way that you would with str.format(). F-strings provide a concise and convenient way to embed python expressions inside string literals for formatting. Simply it is a shortcut for the format method. # Prints today's date with help # of datetime library import datetime today = datetime . datetime . today () print ( f \" { today : %B %d, %Y } \" ) July 30, 2021 name = \"Eric Idle\" f \" { name . lower () } is funny.\" eric idle is funny. x = 42 print ( f 'The number is { x : b } ' ) The number is 101010 name = 'CarPool' age = 23 print ( f \"Hello, My name is { name } and I'm { age } years old.\" ) Hello, My name is CarPool and I'm 23 years old. Template string from string import Template def main (): # Usual string formatting with format() str1 = \"Love {0} and {1} \" . format ( \"Pollo\" , \"Pitas\" ) print ( str1 ) # create a template with placeholders templ = Template ( \"Love $ {title} and $ {author} \" ) # use the substitute method with keyword arguments str2 = templ . substitute ( title = \"Pollo\" , author = \"Pitas\" ) print ( str2 ) # use the substitute method with a dictionary data = { \"author\" : \"Pitas\" , \"title\" : \"Pollo\" } str3 = templ . substitute ( data ) print ( str3 ) if __name__ == \"__main__\" : main () Love Pollo and Pitas Love Pollo and Pitas Love Pollo and Pitas","title":"String formatting"},{"location":"PR/pyStr3/#strformat","text":"String formatting uses a string's format method to substitute a number of arguments in the string. Each argument of the format function is placed in the string at the corresponding position, which is determined using the curly braces { }. # String formatting nums = [ 1 , 2 , 3 ] msg = \"Numbers {0} {1} {2} \" . format ( nums [ 1 ], nums [ 0 ], nums [ 2 ]) print ( msg ) Numbers 2 1 3 String formatting can also be done with named arguments. a = \" {x} , {y} \" . format ( y = 12 , x = 4 ) print ( a ) 4 , 12 x = 42 print ( 'The number is {:b} ' . format ( x )) The number is 101010 count(str) returns how many times the str substring appears in the given string. upper() converts the string to uppercase. lower() converts the string to lowercase. replace(old, new) replaces all occurrences of old with new. len(str) returns the length of the string (number of characters). Note, that these functions return a new string with the corresponding manipulation","title":"str.format()"},{"location":"PR/pyStr3/#fstrings-literal-string-interpolation","text":"After Python 3.6, to create an f-string, prefix the string with the letter \u201c f \u201d. The string itself can be formatted in much the same way that you would with str.format(). F-strings provide a concise and convenient way to embed python expressions inside string literals for formatting. Simply it is a shortcut for the format method. # Prints today's date with help # of datetime library import datetime today = datetime . datetime . today () print ( f \" { today : %B %d, %Y } \" ) July 30, 2021 name = \"Eric Idle\" f \" { name . lower () } is funny.\" eric idle is funny. x = 42 print ( f 'The number is { x : b } ' ) The number is 101010 name = 'CarPool' age = 23 print ( f \"Hello, My name is { name } and I'm { age } years old.\" ) Hello, My name is CarPool and I'm 23 years old.","title":"Fstrings (Literal String Interpolation)"},{"location":"PR/pyStr3/#template-string","text":"from string import Template def main (): # Usual string formatting with format() str1 = \"Love {0} and {1} \" . format ( \"Pollo\" , \"Pitas\" ) print ( str1 ) # create a template with placeholders templ = Template ( \"Love $ {title} and $ {author} \" ) # use the substitute method with keyword arguments str2 = templ . substitute ( title = \"Pollo\" , author = \"Pitas\" ) print ( str2 ) # use the substitute method with a dictionary data = { \"author\" : \"Pitas\" , \"title\" : \"Pollo\" } str3 = templ . substitute ( data ) print ( str3 ) if __name__ == \"__main__\" : main () Love Pollo and Pitas Love Pollo and Pitas Love Pollo and Pitas","title":"Template string"},{"location":"PR/pyStr4/","text":"\u201c Format specifications \u201d are used within replacement fields contained within a format string to define how individual values are presented A general convention is that an empty format specification produces the same result as if you had called str() on the value. A non-empty format specification typically modifies the result. The general form of a standard format specifier is: format_spec [[fill]align][sign][#][0][width][grouping_option][.precision][type] fill any character align \"<\" , \">\" , \"=\" , \"^\" sign \"+\" , \"-\" , \" \" width digit+ grouping_option \"_\" , \",\" precision digit+ type \"b\", \"c\" , \"d\" , \"e\" , \"E\" , \"f\" , \"F\" , \"g\" , \"G\" , \"n\" , \"o\" , \"s\" , \"x\" , \"X\" , \"%\" Fill If a valid align value is specified, it can be preceded by a fill character that can be any character and defaults to a space if omitted. It is not possible to use a literal curly brace (\u201c{\u201d or \u201c}\u201d) as the fill character in a formatted string literal or when using the str.format() method. However, it is possible to insert a curly brace with a nested replacement field. This limitation doesn\u2019t affect the format() function. Align The meaning of the various alignment options is as follows: Option Meaning '<' Forces the field to be left-aligned within the available space (this is the default for most objects) '>' Forces the field to be right-aligned within the available space (this is the default for numbers) '=' Forces the padding to be placed after the sign (if any) but before the digits. This is used for printing fields in the form \u2018+000000120\u2019. This alignment option is only valid for numeric types. It becomes the default for numbers when \u20180\u2019 immediately precedes the field width '^' Forces the field to be centered within the available space ' {:>30} ' . format ( 'right aligned' ) right aligned ' {:*^30} ' . format ( 'centered' ) # use '*' as a fill char ***********centered*********** Sign Note that unless a minimum field width is defined, the field width will always be the same size as the data to fill it, so that the alignment option has no meaning in this case. The sign option is only valid for number types, and can be one of the following: Option Meaning '+' indicates that a sign should be used for both positive as well as negative numbers. '-' indicates that a sign should be used only for negative numbers (this is the default behavior). space indicates that a leading space should be used on positive numbers, and a minus sign on negative numbers. ' {:+f} ; {:+f} ' . format ( 3.14 , - 3.14 ) # show it always +3.140000; -3.140000 ' {: f} ; {: f} ' . format ( 3.14 , - 3.14 ) # show a space for positive numbers 3.140000; -3.140000 ' {:-f} ; {:-f} ' . format ( 3.14 , - 3.14 ) # show only the minus -- same as '{:f}; {:f}' 3.140000; -3.140000 # and , and _ Option '#' option The '#' option causes the \u201calternate form\u201d to be used for the conversion. The alternate form is defined differently for different types. This option is only valid for integer, float and complex types. For integers, when binary, octal, or hexadecimal output is used, this option adds the respective prefix '0b', '0o', '0x', or '0X' to the output value. For float and complex the alternate form causes the result of the conversion to always contain a decimal-point character, even if no digits follow it. Normally, a decimal-point character appears in the result of these conversions only if a digit follows it. In addition, for 'g' and 'G' conversions, trailing zeros are not removed from the result. 'Integer in Octal: {:#o} ' . format ( 1234567 ) Correct answers: 0o4553207 'Integer in Octal: {:o} ' . format ( 1234567 ) Correct answers: 4553207 ',' option The ',' option signals the use of a comma for a thousands separator. For a locale aware separator, use the 'n' integer presentation type instead. see PEP378 ' {:,} ' . format ( 1234567890 ) 1,234,567,890 format ( 1234.5 , \"08,.1f\" ) 01,234.5 '_' option The '_' option signals the use of an underscore for a thousands separator for floating point presentation types and for integer presentation type 'd'. For integer presentation types 'b', 'o', 'x', and 'X', underscores will be inserted every 4 digits. For other presentation types, specifying this option is an error. #{:10_} for a width of 10 with _ separator. format ( 1234.5 , \"08_.1f\" ) 01_234.5 Width width is a decimal integer defining the minimum total field width, including any prefixes, separators, and other formatting characters. If not specified, then the field width will be determined by the content. When no explicit alignment is given, preceding the width field by a zero ('0') character enables sign-aware zero-padding for numeric types. This is equivalent to a fill character of '0' with an alignment type of '='. Changed in version 3.10: Preceding the width field by '0' no longer affects the default alignment for strings. Precision The precision is a decimal integer indicating how many digits should be displayed after the decimal point for presentation types 'f' and 'F', or before and after the decimal point for presentation types 'g' or 'G'. For string presentation types the field indicates the maximum field size - in other words, how many characters will be used from the field content. The precision is not allowed for integer presentation types. points = 19 total = 22 'Correct answers: {:.2%} ' . format ( points / total ) Correct answers: 86.36% Type The type determines how the data should be presented. for strings The available string presentation types are: Type Meaning 's' String format. This is the default type for strings and may be omitted. None The same as 's'. for integers The available integer presentation types are: Type Meaning 'b' Binary format. Outputs the number in base 2. 'c' Character. Converts the integer to the corresponding unicode character before printing. 'd' Octal format. Outputs the number in base 8. 'o' Character. Converts the integer to the corresponding unicode character before printing. 'x' Hex format. Outputs the number in base 16, using lower-case letters for the digits above 9. 'X' Hex format. Outputs the number in base 16, using upper-case letters for the digits above 9. In case '#' is specified, the prefix '0x' will be upper-cased to '0X' as well. 'n' Number. This is the same as 'd', except that it uses the current locale setting to insert the appropriate number separator characters. None The same as 'd'. In addition to the above presentation types, integers can be formatted with the floating point presentation types listed below (except 'n' and None). When doing so, float() is used to convert the integer to a floating point number before formatting. for float and decimal The available presentation types for float and Decimal values are: Type Meaning 'e' Scientific notation. For a given precision p, formats the number in scientific notation with the letter \u2018e\u2019 separating the coefficient from the exponent. The coefficient has one digit before and p digits after the decimal point, for a total of p + 1 significant digits. With no precision given, uses a precision of 6 digits after the decimal point for float, and shows all coefficient digits for Decimal. If no digits follow the decimal point, the decimal point is also removed unless the # option is used. 'E' Scientific notation. Same as 'e' except it uses an upper case \u2018E\u2019 as the separator character. 'f' Fixed-point notation. For a given precision p, formats the number as a decimal number with exactly p digits following the decimal point. With no precision given, uses a precision of 6 digits after the decimal point for float, and uses a precision large enough to show all coefficient digits for Decimal. If no digits follow the decimal point, the decimal point is also removed unless the # option is used. 'F' Fixed-point notation. Same as 'f', but converts nan to NAN and inf to INF. 'g' General format. For a given precision p >= 1, this rounds the number to p significant digits and then formats the result in either fixed-point format or in scientific notation, depending on its magnitude. A precision of 0 is treated as equivalent to a precision of 1. The precise rules are as follows: suppose that the result formatted with presentation type 'e' and precision p-1 would have exponent exp. Then, if m <= exp < p, where m is -4 for floats and -6 for Decimals, the number is formatted with presentation type 'f' and precision p-1-exp. Otherwise, the number is formatted with presentation type 'e' and precision p-1. In both cases insignificant trailing zeros are removed from the significand, and the decimal point is also removed if there are no remaining digits following it, unless the '#' option is used. With no precision given, uses a precision of 6 significant digits for float. For Decimal, the coefficient of the result is formed from the coefficient digits of the value; scientific notation is used for values smaller than 1e-6 in absolute value and values where the place value of the least significant digit is larger than 1, and fixed-point notation is used otherwise. Positive and negative infinity, positive and negative zero, and nans, are formatted as inf, -inf, 0, -0 and nan respectively, regardless of the precision. 'G' General format. Same as 'g' except switches to 'E' if the number gets too large. The representations of infinity and NaN are uppercased, too. 'n' Number. This is the same as 'g', except that it uses the current locale setting to insert the appropriate number separator characters. '%' Percentage. Multiplies the number by 100 and displays in fixed ('f') format, followed by a percent sign. None For float this is the same as 'g', except that when fixed-point notation is used to format the result, it always includes at least one digit past the decimal point. The precision used is as large as needed to represent the given value faithfully. For Decimal, this is the same as either 'g' or 'G' depending on the value of context.capitals for the current decimal context. The overall effect is to match the output of str() as altered by the other format modifiers.","title":"String formatting specifications"},{"location":"PR/pyStr4/#fill","text":"If a valid align value is specified, it can be preceded by a fill character that can be any character and defaults to a space if omitted. It is not possible to use a literal curly brace (\u201c{\u201d or \u201c}\u201d) as the fill character in a formatted string literal or when using the str.format() method. However, it is possible to insert a curly brace with a nested replacement field. This limitation doesn\u2019t affect the format() function.","title":"Fill"},{"location":"PR/pyStr4/#align","text":"The meaning of the various alignment options is as follows: Option Meaning '<' Forces the field to be left-aligned within the available space (this is the default for most objects) '>' Forces the field to be right-aligned within the available space (this is the default for numbers) '=' Forces the padding to be placed after the sign (if any) but before the digits. This is used for printing fields in the form \u2018+000000120\u2019. This alignment option is only valid for numeric types. It becomes the default for numbers when \u20180\u2019 immediately precedes the field width '^' Forces the field to be centered within the available space ' {:>30} ' . format ( 'right aligned' ) right aligned ' {:*^30} ' . format ( 'centered' ) # use '*' as a fill char ***********centered***********","title":"Align"},{"location":"PR/pyStr4/#sign","text":"Note that unless a minimum field width is defined, the field width will always be the same size as the data to fill it, so that the alignment option has no meaning in this case. The sign option is only valid for number types, and can be one of the following: Option Meaning '+' indicates that a sign should be used for both positive as well as negative numbers. '-' indicates that a sign should be used only for negative numbers (this is the default behavior). space indicates that a leading space should be used on positive numbers, and a minus sign on negative numbers. ' {:+f} ; {:+f} ' . format ( 3.14 , - 3.14 ) # show it always +3.140000; -3.140000 ' {: f} ; {: f} ' . format ( 3.14 , - 3.14 ) # show a space for positive numbers 3.140000; -3.140000 ' {:-f} ; {:-f} ' . format ( 3.14 , - 3.14 ) # show only the minus -- same as '{:f}; {:f}' 3.140000; -3.140000","title":"Sign"},{"location":"PR/pyStr4/#and-and-_-option","text":"","title":"# and , and _ Option"},{"location":"PR/pyStr4/#option","text":"The '#' option causes the \u201calternate form\u201d to be used for the conversion. The alternate form is defined differently for different types. This option is only valid for integer, float and complex types. For integers, when binary, octal, or hexadecimal output is used, this option adds the respective prefix '0b', '0o', '0x', or '0X' to the output value. For float and complex the alternate form causes the result of the conversion to always contain a decimal-point character, even if no digits follow it. Normally, a decimal-point character appears in the result of these conversions only if a digit follows it. In addition, for 'g' and 'G' conversions, trailing zeros are not removed from the result. 'Integer in Octal: {:#o} ' . format ( 1234567 ) Correct answers: 0o4553207 'Integer in Octal: {:o} ' . format ( 1234567 ) Correct answers: 4553207","title":"'#' option"},{"location":"PR/pyStr4/#option_1","text":"The ',' option signals the use of a comma for a thousands separator. For a locale aware separator, use the 'n' integer presentation type instead. see PEP378 ' {:,} ' . format ( 1234567890 ) 1,234,567,890 format ( 1234.5 , \"08,.1f\" ) 01,234.5","title":"',' option"},{"location":"PR/pyStr4/#_-option","text":"The '_' option signals the use of an underscore for a thousands separator for floating point presentation types and for integer presentation type 'd'. For integer presentation types 'b', 'o', 'x', and 'X', underscores will be inserted every 4 digits. For other presentation types, specifying this option is an error. #{:10_} for a width of 10 with _ separator. format ( 1234.5 , \"08_.1f\" ) 01_234.5","title":"'_' option"},{"location":"PR/pyStr4/#width","text":"width is a decimal integer defining the minimum total field width, including any prefixes, separators, and other formatting characters. If not specified, then the field width will be determined by the content. When no explicit alignment is given, preceding the width field by a zero ('0') character enables sign-aware zero-padding for numeric types. This is equivalent to a fill character of '0' with an alignment type of '='. Changed in version 3.10: Preceding the width field by '0' no longer affects the default alignment for strings.","title":"Width"},{"location":"PR/pyStr4/#precision","text":"The precision is a decimal integer indicating how many digits should be displayed after the decimal point for presentation types 'f' and 'F', or before and after the decimal point for presentation types 'g' or 'G'. For string presentation types the field indicates the maximum field size - in other words, how many characters will be used from the field content. The precision is not allowed for integer presentation types. points = 19 total = 22 'Correct answers: {:.2%} ' . format ( points / total ) Correct answers: 86.36%","title":"Precision"},{"location":"PR/pyStr4/#type","text":"The type determines how the data should be presented.","title":"Type"},{"location":"PR/pyStr4/#for-strings","text":"The available string presentation types are: Type Meaning 's' String format. This is the default type for strings and may be omitted. None The same as 's'.","title":"for strings"},{"location":"PR/pyStr4/#for-integers","text":"The available integer presentation types are: Type Meaning 'b' Binary format. Outputs the number in base 2. 'c' Character. Converts the integer to the corresponding unicode character before printing. 'd' Octal format. Outputs the number in base 8. 'o' Character. Converts the integer to the corresponding unicode character before printing. 'x' Hex format. Outputs the number in base 16, using lower-case letters for the digits above 9. 'X' Hex format. Outputs the number in base 16, using upper-case letters for the digits above 9. In case '#' is specified, the prefix '0x' will be upper-cased to '0X' as well. 'n' Number. This is the same as 'd', except that it uses the current locale setting to insert the appropriate number separator characters. None The same as 'd'. In addition to the above presentation types, integers can be formatted with the floating point presentation types listed below (except 'n' and None). When doing so, float() is used to convert the integer to a floating point number before formatting.","title":"for integers"},{"location":"PR/pyStr4/#for-float-and-decimal","text":"The available presentation types for float and Decimal values are: Type Meaning 'e' Scientific notation. For a given precision p, formats the number in scientific notation with the letter \u2018e\u2019 separating the coefficient from the exponent. The coefficient has one digit before and p digits after the decimal point, for a total of p + 1 significant digits. With no precision given, uses a precision of 6 digits after the decimal point for float, and shows all coefficient digits for Decimal. If no digits follow the decimal point, the decimal point is also removed unless the # option is used. 'E' Scientific notation. Same as 'e' except it uses an upper case \u2018E\u2019 as the separator character. 'f' Fixed-point notation. For a given precision p, formats the number as a decimal number with exactly p digits following the decimal point. With no precision given, uses a precision of 6 digits after the decimal point for float, and uses a precision large enough to show all coefficient digits for Decimal. If no digits follow the decimal point, the decimal point is also removed unless the # option is used. 'F' Fixed-point notation. Same as 'f', but converts nan to NAN and inf to INF. 'g' General format. For a given precision p >= 1, this rounds the number to p significant digits and then formats the result in either fixed-point format or in scientific notation, depending on its magnitude. A precision of 0 is treated as equivalent to a precision of 1. The precise rules are as follows: suppose that the result formatted with presentation type 'e' and precision p-1 would have exponent exp. Then, if m <= exp < p, where m is -4 for floats and -6 for Decimals, the number is formatted with presentation type 'f' and precision p-1-exp. Otherwise, the number is formatted with presentation type 'e' and precision p-1. In both cases insignificant trailing zeros are removed from the significand, and the decimal point is also removed if there are no remaining digits following it, unless the '#' option is used. With no precision given, uses a precision of 6 significant digits for float. For Decimal, the coefficient of the result is formed from the coefficient digits of the value; scientific notation is used for values smaller than 1e-6 in absolute value and values where the place value of the least significant digit is larger than 1, and fixed-point notation is used otherwise. Positive and negative infinity, positive and negative zero, and nans, are formatted as inf, -inf, 0, -0 and nan respectively, regardless of the precision. 'G' General format. Same as 'g' except switches to 'E' if the number gets too large. The representations of infinity and NaN are uppercased, too. 'n' Number. This is the same as 'g', except that it uses the current locale setting to insert the appropriate number separator characters. '%' Percentage. Multiplies the number by 100 and displays in fixed ('f') format, followed by a percent sign. None For float this is the same as 'g', except that when fixed-point notation is used to format the result, it always includes at least one digit past the decimal point. The precision used is as large as needed to represent the given value faithfully. For Decimal, this is the same as either 'g' or 'G' depending on the value of context.capitals for the current decimal context. The overall effect is to match the output of str() as altered by the other format modifiers.","title":"for float and decimal"},{"location":"PR/pyStr5/","text":"Python contains many useful built-in functions and methods to accomplish common tasks. join joins a list of strings with another string as a separator. print ( \", \" . join ([ \"spam\" , \"eggs\" , \"ham\" ])) spam, eggs, ham replace replaces one substring in a string with another. print ( \"Hello Me\" . replace ( \"Me\" , \"Ham\" )) Hello Ham startswith and endswith determine if there is a substring at the start and end of a string, respectively. find and rfind search for a substr in a larger str. returns the index or -1 if not found. rfind starts from the right end to search. (Use the in operator for boolean result) print ( \"Hello Me\" . find ( 'lo' )) print ( \"Hello Me\" . find ( 'lop' )) print ( \"Hello Me\" . rfind ( 'lo' )) print ( \"lo\" in \"Hello Me\" ) 3 -1 3 True lower and upper To change the case of a string, you can use lower and upper. split is the opposite of join turning a string with a certain separator into a list. print ( \"spam, eggs, ham\" . split ( \", \" )) ['spam', 'eggs', 'ham'] count to count the number of times a substring is present. print ( \"Me Hello Me\" . count ( \"Me\" )) print ( \"Me Hello Me\" . count ( \"Meh\" )) 2 0","title":"String functions"},{"location":"PR/pyStr5/#join","text":"joins a list of strings with another string as a separator. print ( \", \" . join ([ \"spam\" , \"eggs\" , \"ham\" ])) spam, eggs, ham","title":"join"},{"location":"PR/pyStr5/#replace","text":"replaces one substring in a string with another. print ( \"Hello Me\" . replace ( \"Me\" , \"Ham\" )) Hello Ham","title":"replace"},{"location":"PR/pyStr5/#startswith-and-endswith","text":"determine if there is a substring at the start and end of a string, respectively.","title":"startswith and endswith"},{"location":"PR/pyStr5/#find-and-rfind","text":"search for a substr in a larger str. returns the index or -1 if not found. rfind starts from the right end to search. (Use the in operator for boolean result) print ( \"Hello Me\" . find ( 'lo' )) print ( \"Hello Me\" . find ( 'lop' )) print ( \"Hello Me\" . rfind ( 'lo' )) print ( \"lo\" in \"Hello Me\" ) 3 -1 3 True","title":"find and rfind"},{"location":"PR/pyStr5/#lower-and-upper","text":"To change the case of a string, you can use lower and upper.","title":"lower and upper"},{"location":"PR/pyStr5/#split","text":"is the opposite of join turning a string with a certain separator into a list. print ( \"spam, eggs, ham\" . split ( \", \" )) ['spam', 'eggs', 'ham']","title":"split"},{"location":"PR/pyStr5/#count","text":"to count the number of times a substring is present. print ( \"Me Hello Me\" . count ( \"Me\" )) print ( \"Me Hello Me\" . count ( \"Meh\" )) 2 0","title":"count"},{"location":"PR/pyVar1/","text":"In Python, we store all pieces of data -- numbers, characters, strings, everything -- as objects, and we refer to these objects using variables. As a simple case, we can assign a variable a value using the assignment operator, which is the \"equals\" sign. Python's order of operations is the same as that of normal mathematics: parentheses first, then exponentiation, then multiplication/division, and then addition/subtraction. x = 4 y = 5 z = x + y print ( z ) stri = \"Hello\" # or stri='Hello' No difference in python between \"\" and '' print ( stri + str ( z )) a = True print ( a ) b = ( 1 == 3 ) print ( b ) 9 Hello9 True False Three tools for understanding strange objects type() function type() (what is this thing?) To inspect which type is a variable use type(). dir() function dir() (what can I do with it?) help() function help() (tell me more) print ( type ( z )) print ( type ( a )) print ( type ( stri )) print ( dir ( 2 )) #print(help(2)) <class 'int'> <class 'int'> <class 'str'> ['__abs__', '__add__', '__and__', '__bool__', '__ceil__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floor__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__gt__', '__hash__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__le__', '__lshift__', '__lt__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__trunc__', '__xor__', 'bit_length', 'conjugate', 'denominator', 'from_bytes', 'imag', 'numerator', 'real', 'to_bytes'] Python supports addition (+), substraction(-), multiplication( ), division(/),exponentiation( *), quotient (//) and remainder(%). You can chain exponentiations together. In other words, you can rise a number to multiple powers. id() function The id() function returns identity (unique integer) of an object. print ( 'id of 5 =' , id ( 5 )) a = 5 print ( 'id of a =' , id ( a )) b = a print ( 'id of b =' , id ( b )) c = 5.0 print ( 'id of c =' , id ( c )) id of 5 = 94364870744704 id of a = 94364870744704 id of b = 94364870744704 id of c = 140097885531312 It's important to note that everything in Python is an object, even numbers, and Classes. Hence, integer 5 has a unique id. The id of the integer 5 remains constant during the lifetime. Similar is the case for float 5.5 and other objects. isinstance() function To verify the type of an object, the isinstance() function checks if the object (first argument) is an instance or subclass of classinfo class (second argument). The None Object The None object is used to represent the absence of a value. It is similar to null in other programming languages. The None object is returned by any function that doesn't explicitly return anything else.","title":"Variables"},{"location":"PR/pyVar1/#three-tools-for-understanding-strange-objects","text":"","title":"Three tools for understanding strange objects"},{"location":"PR/pyVar1/#type-function","text":"type() (what is this thing?) To inspect which type is a variable use type().","title":"type() function"},{"location":"PR/pyVar1/#dir-function","text":"dir() (what can I do with it?)","title":"dir() function"},{"location":"PR/pyVar1/#help-function","text":"help() (tell me more) print ( type ( z )) print ( type ( a )) print ( type ( stri )) print ( dir ( 2 )) #print(help(2)) <class 'int'> <class 'int'> <class 'str'> ['__abs__', '__add__', '__and__', '__bool__', '__ceil__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floor__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__gt__', '__hash__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__le__', '__lshift__', '__lt__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__trunc__', '__xor__', 'bit_length', 'conjugate', 'denominator', 'from_bytes', 'imag', 'numerator', 'real', 'to_bytes'] Python supports addition (+), substraction(-), multiplication( ), division(/),exponentiation( *), quotient (//) and remainder(%). You can chain exponentiations together. In other words, you can rise a number to multiple powers.","title":"help() function"},{"location":"PR/pyVar1/#id-function","text":"The id() function returns identity (unique integer) of an object. print ( 'id of 5 =' , id ( 5 )) a = 5 print ( 'id of a =' , id ( a )) b = a print ( 'id of b =' , id ( b )) c = 5.0 print ( 'id of c =' , id ( c )) id of 5 = 94364870744704 id of a = 94364870744704 id of b = 94364870744704 id of c = 140097885531312 It's important to note that everything in Python is an object, even numbers, and Classes. Hence, integer 5 has a unique id. The id of the integer 5 remains constant during the lifetime. Similar is the case for float 5.5 and other objects.","title":"id() function"},{"location":"PR/pyVar1/#isinstance-function","text":"To verify the type of an object, the isinstance() function checks if the object (first argument) is an instance or subclass of classinfo class (second argument).","title":"isinstance() function"},{"location":"PR/pyVar1/#the-none-object","text":"The None object is used to represent the absence of a value. It is similar to null in other programming languages. The None object is returned by any function that doesn't explicitly return anything else.","title":"The None Object"},{"location":"PR/pyVar2/","text":"int() function:To convert it to a number str() function:To convert it to a string float() function: To convert it to a float bool() function: To convert it to a bool age = int ( input ()) print ( \"Your age is \" + str ( age ) + \" years old\" ) 15 Your age is 15 years old numbers = [ 1 , 2 , 3 , 4 , 2 , 5 ] # check if numbers is instance of list result = isinstance ( numbers , list ) print ( result ) True","title":"Type Casting"},{"location":"PR/pyVar3/","text":"The location where we can find a variable and also access it if required is called the scope of a variable. Python resolves names using the so-called LEGB rule, which is named after the Python scope for names. The letters in LEGB stand for Local, Enclosing, Global, and Built-in. When you use nested functions, names are resolved by first checking the local scope or the innermost function\u2019s local scope. Then, Python looks at all enclosing scopes of outer functions from the innermost scope to the outermost scope. If no match is found, then Python looks at the global and built-in scopes. If it can\u2019t find the name, then you\u2019ll get an error. Global Keyword Global variables are the ones that are defined and declared outside any function and are not specified to any function. They can be used by any part of the program. We only need to use the global keyword in a function if we want to do assignments or change the global variable. global is not needed for printing and accessing. Python \u201cassumes\u201d that we want a local variable due to the assignment to s inside of f(), so the first statement throws the error message. Any variable which is changed or created inside of a function is local if it hasn\u2019t been declared as a global variable. To tell Python, that we want to use the global variable, we have to use the keyword \u201cglobal\u201d a = 1 # Uses global because there is no local 'a' def f (): print ( 'Inside f() : ' , a ) # Variable 'a' is redefined as a local def g (): a = 2 print ( 'Inside g() : ' , a ) # Uses global keyword to modify global 'a' def h (): global a a = 3 print ( 'Inside h() : ' , a ) # Global scope print ( 'global : ' , a ) f () print ( 'global : ' , a ) g () print ( 'global : ' , a ) h () print ( 'global : ' , a ) global : 1 Inside f() : 1 global : 1 Inside g() : 2 global : 1 Inside h() : 3 global : 3 Nonlocal Keyword In Python, nonlocal keyword is used in the case of nested functions. This keyword works similar to the global, but rather than global, this keyword declares a variable to point to the variable of outside enclosing function, in case of nested functions. # Python program to demonstrate # nonlocal keyword print ( \"Value of a using nonlocal is : \" , end = \"\" ) def outer (): a = 5 def inner (): nonlocal a a = 10 inner () print ( a ) outer () # demonstrating without non local # inner loop not changing the value of outer a # prints 5 print ( \"Value of a without using nonlocal is : \" , end = \"\" ) def outer (): a = 5 def inner (): a = 10 inner () print ( a ) outer () Value of a using nonlocal is : 10 Value of a without using nonlocal is : 5","title":"Scope of Variables"},{"location":"PR/pyVar3/#global-keyword","text":"Global variables are the ones that are defined and declared outside any function and are not specified to any function. They can be used by any part of the program. We only need to use the global keyword in a function if we want to do assignments or change the global variable. global is not needed for printing and accessing. Python \u201cassumes\u201d that we want a local variable due to the assignment to s inside of f(), so the first statement throws the error message. Any variable which is changed or created inside of a function is local if it hasn\u2019t been declared as a global variable. To tell Python, that we want to use the global variable, we have to use the keyword \u201cglobal\u201d a = 1 # Uses global because there is no local 'a' def f (): print ( 'Inside f() : ' , a ) # Variable 'a' is redefined as a local def g (): a = 2 print ( 'Inside g() : ' , a ) # Uses global keyword to modify global 'a' def h (): global a a = 3 print ( 'Inside h() : ' , a ) # Global scope print ( 'global : ' , a ) f () print ( 'global : ' , a ) g () print ( 'global : ' , a ) h () print ( 'global : ' , a ) global : 1 Inside f() : 1 global : 1 Inside g() : 2 global : 1 Inside h() : 3 global : 3","title":"Global Keyword"},{"location":"PR/pyVar3/#nonlocal-keyword","text":"In Python, nonlocal keyword is used in the case of nested functions. This keyword works similar to the global, but rather than global, this keyword declares a variable to point to the variable of outside enclosing function, in case of nested functions. # Python program to demonstrate # nonlocal keyword print ( \"Value of a using nonlocal is : \" , end = \"\" ) def outer (): a = 5 def inner (): nonlocal a a = 10 inner () print ( a ) outer () # demonstrating without non local # inner loop not changing the value of outer a # prints 5 print ( \"Value of a without using nonlocal is : \" , end = \"\" ) def outer (): a = 5 def inner (): a = 10 inner () print ( a ) outer () Value of a using nonlocal is : 10 Value of a without using nonlocal is : 5","title":"Nonlocal Keyword"},{"location":"PR/pyVar4/","text":"del is used in Python to unset a variable or name. You can use it on variable names, but a more common use is to remove indexes from a list or dictionary. my_list1 = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] print ( my_list1 ) # delete second element of my_list1 del my_list1 [ 1 ] print ( my_list1 ) # slice my_list1 from index 3 to 5 del my_list1 [ 3 : 5 ] print ( my_list1 ) [1, 2, 3, 4, 5, 6, 7, 8, 9] [1, 3, 4, 5, 6, 7, 8, 9] [1, 3, 4, 7, 8, 9] my_dict1 = { \"small\" : \"big\" , \"black\" : \"white\" , \"up\" : \"down\" } print ( my_dict1 ) # delete key-value pair with key \"black\" from my_dict1 del my_dict1 [ \"black\" ] print ( my_dict1 ) {'small': 'big', 'black': 'white', 'up': 'down'} {'small': 'big', 'up': 'down'}","title":"del Keyword"},{"location":"PR/pyWhile/","text":"A while loop is used to repeat a block of code multiple times. The while loop is used in cases when the number of iterations is not known and depends on some calculations and conditions in the code block of the loop. To end a while loop prematurely, the break statement can be used. Another statement that can be used within loops is continue. Unlike break, continue jumps back to the top of the loop, rather than stopping it. Basically, the continue statement stops the current iteration and continues with the next one. i = 1 while i <= 5 : print ( i ) i += 1 print ( \"the end\" ) 1 2 3 4 5 the end","title":"While-loops"},{"location":"PR/pyfor/","text":"Looping statements allow for the repeated execution of a section of code. For instance, suppose we wanted to add up all of the integers between zero (0) and ten (10), not including ten. We could, of course, do this in one line, but we could also use a loop to add each integer one at a time. Below is the code for a simple accumulator that accomplishes this: sum = 0 for i in range ( 10 ): sum = sum + i print ( sum ) alternative_sum = 0 + 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 print ( alternative_sum == sum ) 45 True Range function The range () built-in function generates the sequence of values that we loop over, and notice that range(10) does not include 10 itself. In order to output the range as a list, we need to explicitly convert it to a list, using the list() function. If range is called with one argument, it produces an object with values from 0 to that argument. If it is called with two arguments, it produces values from the first to the second. range can have a third argument, which determines the interval of the sequence produced, also called the step. We can also create list of decreasing numbers, using a negative number as the third argument. numbers = list ( range ( 10 )) print ( numbers ) numbers = list ( range ( 5 , 10 )) print ( numbers ) numbers = list ( range ( 5 , 10 , 2 )) print ( numbers ) print ( list ( range ( 20 , 5 , - 2 ))) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [5, 6, 7, 8, 9] [5, 7, 9] [20, 18, 16, 14, 12, 10, 8, 6] elements in lists In addition to looping over a sequence of integers using the range() function, we can also loop over the elements in a list, which is shown below: ingredients = [ \"flour\" , \"sugar\" , \"eggs\" , \"oil\" , \"baking soda\" ] for ingredient in ingredients : print ( ingredient ) flour sugar eggs oil baking soda Above, the for-loop iterates over the elements of the list ingredients , and within the loop each of those elements is referred to as ingredient . The use of singular/plural nouns to handle this iteration is a common Python motif, but is by no means necessary to use in your own programming. Break and Continue Similar to while loops, the break and continue statements can be used in for loops, to stop the loop or jump to the next iteration. # use the break and continue statements for x in range ( 5 , 10 ): if ( x == 7 ): break if ( x % 2 == 0 ): continue print ( x ) #using the enumerate() function to get index days = [ \"Mon\" , \"Tue\" , \"Wed\" , \"Thu\" , \"Fri\" , \"Sat\" , \"Sun\" ] for i , d in enumerate ( days ): print ( i , d ) 5 0 Mon 1 Tue 2 Wed 3 Thu 4 Fri 5 Sat 6 Sun","title":"For-loops"},{"location":"PR/pyfor/#range-function","text":"The range () built-in function generates the sequence of values that we loop over, and notice that range(10) does not include 10 itself. In order to output the range as a list, we need to explicitly convert it to a list, using the list() function. If range is called with one argument, it produces an object with values from 0 to that argument. If it is called with two arguments, it produces values from the first to the second. range can have a third argument, which determines the interval of the sequence produced, also called the step. We can also create list of decreasing numbers, using a negative number as the third argument. numbers = list ( range ( 10 )) print ( numbers ) numbers = list ( range ( 5 , 10 )) print ( numbers ) numbers = list ( range ( 5 , 10 , 2 )) print ( numbers ) print ( list ( range ( 20 , 5 , - 2 ))) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [5, 6, 7, 8, 9] [5, 7, 9] [20, 18, 16, 14, 12, 10, 8, 6]","title":"Range function"},{"location":"PR/pyfor/#elements-in-lists","text":"In addition to looping over a sequence of integers using the range() function, we can also loop over the elements in a list, which is shown below: ingredients = [ \"flour\" , \"sugar\" , \"eggs\" , \"oil\" , \"baking soda\" ] for ingredient in ingredients : print ( ingredient ) flour sugar eggs oil baking soda Above, the for-loop iterates over the elements of the list ingredients , and within the loop each of those elements is referred to as ingredient . The use of singular/plural nouns to handle this iteration is a common Python motif, but is by no means necessary to use in your own programming.","title":"elements in lists"},{"location":"PR/pyfor/#break-and-continue","text":"Similar to while loops, the break and continue statements can be used in for loops, to stop the loop or jump to the next iteration. # use the break and continue statements for x in range ( 5 , 10 ): if ( x == 7 ): break if ( x % 2 == 0 ): continue print ( x ) #using the enumerate() function to get index days = [ \"Mon\" , \"Tue\" , \"Wed\" , \"Thu\" , \"Fri\" , \"Sat\" , \"Sun\" ] for i , d in enumerate ( days ): print ( i , d ) 5 0 Mon 1 Tue 2 Wed 3 Thu 4 Fri 5 Sat 6 Sun","title":"Break and Continue"},{"location":"lib/pyBisect/","text":"The purpose of Bisect algorithm is to find a position in list where an element needs to be inserted to keep the list sorted. Python in its definition provides the bisect algorithms using the module \u201cbisect\u201d which allows to keep the list in sorted order after insertion of each element. This is essential as this reduces overhead time required to sort the list again and again after insertion of each element. Important Bisection Functions bisect(list, num, beg, end) :- This function returns the position in the sorted list, where the number passed in argument can be placed so as to maintain the resultant list in sorted order. If the element is already present in the list, the right most position where element has to be inserted is returned. This function takes 4 arguments, list which has to be worked with, number to insert, starting position in list to consider, ending position which has to be considered. bisect_left(list, num, beg, end) :- This function returns the position in the sorted list, where the number passed in argument can be placed so as to maintain the resultant list in sorted order. If the element is already present in the list, the left most position where element has to be inserted is returned. This function takes 4 arguments, list which has to be worked with, number to insert, starting position in list to consider, ending position which has to be considered. bisect_right(list, num, beg, end) :- This function works similar to the \u201cbisect()\u201d and mentioned above. insort(list, num, beg, end) :- This function returns the sorted list after inserting number in appropriate position, if the element is already present in the list, the element is inserted at the rightmost possible position. This function takes 4 arguments, list which has to be worked with, number to insert, starting position in list to consider, ending position which has to be considered. insort_left(list, num, beg, end) :- This function returns the sorted list after inserting number in appropriate position, if the element is already present in the list, the element is inserted at the leftmost possible position. This function takes 4 arguments, list which has to be worked with, number to insert, starting position in list to consider, ending position which has to be considered. insort_right(list, num, beg, end) :- This function works similar to the \u201cinsort()\u201d as mentioned above. # use the bisection functions to maintain a list in sorted order import bisect values = [ 5 , 7 , 13 , 20 , 25 , 31 , 36 , 43 , 47 , 49 , 50 , 75 ] # exercise the left and right bisection routines print ( bisect . bisect ( values , 25 )) print ( bisect . bisect_right ( values , 25 )) print ( bisect . bisect_left ( values , 25 )) # use insort to insert new items bisect . insort_right ( values , 25 ) # will insert to the right print ( values ) # bisect can be used as an array lookup using breakpoints breakpoints = [ 60 , 70 , 80 , 90 ] gradeLetters = 'FDCBA' scores = [ 81 , 68 , 53 , 91 , 90 , 82 , 76 , 71 , 84 ] def calcGrade ( score ): # use the bisect function to identify cutoff points for the letter grades i = bisect . bisect ( breakpoints , score ) return gradeLetters [ i ] results = [ calcGrade ( score ) for score in scores ] print ( results ) 5 5 4 [5, 7, 13, 20, 25, 25, 31, 36, 43, 47, 49, 50, 75] ['B', 'D', 'F', 'A', 'A', 'B', 'C', 'C', 'B']","title":"Bisect"},{"location":"lib/pyEmoji/","text":"pip install emoji pip install rich print ( ' \\N{JUGGLING} ' ) #Unicode Name print ( ' \\U0001F939 ' ) #Unicode Codepoint print ( '\ud83e\udd39' ) #Literal Character import emoji , rich print ( emoji . emojize ( ':person_juggling:' )) #pip install emoji rich . print ( ':person_juggling:' ) #pip install rich \ud83e\udd39 \ud83e\udd39 \ud83e\udd39 \ud83e\udd39 \ud83e\udd39","title":"Emoji"},{"location":"lib/pyGen/","text":"Modules are pieces of code that other people have written to fulfill common tasks, such as generating random numbers, performing mathematical operations, etc. There are three main types of modules in Python, those you write yourself, those you install from external sources, and those that are preinstalled with Python. The last type is called the standard library , and contains many useful modules. Some of the standard library's useful modules include string , re , datetime , math , random , os , multiprocessing , subprocess , socket , email , json , doctest , unittest , pdb , argparse and sys . Python 3 Module of the Week shows how to use the modules of the Python 3 standard library. Many third-party Python modules are stored on the Python Package Index (PyPI) . The best way to install these is using a program called pip. It's important to enter pip commands at the command line, not the Python interpreter. The basic way to use a module is to add import module_name at the top of your code, and then using module_name.var to access functions and values with the name var in the module. Use a comma separated list to import multiple objects. Text Processing Services string \u2014 Common string operations re \u2014 Regular expression operations Functional Programming Modules itertools \u2014 Functions creating iterators for efficient looping functools \u2014 Higher-order functions and operations on callable objects operator \u2014 Standard operators as functions DataTypes datetime \u2014 Basic date and time types calendar \u2014 General calendar-related functions Collections \u2014 Container datatypes heapq \u2014 Heap queue algorithm bisect \u2014 Array bisection algorithm array \u2014 Efficient arrays of numeric values pprint \u2014 Data pretty printer enum \u2014 Support for enumerations Numeric and Mathematical Modules numbers \u2014 Numeric abstract base classes math \u2014 Mathematical functions cmath \u2014 Mathematical functions for complex numbers decimal \u2014 Decimal fixed point and floating point arithmetic fractions \u2014 Rational numbers random \u2014 Generate pseudo-random numbers statistics \u2014 Mathematical statistics functions File Management Fast IO OS.path \u2014 Common pathname manipulations glob \u2014 Unix style pathname pattern expansion shutil \u2014 High-level file operations PyFileSystem \u2014 Common interface to any filesystem. Data Persistence pickle \u2014 Python object serialization json \u2014 JSON encoder and decoder csv \u2014 CSV File Reading and Writing Graphic User Interface GUI with Tk emoji \u2014 Several ways to print emojis Interactivity with ipywidgets \u2014 Interactive browser controls for Jupyter notebooks Networking and Internet requests \u2014 HTTP for humans BeautifulSoup \u2014 Parsing HTML and XML documents Machine Learning Pandas \u2014 Read and manipulate data Numpy \u2014 Manipulating lists and tables of numerical data Numba \u2014 Makes Python code fast Matplotlib \u2014 For creating static, animated, and interactive visualizations in Python Scikit-learn \u2014 Machine learning in Python Parallel Computing Threads Processes Asyncio Development Tools Debugging and Profiling pdb \u2014 The Python debugger Time Profiling Memory Profiling Testing Software Logging Software Packaging and Distribution","title":"General"},{"location":"lib/pyIO/","text":"File Management Opening Files The argument of the open function is the path to the file. If the file is in the current working directory of the program, you can specify only its name. You can specify the mode used to open a file by applying a second argument to the open function. Sending \"r\" means open in read mode, which is the default. Sending \"w\" means write mode, for rewriting the contents of a file. Sending \"a\" means append mode, for adding new content to the end of the file. Adding \"b\" to a mode opens it in binary mode, which is used for non-text files (such as image and sound files). Once a file has been opened and used, you should close it. This is done with the close method of the file object. Reading Files file.read(): returns all the contents file.readlines():To retrieve each line in a file, you can use the readlines method to return a list in which each element is a line in the file. Writing Files To write to files you use the write method The write method returns the number of bytes written to a file, if successful. try : myfile = open ( \"filename.txt\" , 'r+' ) #mode read and write #read all content cont = myfile . read () myfile . write ( \"New line to insert in file\" ) print ( cont ) finally : myfile . close () myfile = open ( \"textfile.txt\" , 'r+' ) #mode read and write #print each line for line in myfile : print ( line . rstrip ()) myfile . close () This is line 1 This is line 2 This is line 3 This is line 4 This is line 5 This is line 6 This is line 7 This is line 8 This is line 9 This is line 10 An alternative way of doing this is using with statements. This creates a temporary variable (often called f), which is only accessible in the indented block of the with statement. The file is automatically closed at the end of the with statement, even if exceptions occur within it. with open ( \"filename.txt\" ) as f : print ( f . read ()) # Open a file for writing and create it if it doesn't exist f = open ( \"textfile.txt\" , \"w+\" ) # Open the file for appending text to the end # f = open(\"textfile.txt\",\"a+\") # write some lines of data to the file for i in range ( 10 ): f . write ( \"This is line %d \\r\\n \" % ( i + 1 )) # close the file when done f . close () # Open the file back up and read the contents f = open ( \"textfile.txt\" , \"r\" ) if f . mode == 'r' : # check to make sure that the file was opened # use the read() function to read the entire file # contents = f.read() # print (contents) fl = f . readlines () # readlines reads the individual lines into a list for x in fl : print ( x ) This is line 1 This is line 2 This is line 3 This is line 4 This is line 5 This is line 6 This is line 7 This is line 8 This is line 9 This is line 10 Fast Input/Output # Program to show time taken in fast # I / O and normal I / O in python from sys import stdin , stdout import time # Function for fast I / O def fastIO (): # Stores the start time start = time . perf_counter () # To read single integer n = stdin . readline () # To input array arr = [ int ( x ) for x in stdin . readline () . split ()] # Output integer stdout . write ( str ( n )) # Output array stdout . write ( \" \" . join ( map ( str , arr )) + \" \\n \" ) # Stores the end time end = time . perf_counter () print ( \"Time taken in fast IO\" , end - start ) # Function for normal I / O def normalIO (): # Stores the start time start = time . perf_counter () # Input integer n = int ( input ()) # Input array arr = [ int ( x ) for x in input () . split ()] # Output integer print ( n ) # Output array for i in arr : print ( i , end = \" \" ) print () # Stores the end time end = time . perf_counter () print ( \"Time taken in normal IO: \" , end - start ) # Driver Code if __name__ == '__main__' : fastIO () normalIO () Time taken in fast IO 0.001823046999994915 23 23 23 23 Time taken in normal IO: 5.173487307000002 OS Path Module import os from os import path import datetime from datetime import date , time , timedelta import time # Print the name of the OS print ( os . name ) # Check for item existence and type print ( \"Item exists: \" , path . exists ( \"textfile.txt\" )) print ( \"Item is a file: \" , path . isfile ( \"textfile.txt\" )) print ( \"Item is a directory: \" , path . isdir ( \"textfile.txt\" )) # Work with file paths print ( \"Item's path: \" , path . realpath ( \"textfile.txt\" )) print ( \"Item's path and name: \" , path . split ( path . realpath ( \"textfile.txt\" ))) # Get the modification time t = time . ctime ( path . getmtime ( \"textfile.txt\" )) print ( t ) print ( datetime . datetime . fromtimestamp ( path . getmtime ( \"textfile.txt\" ))) # Calculate how long ago the item was modified td = datetime . datetime . now () - datetime . datetime . fromtimestamp ( path . getmtime ( \"textfile.txt\" )) print ( \"It has been \" + str ( td ) + \" since the file was modified\" ) print ( \"Or, \" + str ( td . total_seconds ()) + \" seconds\" ) posix Item exists: True Item is a file: True Item is a directory: False Item's path: /content/textfile.txt Item's path and name: ('/content', 'textfile.txt') Mon Jul 19 14:31:57 2021 2021-07-19 14:31:57.728190 It has been 0:06:55.454337 since the file was modified Or, 415.454337 seconds GLOB Module Glob module searches all path names looking for files matching a specified pattern according to the rules dictated by the Unix shell. Results so obtained are returned in arbitrary order. Some requirements need traversal through a list of files at some location, mostly having a specific pattern. Python\u2019s glob module has several functions that can help in listing files that match a given pattern under a specified folder. Pattern Rules Follow standard Unix path expansion rules. Special characters supported : two different wild-cards- *, ? and character ranges expressed in []. The pattern rules are applied to segments of the filename (stopping at the path separator, /). Paths in the pattern can be relative or absolute. Application It is useful in any situation where your program needs to look for a list of files on the file system with names matching a pattern. If you need a list of filenames that have a certain extension, prefix, or any common string in the middle, use glob instead of writing code to scan the directory contents yourself. import glob # search .py files # in the current working directory for py in glob . glob ( \"*.py\" ): print ( py ) import glob # Using character ranges [] print ( 'Finding file using character ranges [] :- ' ) print ( glob . glob ( './[0-9].*' )) # Using wildcard character * print ( ' \\n Finding file using wildcard character * :- ' ) print ( glob . glob ( '*.gif' )) # Using wildcard character ? print ( ' \\n Finding file using wildcard character ? :- ' ) print ( glob . glob ( '?.gif' )) # Using recursive attribute print ( ' \\n Finding files using recursive attribute :- ' ) print ( glob . glob ( '**/*.txt' , recursive = True )) Shutil Module: Filesystem Shell Methods import os from os import path import shutil from shutil import make_archive from zipfile import ZipFile # make a duplicate of an existing file if path . exists ( \"textfile.txt\" ): # get the path to the file in the current directory src = path . realpath ( \"textfile.txt\" ); # # let's make a backup copy by appending \"bak\" to the name dst = src + \".bak\" # # now use the shell to make a copy of the file shutil . copy ( src , dst ) # # copy over the permissions, modification times, and other info shutil . copystat ( src , dst ) # # rename the original file os . rename ( \"textfile.txt\" , \"newfile.txt\" ) # now put things into a ZIP archive root_dir , tail = path . split ( src ) shutil . make_archive ( \"archive\" , \"zip\" , root_dir ) # more fine-grained control over ZIP files with ZipFile ( \"testzip.zip\" , \"w\" ) as newzip : newzip . write ( \"newfile.txt\" ) newzip . write ( \"textfile.txt.bak\" ) PyFileSystem pip install fs pip show fs Name: fs Version: 2.4.14 Summary: Python's filesystem abstraction layer Home-page: https://github.com/PyFilesystem/pyfilesystem2 Author: Will McGugan Author-email: will@willmcgugan.com License: MIT Location: /usr/local/lib/python3.7/dist-packages Requires: appdirs, six, setuptools, pytz Required-by: # import the PyFilesystem library for OS files from fs.osfs import OSFS # TODO: open a local filesystem for the current directory with OSFS ( \".\" ) as myfs : if ( not myfs . exists ( \"testdir\" )): # create a sample data directory myfs . makedir ( \"testdir\" ) # create a file with myfs . open ( \"testdir/samplefile.txt\" , mode = 'w' ) as f : f . write ( \"This is some text\" ) # read the file contents with myfs . open ( \"testdir/samplefile.txt\" ) as f : content = f . read () print ( content ) # TODO: use the getinfo() function to return resource information info = myfs . getinfo ( \"testdir/samplefile.txt\" , namespaces = [ 'details' ]) print ( info . name ) print ( info . is_dir ) print ( info . size ) print ( info . type ) print ( info . modified ) samplefile.txt False 17 ResourceType.file 2021-12-23 04:09:10.571525+00:00 # TODO: try opening and reading a ZIP archive from fs.zipfs import ZipFS with ZipFS ( \"FileExamples.zip\" ) as thezip : if ( thezip . exists ( \"FileExamples/File1.txt\" )): with thezip . open ( \"FileExamples/File1.txt\" ) as f : content = f . read () print ( content ) from fs.osfs import OSFS # TODO: print a directory tree listing with OSFS ( \".\" ) as myfs : myfs . tree () |-- .config | |-- configurations | | `-- config_default | |-- logs | | `-- 2021.12.03 | | |-- 14.32.30.027140.log | | |-- 14.32.50.522723.log | | |-- 14.33.09.955489.log | | |-- 14.33.16.964195.log | | |-- 14.33.36.903459.log | | `-- 14.33.37.701606.log | |-- .last_opt_in_prompt.yaml | |-- .last_survey_prompt.yaml | |-- .last_update_check.json | |-- active_config | |-- config_sentinel | `-- gce |-- sample_data | |-- anscombe.json | |-- california_housing_test.csv | |-- california_housing_train.csv | |-- mnist_test.csv | |-- mnist_train_small.csv | `-- README.md `-- testdir `-- samplefile.txt # TODO: use directory operation functions # with OSFS(\".\") as myfs: # dirlist = myfs.listdir(\"testdir\") # print(dirlist) # with OSFS(\".\") as myfs: # dirlist = list(myfs.scandir(\"testdir\")) # print(dirlist) with OSFS ( \".\" ) as myfs : dirlist = list ( myfs . filterdir ( \"testdir\" , files = [ \"*.txt\" ])) print ( dirlist ) # TODO: Use resource info with scandir with OSFS ( \".\" ) as myfs : dirlist = myfs . scandir ( \"testdir\" , namespaces = [ \"details\" ]) for info in dirlist : print ( info . name , info . size ) # TODO: make a copy of a directory # with OSFS(\".\") as myfs: # myfs.copydir(\"testdir\", \"CopyOftestdir\", create=True) # TODO: remove a directory # with OSFS(\".\") as myfs: # if (myfs.exists(\"CopyOftestdir\")): # # myfs.removedir(\"CopyOftestdir\") # myfs.removetree(\"CopyOftestdir\") [<file 'samplefile.txt'>] samplefile.txt 17 # Example file for using the File System walker from fs.osfs import OSFS from fs.zipfs import ZipFS # create a basic file walker with OSFS ( \".\" ) as myfs : print ( \"-- Files --\" ) # TODO: use the files walker to process files for path in myfs . walk . files ( filter = [ \"*.txt\" ]): print ( path ) print ( \"-- Directories --\" ) # TODO: use the dirs walker for directories for path in myfs . walk . dirs (): print ( path ) # TODO: use the info property to step through items # with OSFS(\".\") as myfs: # for path, info in myfs.walk.info(namespaces=[\"details\"]): # print(path, info.is_dir, info.size) # TODO: Use the walk object by itself: # with OSFS(\"FileExamples\") as myfs: # for step in myfs.walk(): # print(step.path) # print(step.files) # print(step.dirs) # TODO: Use the walker with a ZIP # with ZipFS(\"FileExamples.zip\") as thezip: # print(\"-- Zip Contents --\") # for path in thezip.walk.files(): # print(path) -- Files -- /testdir/samplefile.txt -- Directories -- /.config /testdir /sample_data /.config/logs /.config/configurations /.config/logs/2021.12.03 from fs.osfs import OSFS # Challenge - figure out the total size of all text files in a folder structure totalsize = 0 # Create a file walker to walk the FileExamples directory with OSFS ( \".\" ) as myfs : # We need to specify the details namespace to get size info for path , info in myfs . walk . info ( namespaces = [ \"details\" ]): # Check for an ending extension of .txt if path . endswith ( \".txt\" ) and not info . is_dir : totalsize += info . size # print the final results print ( \"Total size of files is: {0} \" . format ( totalsize )) Total size of files is: 17 CSV IO # importing the csv module import csv # my data rows as dictionary objects mydict = [{ 'branch' : 'COE' , 'cgpa' : '9.0' , 'name' : 'Nikhil' , 'year' : '2' }, { 'branch' : 'COE' , 'cgpa' : '9.1' , 'name' : 'Sanchit' , 'year' : '2' }, { 'branch' : 'IT' , 'cgpa' : '9.3' , 'name' : 'Aditya' , 'year' : '2' }, { 'branch' : 'SE' , 'cgpa' : '9.5' , 'name' : 'Sagar' , 'year' : '1' }, { 'branch' : 'MCE' , 'cgpa' : '7.8' , 'name' : 'Prateek' , 'year' : '3' }, { 'branch' : 'EP' , 'cgpa' : '9.1' , 'name' : 'Sahil' , 'year' : '2' }] # field names fields = [ 'name' , 'branch' , 'year' , 'cgpa' ] # name of csv file filename = \"university_records.csv\" # writing to csv file with open ( filename , 'w' ) as csvfile : # creating a csv dict writer object writer = csv . DictWriter ( csvfile , fieldnames = fields ) # writing headers (field names) writer . writeheader () # writing data rows writer . writerows ( mydict ) # importing csv module import csv # csv file name filename = \"university_records.csv\" # initializing the titles and rows list fields = [] rows = [] # reading csv file with open ( filename , 'r' ) as csvfile : # creating a csv reader object csvreader = csv . reader ( csvfile ) # extracting field names through first row fields = next ( csvreader ) # extracting each data row one by one for row in csvreader : rows . append ( row ) # get total number of rows print ( \"Total no. of rows: %d \" % ( csvreader . line_num )) # printing the field names print ( 'Field names are:' + ', ' . join ( field for field in fields )) # printing first 5 rows print ( ' \\n First 5 rows are: \\n ' ) for row in rows [: 5 ]: # parsing each column of a row for col in row : print ( \" %10s \" % col ), print ( ' \\n ' ) Total no. of rows: 7 Field names are:name, branch, year, cgpa First 5 rows are: Nikhil COE 2 9.0 Sanchit COE 2 9.1 Aditya IT 2 9.3 Sagar SE 1 9.5 Prateek MCE 3 7.8","title":"IO & OS"},{"location":"lib/pyIO/#file-management","text":"","title":"File Management"},{"location":"lib/pyIO/#opening-files","text":"The argument of the open function is the path to the file. If the file is in the current working directory of the program, you can specify only its name. You can specify the mode used to open a file by applying a second argument to the open function. Sending \"r\" means open in read mode, which is the default. Sending \"w\" means write mode, for rewriting the contents of a file. Sending \"a\" means append mode, for adding new content to the end of the file. Adding \"b\" to a mode opens it in binary mode, which is used for non-text files (such as image and sound files). Once a file has been opened and used, you should close it. This is done with the close method of the file object.","title":"Opening Files"},{"location":"lib/pyIO/#reading-files","text":"file.read(): returns all the contents file.readlines():To retrieve each line in a file, you can use the readlines method to return a list in which each element is a line in the file.","title":"Reading Files"},{"location":"lib/pyIO/#writing-files","text":"To write to files you use the write method The write method returns the number of bytes written to a file, if successful. try : myfile = open ( \"filename.txt\" , 'r+' ) #mode read and write #read all content cont = myfile . read () myfile . write ( \"New line to insert in file\" ) print ( cont ) finally : myfile . close () myfile = open ( \"textfile.txt\" , 'r+' ) #mode read and write #print each line for line in myfile : print ( line . rstrip ()) myfile . close () This is line 1 This is line 2 This is line 3 This is line 4 This is line 5 This is line 6 This is line 7 This is line 8 This is line 9 This is line 10 An alternative way of doing this is using with statements. This creates a temporary variable (often called f), which is only accessible in the indented block of the with statement. The file is automatically closed at the end of the with statement, even if exceptions occur within it. with open ( \"filename.txt\" ) as f : print ( f . read ()) # Open a file for writing and create it if it doesn't exist f = open ( \"textfile.txt\" , \"w+\" ) # Open the file for appending text to the end # f = open(\"textfile.txt\",\"a+\") # write some lines of data to the file for i in range ( 10 ): f . write ( \"This is line %d \\r\\n \" % ( i + 1 )) # close the file when done f . close () # Open the file back up and read the contents f = open ( \"textfile.txt\" , \"r\" ) if f . mode == 'r' : # check to make sure that the file was opened # use the read() function to read the entire file # contents = f.read() # print (contents) fl = f . readlines () # readlines reads the individual lines into a list for x in fl : print ( x ) This is line 1 This is line 2 This is line 3 This is line 4 This is line 5 This is line 6 This is line 7 This is line 8 This is line 9 This is line 10","title":"Writing Files"},{"location":"lib/pyIO/#fast-inputoutput","text":"# Program to show time taken in fast # I / O and normal I / O in python from sys import stdin , stdout import time # Function for fast I / O def fastIO (): # Stores the start time start = time . perf_counter () # To read single integer n = stdin . readline () # To input array arr = [ int ( x ) for x in stdin . readline () . split ()] # Output integer stdout . write ( str ( n )) # Output array stdout . write ( \" \" . join ( map ( str , arr )) + \" \\n \" ) # Stores the end time end = time . perf_counter () print ( \"Time taken in fast IO\" , end - start ) # Function for normal I / O def normalIO (): # Stores the start time start = time . perf_counter () # Input integer n = int ( input ()) # Input array arr = [ int ( x ) for x in input () . split ()] # Output integer print ( n ) # Output array for i in arr : print ( i , end = \" \" ) print () # Stores the end time end = time . perf_counter () print ( \"Time taken in normal IO: \" , end - start ) # Driver Code if __name__ == '__main__' : fastIO () normalIO () Time taken in fast IO 0.001823046999994915 23 23 23 23 Time taken in normal IO: 5.173487307000002","title":"Fast Input/Output"},{"location":"lib/pyIO/#os-path-module","text":"import os from os import path import datetime from datetime import date , time , timedelta import time # Print the name of the OS print ( os . name ) # Check for item existence and type print ( \"Item exists: \" , path . exists ( \"textfile.txt\" )) print ( \"Item is a file: \" , path . isfile ( \"textfile.txt\" )) print ( \"Item is a directory: \" , path . isdir ( \"textfile.txt\" )) # Work with file paths print ( \"Item's path: \" , path . realpath ( \"textfile.txt\" )) print ( \"Item's path and name: \" , path . split ( path . realpath ( \"textfile.txt\" ))) # Get the modification time t = time . ctime ( path . getmtime ( \"textfile.txt\" )) print ( t ) print ( datetime . datetime . fromtimestamp ( path . getmtime ( \"textfile.txt\" ))) # Calculate how long ago the item was modified td = datetime . datetime . now () - datetime . datetime . fromtimestamp ( path . getmtime ( \"textfile.txt\" )) print ( \"It has been \" + str ( td ) + \" since the file was modified\" ) print ( \"Or, \" + str ( td . total_seconds ()) + \" seconds\" ) posix Item exists: True Item is a file: True Item is a directory: False Item's path: /content/textfile.txt Item's path and name: ('/content', 'textfile.txt') Mon Jul 19 14:31:57 2021 2021-07-19 14:31:57.728190 It has been 0:06:55.454337 since the file was modified Or, 415.454337 seconds","title":"OS Path Module"},{"location":"lib/pyIO/#glob-module","text":"Glob module searches all path names looking for files matching a specified pattern according to the rules dictated by the Unix shell. Results so obtained are returned in arbitrary order. Some requirements need traversal through a list of files at some location, mostly having a specific pattern. Python\u2019s glob module has several functions that can help in listing files that match a given pattern under a specified folder.","title":"GLOB Module"},{"location":"lib/pyIO/#pattern-rules","text":"Follow standard Unix path expansion rules. Special characters supported : two different wild-cards- *, ? and character ranges expressed in []. The pattern rules are applied to segments of the filename (stopping at the path separator, /). Paths in the pattern can be relative or absolute.","title":"Pattern Rules"},{"location":"lib/pyIO/#application","text":"It is useful in any situation where your program needs to look for a list of files on the file system with names matching a pattern. If you need a list of filenames that have a certain extension, prefix, or any common string in the middle, use glob instead of writing code to scan the directory contents yourself. import glob # search .py files # in the current working directory for py in glob . glob ( \"*.py\" ): print ( py ) import glob # Using character ranges [] print ( 'Finding file using character ranges [] :- ' ) print ( glob . glob ( './[0-9].*' )) # Using wildcard character * print ( ' \\n Finding file using wildcard character * :- ' ) print ( glob . glob ( '*.gif' )) # Using wildcard character ? print ( ' \\n Finding file using wildcard character ? :- ' ) print ( glob . glob ( '?.gif' )) # Using recursive attribute print ( ' \\n Finding files using recursive attribute :- ' ) print ( glob . glob ( '**/*.txt' , recursive = True ))","title":"Application"},{"location":"lib/pyIO/#shutil-module-filesystem-shell-methods","text":"import os from os import path import shutil from shutil import make_archive from zipfile import ZipFile # make a duplicate of an existing file if path . exists ( \"textfile.txt\" ): # get the path to the file in the current directory src = path . realpath ( \"textfile.txt\" ); # # let's make a backup copy by appending \"bak\" to the name dst = src + \".bak\" # # now use the shell to make a copy of the file shutil . copy ( src , dst ) # # copy over the permissions, modification times, and other info shutil . copystat ( src , dst ) # # rename the original file os . rename ( \"textfile.txt\" , \"newfile.txt\" ) # now put things into a ZIP archive root_dir , tail = path . split ( src ) shutil . make_archive ( \"archive\" , \"zip\" , root_dir ) # more fine-grained control over ZIP files with ZipFile ( \"testzip.zip\" , \"w\" ) as newzip : newzip . write ( \"newfile.txt\" ) newzip . write ( \"textfile.txt.bak\" )","title":"Shutil Module: Filesystem Shell Methods"},{"location":"lib/pyIO/#pyfilesystem","text":"pip install fs pip show fs Name: fs Version: 2.4.14 Summary: Python's filesystem abstraction layer Home-page: https://github.com/PyFilesystem/pyfilesystem2 Author: Will McGugan Author-email: will@willmcgugan.com License: MIT Location: /usr/local/lib/python3.7/dist-packages Requires: appdirs, six, setuptools, pytz Required-by: # import the PyFilesystem library for OS files from fs.osfs import OSFS # TODO: open a local filesystem for the current directory with OSFS ( \".\" ) as myfs : if ( not myfs . exists ( \"testdir\" )): # create a sample data directory myfs . makedir ( \"testdir\" ) # create a file with myfs . open ( \"testdir/samplefile.txt\" , mode = 'w' ) as f : f . write ( \"This is some text\" ) # read the file contents with myfs . open ( \"testdir/samplefile.txt\" ) as f : content = f . read () print ( content ) # TODO: use the getinfo() function to return resource information info = myfs . getinfo ( \"testdir/samplefile.txt\" , namespaces = [ 'details' ]) print ( info . name ) print ( info . is_dir ) print ( info . size ) print ( info . type ) print ( info . modified ) samplefile.txt False 17 ResourceType.file 2021-12-23 04:09:10.571525+00:00 # TODO: try opening and reading a ZIP archive from fs.zipfs import ZipFS with ZipFS ( \"FileExamples.zip\" ) as thezip : if ( thezip . exists ( \"FileExamples/File1.txt\" )): with thezip . open ( \"FileExamples/File1.txt\" ) as f : content = f . read () print ( content ) from fs.osfs import OSFS # TODO: print a directory tree listing with OSFS ( \".\" ) as myfs : myfs . tree () |-- .config | |-- configurations | | `-- config_default | |-- logs | | `-- 2021.12.03 | | |-- 14.32.30.027140.log | | |-- 14.32.50.522723.log | | |-- 14.33.09.955489.log | | |-- 14.33.16.964195.log | | |-- 14.33.36.903459.log | | `-- 14.33.37.701606.log | |-- .last_opt_in_prompt.yaml | |-- .last_survey_prompt.yaml | |-- .last_update_check.json | |-- active_config | |-- config_sentinel | `-- gce |-- sample_data | |-- anscombe.json | |-- california_housing_test.csv | |-- california_housing_train.csv | |-- mnist_test.csv | |-- mnist_train_small.csv | `-- README.md `-- testdir `-- samplefile.txt # TODO: use directory operation functions # with OSFS(\".\") as myfs: # dirlist = myfs.listdir(\"testdir\") # print(dirlist) # with OSFS(\".\") as myfs: # dirlist = list(myfs.scandir(\"testdir\")) # print(dirlist) with OSFS ( \".\" ) as myfs : dirlist = list ( myfs . filterdir ( \"testdir\" , files = [ \"*.txt\" ])) print ( dirlist ) # TODO: Use resource info with scandir with OSFS ( \".\" ) as myfs : dirlist = myfs . scandir ( \"testdir\" , namespaces = [ \"details\" ]) for info in dirlist : print ( info . name , info . size ) # TODO: make a copy of a directory # with OSFS(\".\") as myfs: # myfs.copydir(\"testdir\", \"CopyOftestdir\", create=True) # TODO: remove a directory # with OSFS(\".\") as myfs: # if (myfs.exists(\"CopyOftestdir\")): # # myfs.removedir(\"CopyOftestdir\") # myfs.removetree(\"CopyOftestdir\") [<file 'samplefile.txt'>] samplefile.txt 17 # Example file for using the File System walker from fs.osfs import OSFS from fs.zipfs import ZipFS # create a basic file walker with OSFS ( \".\" ) as myfs : print ( \"-- Files --\" ) # TODO: use the files walker to process files for path in myfs . walk . files ( filter = [ \"*.txt\" ]): print ( path ) print ( \"-- Directories --\" ) # TODO: use the dirs walker for directories for path in myfs . walk . dirs (): print ( path ) # TODO: use the info property to step through items # with OSFS(\".\") as myfs: # for path, info in myfs.walk.info(namespaces=[\"details\"]): # print(path, info.is_dir, info.size) # TODO: Use the walk object by itself: # with OSFS(\"FileExamples\") as myfs: # for step in myfs.walk(): # print(step.path) # print(step.files) # print(step.dirs) # TODO: Use the walker with a ZIP # with ZipFS(\"FileExamples.zip\") as thezip: # print(\"-- Zip Contents --\") # for path in thezip.walk.files(): # print(path) -- Files -- /testdir/samplefile.txt -- Directories -- /.config /testdir /sample_data /.config/logs /.config/configurations /.config/logs/2021.12.03 from fs.osfs import OSFS # Challenge - figure out the total size of all text files in a folder structure totalsize = 0 # Create a file walker to walk the FileExamples directory with OSFS ( \".\" ) as myfs : # We need to specify the details namespace to get size info for path , info in myfs . walk . info ( namespaces = [ \"details\" ]): # Check for an ending extension of .txt if path . endswith ( \".txt\" ) and not info . is_dir : totalsize += info . size # print the final results print ( \"Total size of files is: {0} \" . format ( totalsize )) Total size of files is: 17","title":"PyFileSystem"},{"location":"lib/pyIO/#csv-io","text":"# importing the csv module import csv # my data rows as dictionary objects mydict = [{ 'branch' : 'COE' , 'cgpa' : '9.0' , 'name' : 'Nikhil' , 'year' : '2' }, { 'branch' : 'COE' , 'cgpa' : '9.1' , 'name' : 'Sanchit' , 'year' : '2' }, { 'branch' : 'IT' , 'cgpa' : '9.3' , 'name' : 'Aditya' , 'year' : '2' }, { 'branch' : 'SE' , 'cgpa' : '9.5' , 'name' : 'Sagar' , 'year' : '1' }, { 'branch' : 'MCE' , 'cgpa' : '7.8' , 'name' : 'Prateek' , 'year' : '3' }, { 'branch' : 'EP' , 'cgpa' : '9.1' , 'name' : 'Sahil' , 'year' : '2' }] # field names fields = [ 'name' , 'branch' , 'year' , 'cgpa' ] # name of csv file filename = \"university_records.csv\" # writing to csv file with open ( filename , 'w' ) as csvfile : # creating a csv dict writer object writer = csv . DictWriter ( csvfile , fieldnames = fields ) # writing headers (field names) writer . writeheader () # writing data rows writer . writerows ( mydict ) # importing csv module import csv # csv file name filename = \"university_records.csv\" # initializing the titles and rows list fields = [] rows = [] # reading csv file with open ( filename , 'r' ) as csvfile : # creating a csv reader object csvreader = csv . reader ( csvfile ) # extracting field names through first row fields = next ( csvreader ) # extracting each data row one by one for row in csvreader : rows . append ( row ) # get total number of rows print ( \"Total no. of rows: %d \" % ( csvreader . line_num )) # printing the field names print ( 'Field names are:' + ', ' . join ( field for field in fields )) # printing first 5 rows print ( ' \\n First 5 rows are: \\n ' ) for row in rows [: 5 ]: # parsing each column of a row for col in row : print ( \" %10s \" % col ), print ( ' \\n ' ) Total no. of rows: 7 Field names are:name, branch, year, cgpa First 5 rows are: Nikhil COE 2 9.0 Sanchit COE 2 9.1 Aditya IT 2 9.3 Sagar SE 1 9.5 Prateek MCE 3 7.8","title":"CSV IO"},{"location":"lib/pyIter/","text":"The module itertools is a standard library that contains several functions that are useful in functional programming. Advanced iteration functions capabilities for: Infinite iterators Iterators terminating on the shortest input sequence Combinatoric iterators Infinite iterators Iterator Arguments Syntax Example count() start, [step] start, start+step, start+2*step, \u2026 count(10) --> 10 11 12 13 14 ... cycle() p p0, p1, \u2026 plast, p0, p1, \u2026 cycle('ABCD') --> A B C D A B C D ... repeat() elem [,n] elem, elem, elem, \u2026 endlessly or up to n times repeat(10, 3) --> 10 10 10 The function count() counts up infinitely from a value. from itertools import count for i in count ( 3 ): print ( i ) if i >= 11 : break 3 4 5 6 7 8 9 10 11 import itertools # use count to create a simple counter count1 = itertools . count ( 100 , 10 ) print ( next ( count1 )) print ( next ( count1 )) print ( next ( count1 )) 100 110 120 The function cycle() infinitely iterates through an iterable (for instance a list or string). import itertools # cycle iterator can be used to cycle over a collection seq1 = [ \"Joe\" , \"John\" , \"Mike\" ] cycle1 = itertools . cycle ( seq1 ) print ( next ( cycle1 )) print ( next ( cycle1 )) print ( next ( cycle1 )) print ( next ( cycle1 )) Joe John Mike Joe The function repeat() repeats an object, either infinitely or a specific number of times. A common use for repeat is to supply a stream of constant values to map or zip list ( map ( pow , range ( 10 ), itertools . repeat ( 2 ))) [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] Iterators terminating on the shortest input sequence All list Iterator Arguments Syntax Example accumulate() p [,func] p0, p0+p1, p0+p1+p2, \u2026 accumulate([1,2,3,4,5]) --> 1 3 6 10 15 dropwhile() pred, seq seq[n], seq[n+1], starting when pred fails dropwhile(lambda x: x<5, [1,4,6,4,1]) --> 6 4 1 takewhile() pred, seq seq[0], seq[1], until pred fails takewhile(lambda x: x<5, [1,4,6,4,1]) --> 1 4 chain() p, q, \u2026 p0, p1, \u2026 plast, q0, q1, \u2026 chain('ABC', 'DEF') --> A B C D E F chain.from_iterable() iterable p0, p1, \u2026 plast, q0, q1, \u2026 chain.from_iterable(['ABC', 'DEF']) --> A B C D E F groupby() iterable[, key] sub-iterators grouped by value of key(v) zip_longest() p, q, \u2026 (p[0], q[0]), (p[1], q[1]), \u2026 zip_longest('ABCD', 'xy', fillvalue='-') --> Ax By C- D- compress() data, selectors (d[0] if s[0]), (d[1] if s[1]), \u2026 compress('ABCDEF', [1,0,1,0,1,1]) --> A C E F filterfalse() pred, seq elements of seq where pred(elem) is false filterfalse(lambda x: x%2, range(10)) --> 0 2 4 6 8 islice() seq, [start,] stop [, step] elements from seq[start:stop:step] islice('ABCDEFG', 2, None) --> C D E F G pairwise() iterable (p[0], p[1]), (p[1], p[2]) pairwise('ABCDEFG') --> AB BC CD DE EF FG starmap() func, seq func( seq[0]), func( seq[1]), \u2026 starmap(pow, [(2,5), (3,2), (10,3)]) --> 32 9 1000 tee() it, n it1, it2, \u2026 itn splits one iterator into n accumulate() - returns a running total of values in an iterable from itertools import accumulate nums = list ( accumulate ( range ( 8 ))) print ( nums ) [0, 1, 3, 6, 10, 15, 21, 28] # accumulate creates an iterator that accumulates values vals = [ 10 , 20 , 30 , 40 , 50 , 40 , 30 ] acc = itertools . accumulate ( vals , max ) print ( list ( acc )) [10, 20, 30, 40, 50, 50, 50] takewhile() - takes items from an iterable while a predicate function remains true from itertools import accumulate , takewhile nums = list ( accumulate ( range ( 8 ))) print ( nums ) print ( list ( takewhile ( lambda x : x <= 6 , nums ))) [0, 1, 3, 6, 10, 15, 21, 28] [0, 1, 3, 6] dropwhile() - drops elements from the iterable as long as the predicate is true # advanced iteration functions in the itertools package import itertools def testFunction ( x ): return x < 40 def main (): # dropwhile and takewhile will return values until # a certain condition is met that stops them print ( list ( itertools . dropwhile ( testFunction , vals ))) print ( list ( itertools . takewhile ( testFunction , vals ))) if __name__ == \"__main__\" : main () [40, 50, 40, 30] [10, 20, 30] chain() - combines several iterables into one long one # advanced iteration functions in the itertools package import itertools def main (): # use chain to connect sequences together x = itertools . chain ( \"ABCD\" , \"1234\" ) print ( list ( x )) if __name__ == \"__main__\" : main () ['A', 'B', 'C', 'D', '1', '2', '3', '4'] Combinatoric iterators Product : cartesian product, equivalent to a nested for-loop print ( list ( itertools . product ( 'ABC' , repeat = 2 ))) [('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'), ('B', 'C'), ('C', 'A'), ('C', 'B'), ('C', 'C')] Permutations : r-length tuples, all possible orderings, no repeated elements print ( list ( itertools . permutations ( 'ABC' , 2 ))) [('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B')] from itertools import product , permutations letters = ( \"A\" , \"B\" ) print ( list ( product ( letters , range ( 2 )))) print ( list ( permutations ( letters ))) [('A', 0), ('A', 1), ('B', 0), ('B', 1)] [('A', 'B'), ('B', 'A')] Combinations : r-length tuples, in sorted order, no repeated elements print ( list ( itertools . combinations ( 'ABC' , 2 ))) [('A', 'B'), ('A', 'C'), ('B', 'C')] combinations_with_replacement : r-length tuples, in sorted order, with repeated elements print ( list ( itertools . combinations_with_replacement ( 'ABC' , 2 ))) [('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'B'), ('B', 'C'), ('C', 'C')]","title":"Itertools"},{"location":"lib/pyIter/#infinite-iterators","text":"Iterator Arguments Syntax Example count() start, [step] start, start+step, start+2*step, \u2026 count(10) --> 10 11 12 13 14 ... cycle() p p0, p1, \u2026 plast, p0, p1, \u2026 cycle('ABCD') --> A B C D A B C D ... repeat() elem [,n] elem, elem, elem, \u2026 endlessly or up to n times repeat(10, 3) --> 10 10 10 The function count() counts up infinitely from a value. from itertools import count for i in count ( 3 ): print ( i ) if i >= 11 : break 3 4 5 6 7 8 9 10 11 import itertools # use count to create a simple counter count1 = itertools . count ( 100 , 10 ) print ( next ( count1 )) print ( next ( count1 )) print ( next ( count1 )) 100 110 120 The function cycle() infinitely iterates through an iterable (for instance a list or string). import itertools # cycle iterator can be used to cycle over a collection seq1 = [ \"Joe\" , \"John\" , \"Mike\" ] cycle1 = itertools . cycle ( seq1 ) print ( next ( cycle1 )) print ( next ( cycle1 )) print ( next ( cycle1 )) print ( next ( cycle1 )) Joe John Mike Joe The function repeat() repeats an object, either infinitely or a specific number of times. A common use for repeat is to supply a stream of constant values to map or zip list ( map ( pow , range ( 10 ), itertools . repeat ( 2 ))) [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]","title":"Infinite iterators"},{"location":"lib/pyIter/#iterators-terminating-on-the-shortest-input-sequence","text":"All list Iterator Arguments Syntax Example accumulate() p [,func] p0, p0+p1, p0+p1+p2, \u2026 accumulate([1,2,3,4,5]) --> 1 3 6 10 15 dropwhile() pred, seq seq[n], seq[n+1], starting when pred fails dropwhile(lambda x: x<5, [1,4,6,4,1]) --> 6 4 1 takewhile() pred, seq seq[0], seq[1], until pred fails takewhile(lambda x: x<5, [1,4,6,4,1]) --> 1 4 chain() p, q, \u2026 p0, p1, \u2026 plast, q0, q1, \u2026 chain('ABC', 'DEF') --> A B C D E F chain.from_iterable() iterable p0, p1, \u2026 plast, q0, q1, \u2026 chain.from_iterable(['ABC', 'DEF']) --> A B C D E F groupby() iterable[, key] sub-iterators grouped by value of key(v) zip_longest() p, q, \u2026 (p[0], q[0]), (p[1], q[1]), \u2026 zip_longest('ABCD', 'xy', fillvalue='-') --> Ax By C- D- compress() data, selectors (d[0] if s[0]), (d[1] if s[1]), \u2026 compress('ABCDEF', [1,0,1,0,1,1]) --> A C E F filterfalse() pred, seq elements of seq where pred(elem) is false filterfalse(lambda x: x%2, range(10)) --> 0 2 4 6 8 islice() seq, [start,] stop [, step] elements from seq[start:stop:step] islice('ABCDEFG', 2, None) --> C D E F G pairwise() iterable (p[0], p[1]), (p[1], p[2]) pairwise('ABCDEFG') --> AB BC CD DE EF FG starmap() func, seq func( seq[0]), func( seq[1]), \u2026 starmap(pow, [(2,5), (3,2), (10,3)]) --> 32 9 1000 tee() it, n it1, it2, \u2026 itn splits one iterator into n accumulate() - returns a running total of values in an iterable from itertools import accumulate nums = list ( accumulate ( range ( 8 ))) print ( nums ) [0, 1, 3, 6, 10, 15, 21, 28] # accumulate creates an iterator that accumulates values vals = [ 10 , 20 , 30 , 40 , 50 , 40 , 30 ] acc = itertools . accumulate ( vals , max ) print ( list ( acc )) [10, 20, 30, 40, 50, 50, 50] takewhile() - takes items from an iterable while a predicate function remains true from itertools import accumulate , takewhile nums = list ( accumulate ( range ( 8 ))) print ( nums ) print ( list ( takewhile ( lambda x : x <= 6 , nums ))) [0, 1, 3, 6, 10, 15, 21, 28] [0, 1, 3, 6] dropwhile() - drops elements from the iterable as long as the predicate is true # advanced iteration functions in the itertools package import itertools def testFunction ( x ): return x < 40 def main (): # dropwhile and takewhile will return values until # a certain condition is met that stops them print ( list ( itertools . dropwhile ( testFunction , vals ))) print ( list ( itertools . takewhile ( testFunction , vals ))) if __name__ == \"__main__\" : main () [40, 50, 40, 30] [10, 20, 30] chain() - combines several iterables into one long one # advanced iteration functions in the itertools package import itertools def main (): # use chain to connect sequences together x = itertools . chain ( \"ABCD\" , \"1234\" ) print ( list ( x )) if __name__ == \"__main__\" : main () ['A', 'B', 'C', 'D', '1', '2', '3', '4']","title":"Iterators terminating on the shortest input sequence"},{"location":"lib/pyIter/#combinatoric-iterators","text":"Product : cartesian product, equivalent to a nested for-loop print ( list ( itertools . product ( 'ABC' , repeat = 2 ))) [('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'), ('B', 'C'), ('C', 'A'), ('C', 'B'), ('C', 'C')] Permutations : r-length tuples, all possible orderings, no repeated elements print ( list ( itertools . permutations ( 'ABC' , 2 ))) [('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B')] from itertools import product , permutations letters = ( \"A\" , \"B\" ) print ( list ( product ( letters , range ( 2 )))) print ( list ( permutations ( letters ))) [('A', 0), ('A', 1), ('B', 0), ('B', 1)] [('A', 'B'), ('B', 'A')] Combinations : r-length tuples, in sorted order, no repeated elements print ( list ( itertools . combinations ( 'ABC' , 2 ))) [('A', 'B'), ('A', 'C'), ('B', 'C')] combinations_with_replacement : r-length tuples, in sorted order, with repeated elements print ( list ( itertools . combinations_with_replacement ( 'ABC' , 2 ))) [('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'B'), ('B', 'C'), ('C', 'C')]","title":"Combinatoric iterators"},{"location":"lib/pyMemP/","text":"Managing memory is important in any programming logic but this becomes necessary for python. As python is used in Ml and AI where vast data are used which needs to be managed. Memory leaks, i.e. the program is out of memory after running for several hours. To manage these memory leaks memory monitoring is essential. Monitoring memory is also called profiling. As a developer, it\u2019s a necessity that we profile our program and use less memory allocation as much as possible. Using Tracemalloc Tracemalloc is a library module that traces every memory block in python. The tracing starts by using the start() during runtime. This library module can also give information about the total size, number, and average size of allocated memory blocks. # importing the module import tracemalloc # code or function for which memory # has to be monitored def app (): lt = [] for i in range ( 0 , 100000 ): lt . append ( i ) # starting the monitoring tracemalloc . start () # function call app () # displaying the memory print ( tracemalloc . get_traced_memory ()) # stopping the library tracemalloc . stop () (729, 3617551) The output is given in form of (current, peak), i.e. current memory is the memory the code is currently using and peak memory is the maximum space the program used while executing. Using Psutil Psutil is a python system library used to keep track of various resources in the system and their utilization. The library is used for profiling, limiting, and management of process resources. import os import psutil p = psutil . Process ( os . getpid ()) for i in range ( 10 ): print ( i ) for j in range ( 5 ): mem_usage = p . memory_info () . rss / 1024 / 1024 print ( \" {} {} MB\" . format ( j , mem_usage )) def memory_usage_psutil (): # return the memory usage in MB import psutil process = psutil . Process ( os . getpid ()) mem = process . get_memory_info ()[ 0 ] / float ( 2 ** 20 ) return mem The above function returns the memory usage of the current Python process in MiB. Depending on the platform it will choose the most accurate and fastest way to get this information. For example, in Windows it will use the C++ Win32 API while in Linux it will read from /proc, hiding the implementation details and proving on each platform a fast and accurate measurement. If you are looking for an easy way to get the memory consumption within Python this in my opinion your best shot. Using Memory Profiler ! pip install memory - profiler Notice the @profile this is a decorator. Any function which is decorated by this decorator, that function will be tracked. %% file sos . py \"\"\"memory_profiler example\"\"\" @profile def sum_of_diffs ( vals ): \"\"\"Compute sum of diffs\"\"\" vals2 = vals [ 1 :] total = 0 for v1 , v2 in zip ( vals , vals2 ): total += v2 - v1 return total if __name__ == '__main__' : vals = list ( range ( 1 , 1_000_000 , 3 )) print ( sum_of_diffs ( vals )) Writing sos.py ! python - m memory_profiler sos 999996 Filename: /content/sos.py Line # Mem usage Increment Occurrences Line Contents ============================================================= 3 52.160 MiB 52.160 MiB 1 @profile 4 def sum_of_diffs(vals): 5 \"\"\"Compute sum of diffs\"\"\" 6 54.738 MiB 2.578 MiB 1 vals2 = vals[1:] 7 8 54.738 MiB 0.000 MiB 1 total = 0 9 54.738 MiB 0.000 MiB 333333 for v1, v2 in zip(vals, vals2): 10 54.738 MiB 0.000 MiB 333332 total += v2 - v1 11 12 54.738 MiB 0.000 MiB 1 return total %% file sos1 . py \"\"\"memory_profiler example\"\"\" def sum_of_diffs ( vals ): \"\"\"Compute sum of diffs\"\"\" vals2 = vals [ 1 :] total = 0 for v1 , v2 in zip ( vals , vals2 ): total += v2 - v1 return total if __name__ == '__main__' : vals = list ( range ( 1 , 100_000_000 , 3 )) print ( sum_of_diffs ( vals )) Writing sos1.py mprof generates profile data ! mprof run sos1 . py mprof: Sampling memory every 0.1s running new process running as a Python program... 99999996 ! mprof plot mprofile_20211225174100 . dat <Figure size 1260x540 with 1 Axes> Reducing Memory Consumption with Slots Slots in Python is a special mechanism that is used to reduce memory of the objects. In Python, all the objects use a dynamic dictionary for adding an attribute. Slots is a static type method in this no dynamic dictionary are required for allocating attribute. # defining the class. class myCourse : # defining the slots. __slots__ = ( 'course' , 'price' ) def __init__ ( self ): # initializing the values self . course = 'MAth' self . price = 300 # create an object of gfg class a = myCourse () # print the slot print ( a . __slots__ ) # print the slot variable print ( a . course , a . price ) ('course', 'price') MAth 300 When we create objects for classes, it requires memory and the attribute are stored in the form of a dictionary. In case if we need to allocate thousands of objects, it will take a lot of memory space. slots provide a special mechanism to reduce the size of objects.It is a concept of memory optimisation on objects. As every object in Python contains a dynamic dictionary that allows adding attributes. For every instance object, we will have an instance of a dictionary that consumes more space and wastes a lot of RAM. In Python, there is no default functionality to allocate a static amount of memory while creating the object to store all its attributes. Usage of slots reduce the wastage of space and speed up the program by allocating space for a fixed amount of attributes. #Example of python object without slots class MFT ( object ): def __init__ ( self , * args , ** kwargs ): self . a = 1 self . b = 2 if __name__ == \"__main__\" : instance = MFT () print ( instance . __dict__ ) {'a': 1, 'b': 2} #Example of python object with slots class MFT ( object ): __slots__ = [ 'a' , 'b' ] def __init__ ( self , * args , ** kwargs ): self . a = 1 self . b = 2 if __name__ == \"__main__\" : instance = MFT () print ( instance . __slots__ ) ['a', 'b'] Result of using slots: Fast access to attributes Saves memory space","title":"Memory Profiling"},{"location":"lib/pyMemP/#using-tracemalloc","text":"Tracemalloc is a library module that traces every memory block in python. The tracing starts by using the start() during runtime. This library module can also give information about the total size, number, and average size of allocated memory blocks. # importing the module import tracemalloc # code or function for which memory # has to be monitored def app (): lt = [] for i in range ( 0 , 100000 ): lt . append ( i ) # starting the monitoring tracemalloc . start () # function call app () # displaying the memory print ( tracemalloc . get_traced_memory ()) # stopping the library tracemalloc . stop () (729, 3617551) The output is given in form of (current, peak), i.e. current memory is the memory the code is currently using and peak memory is the maximum space the program used while executing.","title":"Using Tracemalloc"},{"location":"lib/pyMemP/#using-psutil","text":"Psutil is a python system library used to keep track of various resources in the system and their utilization. The library is used for profiling, limiting, and management of process resources. import os import psutil p = psutil . Process ( os . getpid ()) for i in range ( 10 ): print ( i ) for j in range ( 5 ): mem_usage = p . memory_info () . rss / 1024 / 1024 print ( \" {} {} MB\" . format ( j , mem_usage )) def memory_usage_psutil (): # return the memory usage in MB import psutil process = psutil . Process ( os . getpid ()) mem = process . get_memory_info ()[ 0 ] / float ( 2 ** 20 ) return mem The above function returns the memory usage of the current Python process in MiB. Depending on the platform it will choose the most accurate and fastest way to get this information. For example, in Windows it will use the C++ Win32 API while in Linux it will read from /proc, hiding the implementation details and proving on each platform a fast and accurate measurement. If you are looking for an easy way to get the memory consumption within Python this in my opinion your best shot.","title":"Using Psutil"},{"location":"lib/pyMemP/#using-memory-profiler","text":"! pip install memory - profiler Notice the @profile this is a decorator. Any function which is decorated by this decorator, that function will be tracked. %% file sos . py \"\"\"memory_profiler example\"\"\" @profile def sum_of_diffs ( vals ): \"\"\"Compute sum of diffs\"\"\" vals2 = vals [ 1 :] total = 0 for v1 , v2 in zip ( vals , vals2 ): total += v2 - v1 return total if __name__ == '__main__' : vals = list ( range ( 1 , 1_000_000 , 3 )) print ( sum_of_diffs ( vals )) Writing sos.py ! python - m memory_profiler sos 999996 Filename: /content/sos.py Line # Mem usage Increment Occurrences Line Contents ============================================================= 3 52.160 MiB 52.160 MiB 1 @profile 4 def sum_of_diffs(vals): 5 \"\"\"Compute sum of diffs\"\"\" 6 54.738 MiB 2.578 MiB 1 vals2 = vals[1:] 7 8 54.738 MiB 0.000 MiB 1 total = 0 9 54.738 MiB 0.000 MiB 333333 for v1, v2 in zip(vals, vals2): 10 54.738 MiB 0.000 MiB 333332 total += v2 - v1 11 12 54.738 MiB 0.000 MiB 1 return total %% file sos1 . py \"\"\"memory_profiler example\"\"\" def sum_of_diffs ( vals ): \"\"\"Compute sum of diffs\"\"\" vals2 = vals [ 1 :] total = 0 for v1 , v2 in zip ( vals , vals2 ): total += v2 - v1 return total if __name__ == '__main__' : vals = list ( range ( 1 , 100_000_000 , 3 )) print ( sum_of_diffs ( vals )) Writing sos1.py mprof generates profile data ! mprof run sos1 . py mprof: Sampling memory every 0.1s running new process running as a Python program... 99999996 ! mprof plot mprofile_20211225174100 . dat <Figure size 1260x540 with 1 Axes>","title":"Using Memory Profiler"},{"location":"lib/pyMemP/#reducing-memory-consumption-with-slots","text":"Slots in Python is a special mechanism that is used to reduce memory of the objects. In Python, all the objects use a dynamic dictionary for adding an attribute. Slots is a static type method in this no dynamic dictionary are required for allocating attribute. # defining the class. class myCourse : # defining the slots. __slots__ = ( 'course' , 'price' ) def __init__ ( self ): # initializing the values self . course = 'MAth' self . price = 300 # create an object of gfg class a = myCourse () # print the slot print ( a . __slots__ ) # print the slot variable print ( a . course , a . price ) ('course', 'price') MAth 300 When we create objects for classes, it requires memory and the attribute are stored in the form of a dictionary. In case if we need to allocate thousands of objects, it will take a lot of memory space. slots provide a special mechanism to reduce the size of objects.It is a concept of memory optimisation on objects. As every object in Python contains a dynamic dictionary that allows adding attributes. For every instance object, we will have an instance of a dictionary that consumes more space and wastes a lot of RAM. In Python, there is no default functionality to allocate a static amount of memory while creating the object to store all its attributes. Usage of slots reduce the wastage of space and speed up the program by allocating space for a fixed amount of attributes. #Example of python object without slots class MFT ( object ): def __init__ ( self , * args , ** kwargs ): self . a = 1 self . b = 2 if __name__ == \"__main__\" : instance = MFT () print ( instance . __dict__ ) {'a': 1, 'b': 2} #Example of python object with slots class MFT ( object ): __slots__ = [ 'a' , 'b' ] def __init__ ( self , * args , ** kwargs ): self . a = 1 self . b = 2 if __name__ == \"__main__\" : instance = MFT () print ( instance . __slots__ ) ['a', 'b'] Result of using slots: Fast access to attributes Saves memory space","title":"Reducing Memory Consumption with Slots"},{"location":"lib/pyPack/","text":"In Python, the term packaging refers to putting modules you have written in a standard format, so that other programmers can install and use them with ease. This involves use of the modules setuptools and distutils . The first step in packaging is to organize existing files correctly. Place all of the files you want to put in a library in the same parent directory. This directory should also contain a file called __init__.py, which can be blank but must be present in the directory. This directory goes into another directory containing the readme and license, as well as an important file called setup.py. example/ LICENSE.txt README.txt setup.py example/ __init__.py example.py example2.py The next step in packaging is to write the setup.py file. This contains information necessary to assemble the package so it can be uploaded to PyPI and installed with pip (name, version, etc.). from distutils.core import setup setup ( name = 'example' , version = '0.1dev' , packages = [ 'example' ,], license = 'MIT' , long_description = open ( 'README.txt' ) . read (), ) After creating the setup.py file, upload it to PyPI, or use the command line to create a binary distribution (an executable installer). To build a source distribution, use the command line to navigate to the directory containing setup.py, and run the command python setup.py sdist . Run python setup.py bdist or, for Windows, python setup.py bdist_wininst to build a binary distribution. Use python setup.py register , followed by python setup.py sdist upload to upload a package. Finally, install a package with python setup.py install. Packaging for Users Most Python users will not want to use this module directly, but instead use the cross-version tools maintained by the Python Packaging Authority. In particular, setuptools is an enhanced alternative to distutils . For Windows, many tools are available for converting scripts to executables. For example, py2exe, can be used to package a Python script, along with the libraries it requires, into a single executable. PyInstaller and cx_Freeze serve the same purpose. For Macs, use py2app, PyInstaller or cx_Freeze.","title":"Packaging"},{"location":"lib/pyPack/#packaging-for-users","text":"Most Python users will not want to use this module directly, but instead use the cross-version tools maintained by the Python Packaging Authority. In particular, setuptools is an enhanced alternative to distutils . For Windows, many tools are available for converting scripts to executables. For example, py2exe, can be used to package a Python script, along with the libraries it requires, into a single executable. PyInstaller and cx_Freeze serve the same purpose. For Macs, use py2app, PyInstaller or cx_Freeze.","title":"Packaging for Users"},{"location":"lib/pyRandom/","text":"Python Random module is an in-built module of Python which is used to generate random numbers. These are pseudo-random numbers means these are not truly random. This module can be used to perform random actions such as generating random numbers, print random a value for a list or string, etc. # import random import random # prints a random value from the list list1 = [ 1 , 2 , 3 , 4 , 5 , 6 ] print ( random . choice ( list1 )) 3 import random random . seed ( 5 ) print ( random . random ()) print ( random . random ()) 0.6229016948897019 0.7417869892607294 # import the random module import random # declare a list sample_list = [ 1 , 2 , 3 , 4 , 5 ] print ( \"Original list : \" ) print ( sample_list ) # first shuffle random . shuffle ( sample_list ) print ( \" \\n After the first shuffle : \" ) print ( sample_list ) # second shuffle random . shuffle ( sample_list ) print ( \" \\n After the second shuffle : \" ) print ( sample_list ) Original list : [1, 2, 3, 4, 5] After the first shuffle : [3, 4, 2, 1, 5] After the second shuffle : [2, 1, 5, 4, 3] # Functions for generating random data sequences import random import string # Use the choice function to randomly select from a sequence moves = [ \"rock\" , \"paper\" , \"scissors\" ] print ( random . choice ( moves )) # Use the choices function to create a list of random elements roulette_wheel = [ \"black\" , \"red\" , \"green\" ] weights = [ 18 , 18 , 2 ] print ( random . choices ( roulette_wheel , weights , k = 10 )) # The sample function randomly selects elements from a population # without replacement (the chosen items are not replaced) chosen = random . sample ( string . ascii_uppercase , 6 ) print ( chosen ) # The shuffle function shuffles a sequence in place players = [ \"Bill\" , \"Jane\" , \"Joe\" , \"Sally\" , \"Mike\" , \"Lindsay\" ] random . shuffle ( players ) print ( players ) # to shuffle an immutable sequence, use the sample function first result = random . sample ( string . ascii_uppercase , k = len ( string . ascii_uppercase )) random . shuffle ( result ) print ( '' . join ( result )) paper ['red', 'black', 'black', 'black', 'red', 'black', 'black', 'red', 'black', 'red'] ['C', 'E', 'T', 'X', 'O', 'Y'] ['Lindsay', 'Joe', 'Sally', 'Mike', 'Bill', 'Jane'] EJYQHGKSUNCVBIFTZPWORXLDAM Criptographic random The secrets module is used for generating random numbers for managing important data such as passwords, account authentication, security tokens, and related secrets, that are cryptographically strong. This module is responsible for providing access to the most secure source of randomness. This module is present in Python 3.6 and above. # Using cryptographic-appropriate methods to generate random data # that may be sensitive. secrets module introduced in Python 3.6 import os import secrets # the urandom() function in the OS module produces random numbers that # are cryptographically safe to use for sensitive purposes result = os . urandom ( 8 ) print ([ hex ( b ) for b in result ]) # secrets.choice is the same as random.choice but more secure moves = [ \"rock\" , \"paper\" , \"scissors\" ] print ( secrets . choice ( moves )) # secrets.token_bytes generates random bytes result = secrets . token_bytes () print ( result ) # secrets.token_hex creates a random string in hexadecimal result = secrets . token_hex () print ( result ) # secrets.token_urlsafe generates characters that can be in URLs result = secrets . token_urlsafe () print ( result ) ['0x54', '0xd8', '0xb2', '0xf3', '0x68', '0xed', '0x35', '0xab'] scissors b'\\xbc\\x14\\xf5\\xc6\\x02\\xe5\\xd3W\\xcc9\\x88\\xddk\\xc7\\xb7\\xc4\\xc9\\x9a\\xcd\\xf0N\\x94\\x95\\xdd\\xfe\\xac\\xa1\\x07\\x80|L,' 4db4b731c2de8d0af94fe8563fc21f00aae04e41ba52ce56c5b9467583d3e57d tYBwYVYQXVdsXdeztE2-ki-osvgvsNnnWT2ZVLj3kng # Create a temporary password using Python import secrets import string # Function to return a temporary password given a length def generateTempPass ( numChars = 8 ): potentialChars = string . ascii_letters + string . digits + \"+=?/!@#$%*\" result = '' . join ( secrets . choice ( potentialChars ) for i in range ( numChars )) return result # Function to return a temporary password and enforce 1 number and 1 uppercase def generateBetterPass ( numChars = 8 ): potentialChars = string . ascii_letters + string . digits + \"+=?/!@#$%*\" while True : result = '' . join ( secrets . choice ( potentialChars ) for i in range ( numChars )) # if the password has at least one number and one uppercase char we can stop if ( any ( c . isupper () for c in result ) and any ( c . isdigit () for c in result )): break return result # create a temporary password print ( generateTempPass ( 10 )) # create a stronger temporary password print ( generateBetterPass ( 10 )) # create a temporary, hard-to-guess URL resultUrl = \"https://my.example.com?reset=\" resultUrl += secrets . token_urlsafe ( 15 ) print ( resultUrl ) p9oV1vn?eJ A2*Mlp7p!l https://my.example.com?reset=DhpaK2rfeYIbFs73nbl1 UUID UUID, Universal Unique Identifier, is a python library which helps in generating random objects of 128 bits as ids. It provides the uniqueness as it generates ids on the basis of time, Computer hardware (MAC etc.) Can be used as general utility to generate unique random id. Can be used in cryptography and hashing applications. Useful in generating random documents, addresses etc. # Generating unique identifiers import uuid # use the uuid4 function to create a random sequence using # the underlying os.urandom() function result = uuid . uuid4 () print ( \"UUID4: \" ) print ( result ) print ( result . hex ) print ( result . urn ) print ( \"~~~~~~~~~~~~~~~~~~~~~~~ \\n \" ) # create a UUID using uuid5, which takes a namespace and # name value. Note that this version is not crypto-safe result = uuid . uuid5 ( uuid . NAMESPACE_DNS , \"example.com\" ) print ( \"UUID5: \" ) print ( result ) print ( result . hex ) print ( result . urn ) print ( \"~~~~~~~~~~~~~~~~~~~~~~~ \\n \" ) UUID4: dfbf3df5-8647-4a91-8e51-81e1a3465ec3 dfbf3df586474a918e5181e1a3465ec3 urn:uuid:dfbf3df5-8647-4a91-8e51-81e1a3465ec3 ~~~~~~~~~~~~~~~~~~~~~~~ UUID5: cfbff0d1-9375-5685-968c-48ce8b15ae17 cfbff0d193755685968c48ce8b15ae17 urn:uuid:cfbff0d1-9375-5685-968c-48ce8b15ae17 ~~~~~~~~~~~~~~~~~~~~~~~","title":"Random"},{"location":"lib/pyRandom/#criptographic-random","text":"The secrets module is used for generating random numbers for managing important data such as passwords, account authentication, security tokens, and related secrets, that are cryptographically strong. This module is responsible for providing access to the most secure source of randomness. This module is present in Python 3.6 and above. # Using cryptographic-appropriate methods to generate random data # that may be sensitive. secrets module introduced in Python 3.6 import os import secrets # the urandom() function in the OS module produces random numbers that # are cryptographically safe to use for sensitive purposes result = os . urandom ( 8 ) print ([ hex ( b ) for b in result ]) # secrets.choice is the same as random.choice but more secure moves = [ \"rock\" , \"paper\" , \"scissors\" ] print ( secrets . choice ( moves )) # secrets.token_bytes generates random bytes result = secrets . token_bytes () print ( result ) # secrets.token_hex creates a random string in hexadecimal result = secrets . token_hex () print ( result ) # secrets.token_urlsafe generates characters that can be in URLs result = secrets . token_urlsafe () print ( result ) ['0x54', '0xd8', '0xb2', '0xf3', '0x68', '0xed', '0x35', '0xab'] scissors b'\\xbc\\x14\\xf5\\xc6\\x02\\xe5\\xd3W\\xcc9\\x88\\xddk\\xc7\\xb7\\xc4\\xc9\\x9a\\xcd\\xf0N\\x94\\x95\\xdd\\xfe\\xac\\xa1\\x07\\x80|L,' 4db4b731c2de8d0af94fe8563fc21f00aae04e41ba52ce56c5b9467583d3e57d tYBwYVYQXVdsXdeztE2-ki-osvgvsNnnWT2ZVLj3kng # Create a temporary password using Python import secrets import string # Function to return a temporary password given a length def generateTempPass ( numChars = 8 ): potentialChars = string . ascii_letters + string . digits + \"+=?/!@#$%*\" result = '' . join ( secrets . choice ( potentialChars ) for i in range ( numChars )) return result # Function to return a temporary password and enforce 1 number and 1 uppercase def generateBetterPass ( numChars = 8 ): potentialChars = string . ascii_letters + string . digits + \"+=?/!@#$%*\" while True : result = '' . join ( secrets . choice ( potentialChars ) for i in range ( numChars )) # if the password has at least one number and one uppercase char we can stop if ( any ( c . isupper () for c in result ) and any ( c . isdigit () for c in result )): break return result # create a temporary password print ( generateTempPass ( 10 )) # create a stronger temporary password print ( generateBetterPass ( 10 )) # create a temporary, hard-to-guess URL resultUrl = \"https://my.example.com?reset=\" resultUrl += secrets . token_urlsafe ( 15 ) print ( resultUrl ) p9oV1vn?eJ A2*Mlp7p!l https://my.example.com?reset=DhpaK2rfeYIbFs73nbl1","title":"Criptographic random"},{"location":"lib/pyRandom/#uuid","text":"UUID, Universal Unique Identifier, is a python library which helps in generating random objects of 128 bits as ids. It provides the uniqueness as it generates ids on the basis of time, Computer hardware (MAC etc.) Can be used as general utility to generate unique random id. Can be used in cryptography and hashing applications. Useful in generating random documents, addresses etc. # Generating unique identifiers import uuid # use the uuid4 function to create a random sequence using # the underlying os.urandom() function result = uuid . uuid4 () print ( \"UUID4: \" ) print ( result ) print ( result . hex ) print ( result . urn ) print ( \"~~~~~~~~~~~~~~~~~~~~~~~ \\n \" ) # create a UUID using uuid5, which takes a namespace and # name value. Note that this version is not crypto-safe result = uuid . uuid5 ( uuid . NAMESPACE_DNS , \"example.com\" ) print ( \"UUID5: \" ) print ( result ) print ( result . hex ) print ( result . urn ) print ( \"~~~~~~~~~~~~~~~~~~~~~~~ \\n \" ) UUID4: dfbf3df5-8647-4a91-8e51-81e1a3465ec3 dfbf3df586474a918e5181e1a3465ec3 urn:uuid:dfbf3df5-8647-4a91-8e51-81e1a3465ec3 ~~~~~~~~~~~~~~~~~~~~~~~ UUID5: cfbff0d1-9375-5685-968c-48ce8b15ae17 cfbff0d193755685968c48ce8b15ae17 urn:uuid:cfbff0d1-9375-5685-968c-48ce8b15ae17 ~~~~~~~~~~~~~~~~~~~~~~~","title":"UUID"},{"location":"lib/pySerial/","text":"Data Persistence The standard library includes a variety of modules for persisting data. The most common pattern for storing data from Python objects for reuse is to serialize them with pickle and then either write them directly to a file or store them using one of the many key-value pair database formats available with the dbm API. If you don\u2019t care about the underlying dbm format, the best persistence interface is provided by shelve. If you do care, you can use one of the other dbm-based modules directly. anydbm \u2013 Access to DBM-style databases dbhash \u2013 DBM-style API for the BSD database library dbm \u2013 Simple database interface dumbdbm \u2013 Portable DBM Implementation gdbm \u2013 GNU\u2019s version of the dbm library pickle and cPickle \u2013 Python object serialization shelve \u2013 Persistent storage of arbitrary Python objects whichdb \u2013 Identify DBM-style database formats sqlite3 \u2013 Embedded Relational Database For serializing over the web, the json module may be a better choice since its format is more portable. Serialization with Pickle The pickle module is used for implementing binary protocols for serializing and de-serializing a Python object structure. Pickling: It is a process where a Python object hierarchy is converted into a byte stream. Unpickling: It is the inverse of Pickling process where a byte stream is converted into an object hierarchy. Module Interface : dumps() \u2013 This function is called to serialize an object hierarchy. loads() \u2013 This function is called to de-serialize a data stream. # Python program to illustrate #Picle.dumps() import pickle data = [ { 'a' : 'A' , 'b' : 2 , 'c' : 3.0 } ] data_string = pickle . dumps ( data ) print ( 'PICKLE:' , data_string ) PICKLE: b'\\x80\\x03]q\\x00}q\\x01(X\\x01\\x00\\x00\\x00aq\\x02X\\x01\\x00\\x00\\x00Aq\\x03X\\x01\\x00\\x00\\x00bq\\x04K\\x02X\\x01\\x00\\x00\\x00cq\\x05G@\\x08\\x00\\x00\\x00\\x00\\x00\\x00ua.' # Python program to illustrate # pickle.loads() import pickle import pprint data1 = [ { 'a' : 'A' , 'b' : 2 , 'c' : 3.0 } ] print ( 'BEFORE:' ,) pprint . pprint ( data1 ) data1_string = pickle . dumps ( data1 ) data2 = pickle . loads ( data1_string ) print ( 'AFTER:' ,) pprint . pprint ( data2 ) print ( 'SAME?:' , ( data1 is data2 )) print ( 'EQUAL?:' , ( data1 == data2 )) BEFORE: [{'a': 'A', 'b': 2, 'c': 3.0}] AFTER: [{'a': 'A', 'b': 2, 'c': 3.0}] SAME?: False EQUAL?: True Python pickle module is used for serializing and de-serializing a Python object structure. Any object in Python can be pickled so that it can be saved on disk. What pickle does is that it \u201cserializes\u201d the object first before writing it to file. Pickling is a way to convert a python object (list, dict, etc.) into a character stream. The idea is that this character stream contains all the information necessary to reconstruct the object in another python script. # Python3 program to illustrate store # efficiently using pickle module # Module translates an in-memory Python object # into a serialized byte stream\u2014a string of # bytes that can be written to any file-like object. import pickle def storeData (): # initializing data to be stored in db Omkar = { 'key' : 'Omkar' , 'name' : 'Omkar Pathak' , 'age' : 21 , 'pay' : 40000 } Jagdish = { 'key' : 'Jagdish' , 'name' : 'Jagdish Pathak' , 'age' : 50 , 'pay' : 50000 } # database db = {} db [ 'Omkar' ] = Omkar db [ 'Jagdish' ] = Jagdish # Its important to use binary mode dbfile = open ( 'examplePickle' , 'ab' ) # source, destination pickle . dump ( db , dbfile ) dbfile . close () def loadData (): # for reading also binary mode is important dbfile = open ( 'examplePickle' , 'rb' ) db = pickle . load ( dbfile ) for keys in db : print ( keys , '=>' , db [ keys ]) dbfile . close () if __name__ == '__main__' : storeData () loadData () Omkar => {'key': 'Omkar', 'name': 'Omkar Pathak', 'age': 21, 'pay': 40000} Jagdish => {'key': 'Jagdish', 'name': 'Jagdish Pathak', 'age': 50, 'pay': 50000} Without a File # initializing data to be stored in db Omkar = { 'key' : 'Omkar' , 'name' : 'Omkar Pathak' , 'age' : 21 , 'pay' : 40000 } Jagdish = { 'key' : 'Jagdish' , 'name' : 'Jagdish Pathak' , 'age' : 50 , 'pay' : 50000 } # database db = {} db [ 'Omkar' ] = Omkar db [ 'Jagdish' ] = Jagdish # For storing b = pickle . dumps ( db ) # type(b) gives <class 'bytes'> # For loading myEntry = pickle . loads ( b ) print ( myEntry ) {'Omkar': {'key': 'Omkar', 'name': 'Omkar Pathak', 'age': 21, 'pay': 40000}, 'Jagdish': {'key': 'Jagdish', 'name': 'Jagdish Pathak', 'age': 50, 'pay': 50000}} Advantages of Using Pickle Recursive objects (objects containing references to themselves): Pickle keeps track of the objects it has already serialized, so later references to the same object won\u2019t be serialized again. (The marshal module breaks for this.) Object sharing (references to the same object in different places): This is similar to self- referencing objects; pickle stores the object once, and ensures that all other references point to the master copy. Shared objects remain shared, which can be very important for mutable objects. User-defined classes and their instances: Marshal does not support these at all, but pickle can save and restore class instances transparently. The class definition must be importable and live in the same module as when the object was stored. Serialization with JSON # import module import json # Data to be written data = { \"user\" : { \"name\" : \"satyam kumar\" , \"age\" : 21 , \"Place\" : \"Patna\" , \"Blood group\" : \"O+\" } } # Serializing json and # Writing json file with open ( \"datafile.json\" , \"w\" ) as write : json . dump ( data , write ) # importing the module import json # creating the JSON data as a string data = '{\"Name\" : \"Romy\", \"Gender\" : \"Female\"}' print ( \"Datatype before deserialization : \" + str ( type ( data ))) # deserializing the data data = json . loads ( data ) print ( \"Datatype after deserialization : \" + str ( type ( data ))) Datatype before deserialization : <class 'str'> Datatype after deserialization : <class 'dict'>","title":"Serialization"},{"location":"lib/pySerial/#serialization-with-pickle","text":"The pickle module is used for implementing binary protocols for serializing and de-serializing a Python object structure. Pickling: It is a process where a Python object hierarchy is converted into a byte stream. Unpickling: It is the inverse of Pickling process where a byte stream is converted into an object hierarchy. Module Interface : dumps() \u2013 This function is called to serialize an object hierarchy. loads() \u2013 This function is called to de-serialize a data stream. # Python program to illustrate #Picle.dumps() import pickle data = [ { 'a' : 'A' , 'b' : 2 , 'c' : 3.0 } ] data_string = pickle . dumps ( data ) print ( 'PICKLE:' , data_string ) PICKLE: b'\\x80\\x03]q\\x00}q\\x01(X\\x01\\x00\\x00\\x00aq\\x02X\\x01\\x00\\x00\\x00Aq\\x03X\\x01\\x00\\x00\\x00bq\\x04K\\x02X\\x01\\x00\\x00\\x00cq\\x05G@\\x08\\x00\\x00\\x00\\x00\\x00\\x00ua.' # Python program to illustrate # pickle.loads() import pickle import pprint data1 = [ { 'a' : 'A' , 'b' : 2 , 'c' : 3.0 } ] print ( 'BEFORE:' ,) pprint . pprint ( data1 ) data1_string = pickle . dumps ( data1 ) data2 = pickle . loads ( data1_string ) print ( 'AFTER:' ,) pprint . pprint ( data2 ) print ( 'SAME?:' , ( data1 is data2 )) print ( 'EQUAL?:' , ( data1 == data2 )) BEFORE: [{'a': 'A', 'b': 2, 'c': 3.0}] AFTER: [{'a': 'A', 'b': 2, 'c': 3.0}] SAME?: False EQUAL?: True Python pickle module is used for serializing and de-serializing a Python object structure. Any object in Python can be pickled so that it can be saved on disk. What pickle does is that it \u201cserializes\u201d the object first before writing it to file. Pickling is a way to convert a python object (list, dict, etc.) into a character stream. The idea is that this character stream contains all the information necessary to reconstruct the object in another python script. # Python3 program to illustrate store # efficiently using pickle module # Module translates an in-memory Python object # into a serialized byte stream\u2014a string of # bytes that can be written to any file-like object. import pickle def storeData (): # initializing data to be stored in db Omkar = { 'key' : 'Omkar' , 'name' : 'Omkar Pathak' , 'age' : 21 , 'pay' : 40000 } Jagdish = { 'key' : 'Jagdish' , 'name' : 'Jagdish Pathak' , 'age' : 50 , 'pay' : 50000 } # database db = {} db [ 'Omkar' ] = Omkar db [ 'Jagdish' ] = Jagdish # Its important to use binary mode dbfile = open ( 'examplePickle' , 'ab' ) # source, destination pickle . dump ( db , dbfile ) dbfile . close () def loadData (): # for reading also binary mode is important dbfile = open ( 'examplePickle' , 'rb' ) db = pickle . load ( dbfile ) for keys in db : print ( keys , '=>' , db [ keys ]) dbfile . close () if __name__ == '__main__' : storeData () loadData () Omkar => {'key': 'Omkar', 'name': 'Omkar Pathak', 'age': 21, 'pay': 40000} Jagdish => {'key': 'Jagdish', 'name': 'Jagdish Pathak', 'age': 50, 'pay': 50000}","title":"Serialization with Pickle"},{"location":"lib/pySerial/#without-a-file","text":"# initializing data to be stored in db Omkar = { 'key' : 'Omkar' , 'name' : 'Omkar Pathak' , 'age' : 21 , 'pay' : 40000 } Jagdish = { 'key' : 'Jagdish' , 'name' : 'Jagdish Pathak' , 'age' : 50 , 'pay' : 50000 } # database db = {} db [ 'Omkar' ] = Omkar db [ 'Jagdish' ] = Jagdish # For storing b = pickle . dumps ( db ) # type(b) gives <class 'bytes'> # For loading myEntry = pickle . loads ( b ) print ( myEntry ) {'Omkar': {'key': 'Omkar', 'name': 'Omkar Pathak', 'age': 21, 'pay': 40000}, 'Jagdish': {'key': 'Jagdish', 'name': 'Jagdish Pathak', 'age': 50, 'pay': 50000}}","title":"Without a File"},{"location":"lib/pySerial/#advantages-of-using-pickle","text":"Recursive objects (objects containing references to themselves): Pickle keeps track of the objects it has already serialized, so later references to the same object won\u2019t be serialized again. (The marshal module breaks for this.) Object sharing (references to the same object in different places): This is similar to self- referencing objects; pickle stores the object once, and ensures that all other references point to the master copy. Shared objects remain shared, which can be very important for mutable objects. User-defined classes and their instances: Marshal does not support these at all, but pickle can save and restore class instances transparently. The class definition must be importable and live in the same module as when the object was stored.","title":"Advantages of Using Pickle"},{"location":"lib/pySerial/#serialization-with-json","text":"# import module import json # Data to be written data = { \"user\" : { \"name\" : \"satyam kumar\" , \"age\" : 21 , \"Place\" : \"Patna\" , \"Blood group\" : \"O+\" } } # Serializing json and # Writing json file with open ( \"datafile.json\" , \"w\" ) as write : json . dump ( data , write ) # importing the module import json # creating the JSON data as a string data = '{\"Name\" : \"Romy\", \"Gender\" : \"Female\"}' print ( \"Datatype before deserialization : \" + str ( type ( data ))) # deserializing the data data = json . loads ( data ) print ( \"Datatype after deserialization : \" + str ( type ( data ))) Datatype before deserialization : <class 'str'> Datatype after deserialization : <class 'dict'>","title":"Serialization with JSON"},{"location":"lib/pyTest/","text":"Software testing can be divided into two classes, Manual testing and Automated testing. Automated testing is the execution of your tests using a script instead of a human. There are different Test Runners available in Python. Popular ones are: unittest nose or nose2 pytest Unitest It is built into the standard python library. import unittest should be the starting line of code for using it. Depends upon the python version, it should differ as later versions of Python supports unittest and earlier versions supported unittest2. One of the major problems with manual testing is that it requires time and effort. In manual testing, we test the application over some input, if it fails, either we note it down or we debug the application for that particular test input, and then we repeat the process. With unittest, all the test inputs can be provided at once and then you can test your application. In the end, you get a detailed report with all the failed test cases clearly specified, if any. The unittest module has both a built-in testing framework and a test runner. A testing framework is a set of rules which must be followed while writing test cases, while a test runner is a tool which executes these tests with a bunch of settings, and collects the results. Nose or Nose2 This is an open source application and similar to unittest only.It is compatible with numerous kinds of tests that are written using unittest framework. nose2 is the recent version one, and they are installed by using. pip install nose2 Pytest t supports unittest test cases execution. It has benefits like supporting built in assert statement, filtering of test cases, returning from last failing test etc def test_sum_numbers_using_pytest (): assert sum ([ 700 , 900 ]) == 1600 , \"Resultant should be 1600\" def test_sum_tuple_using_pytest (): assert sum (( 700 , 1900 )) == 1600 , \"Resultant should be 1600\"","title":"Testing"},{"location":"lib/pyTest/#unitest","text":"It is built into the standard python library. import unittest should be the starting line of code for using it. Depends upon the python version, it should differ as later versions of Python supports unittest and earlier versions supported unittest2. One of the major problems with manual testing is that it requires time and effort. In manual testing, we test the application over some input, if it fails, either we note it down or we debug the application for that particular test input, and then we repeat the process. With unittest, all the test inputs can be provided at once and then you can test your application. In the end, you get a detailed report with all the failed test cases clearly specified, if any. The unittest module has both a built-in testing framework and a test runner. A testing framework is a set of rules which must be followed while writing test cases, while a test runner is a tool which executes these tests with a bunch of settings, and collects the results.","title":"Unitest"},{"location":"lib/pyTest/#nose-or-nose2","text":"This is an open source application and similar to unittest only.It is compatible with numerous kinds of tests that are written using unittest framework. nose2 is the recent version one, and they are installed by using. pip install nose2","title":"Nose or Nose2"},{"location":"lib/pyTest/#pytest","text":"t supports unittest test cases execution. It has benefits like supporting built in assert statement, filtering of test cases, returning from last failing test etc def test_sum_numbers_using_pytest (): assert sum ([ 700 , 900 ]) == 1600 , \"Resultant should be 1600\" def test_sum_tuple_using_pytest (): assert sum (( 700 , 1900 )) == 1600 , \"Resultant should be 1600\"","title":"Pytest"},{"location":"lib/pyTimeP/","text":"In Python, we have three modules that help us find the execution time of a program. Using Timeit Module Python timeit module is often used to measure the execution time of small code snippets. We can also use the timeit() function which executes an anonymous function with a number of executions. It temporarily turns off garbage collection (the process of collecting unwanted variables whose use has been over and clears them by marking them as garbage values to free up the memory) during calculating the time of execution. How is the \"timeit\" command better than \"time\"? It averages results and ignores warm up. # importing the module import timeit # using the timeit method and lambda # expression to get the execution time of # the function. t = timeit . timeit ( lambda : \"print('Hello World!')\" ) # printing the execution time print ( t ) 0.08696221099944523 # importing the module import timeit # sample function that returns square # of the value passed def print_square ( x ): return ( x ** 2 ) # using the timeit method and lambda # expression to get the execution time of # the function, number defines how many # times we execute this function t = timeit . timeit ( lambda : print_square ( 3 ), number = 10 ) # printing the execution time print ( t ) 4.83900021208683e-06 Now, practically to estimate the execution time of a program we generally do not use the value obtained once as the ultimate correct value, because the execution of a program may depend upon os and availability of hardware at a particular instant of time. So generally we take multiple values of execution time and generally the average computed gives us the best possible answer. For this, we would use the timeit.repeat() method instead of timeit.timeit() which takes a repeat parameter and saves you the trouble of creating a loop and storing the values in the array. # importing the module import timeit # sample function that returns square # of the value passed def print_square ( x ): return ( x ** 2 ) # using the repeat method and lambda # expression to get the execution time of # the function, number defines how many # times we execute this function and the # repeat defines the number of times the # time calculation needs to be done. t = timeit . repeat ( lambda : print_square ( 3 ), number = 10 , repeat = 5 ) # printing the execution time print ( t ) [5.1380002332734875e-06, 3.6009996620123275e-06, 3.5189996197004803e-06, 3.484999979264103e-06, 3.50700065609999e-06] Also, we can use the timeit.default_timer() which basically records the time at the instant when the method is called. So we call the method just before and after the lines of the code for which we want to calculate the execution time and then basically the difference between the two times give us the result. So for finding the time we record the times using the timeit.defaultTimer() method, and then we print the difference between the two times in the last line. # importing the module import timeit # sample function that returns # square of the value passed def print_square ( x ): return ( x ** 2 ) # records the time at this instant # of the program start = timeit . default_timer () # calls the function print_square ( 3 ) # records the time at this instant # of the program end = timeit . default_timer () # printing the execution time by subtracting # the time before the function from # the time after the function print ( end - start ) 3.099999958067201e-05 \"\"\"Using \"timeit\"\"\" from timeit import timeit items = { 'a' : 1 , 'b' : 2 , } default = - 1 def use_catch ( key ): \"\"\"Use try/catch to get a key with default\"\"\" try : return items [ key ] except KeyError : return default def use_get ( key ): \"\"\"Use dict.get to get a key with default\"\"\" return items . get ( key , default ) if __name__ == '__main__' : # Key is in the dictionary print ( 'catch' , timeit ( 'use_catch(\"a\")' , 'from __main__ import use_catch' )) print ( 'get' , timeit ( 'use_get(\"a\")' , 'from __main__ import use_get' )) # Key is missing from the dictionary print ( 'catch' , timeit ( 'use_catch(\"x\")' , 'from __main__ import use_catch' )) print ( 'get' , timeit ( 'use_get(\"x\")' , 'from __main__ import use_get' )) catch 0.12401290399975551 get 0.16413491899947985 catch 0.32834199399985664 get 0.17250985499958915 Using Time Module We can use the time.perf_counter() method in the same way as the timeit.default_timer() method discussed above. It can use the highest possible resolution clock and gives you the most accurate result. In fact the timeit.default_timer() also uses time.perf_counter() as it\u2019s base. This also records the time before and after the required lines of code whose execution or elapsed time needs to be calculated. Then we subtract the recorded time before the start of the lines from the recorded time after the lines of the code. # importing the module import time # sample function that returns square # of the value passed def print_square ( x ): return ( x ** 2 ) # records the time at this instant of the # program start = time . perf_counter () # calls the function print_square ( 3 ) # records the time at this instant of the # program end = time . perf_counter () # printing the execution time by subtracting # the time before the function from # the time after the function print ( end - start ) 2.8733999897667672e-05 To measure the elapsed time or execution time of a block of code in nanoseconds, we can use the time.time_ns() function. This follows the same syntax as the time.perf_counter() function, like recording the time before and after the lines of the code and then subtracting the values and then printing them to the screen, but it records in nanoseconds instead of seconds. # importing the module import time # sample function that returns square # of the value passed def print_square ( x ): return ( x ** 2 ) # records the time in nanoseceonds at # this instant of the program start = time . time_ns () # calls the function print_square ( 3 ) # records the time in nanoseceonds at this # instant of the program end = time . time_ns () # printing the execution time by subtracting # the time before the function from # the time after the function print ( end - start ) 50676 \"\"\"Measuring time\"\"\" from time import perf_counter def upto_for ( n ): \"\"\"Sum 1...n with a for loop\"\"\" total = 0 for i in range ( n ): total += i return total def upto_sum ( n ): \"\"\"Sum 1...n with built-in sum and range\"\"\" return sum ( range ( n )) if __name__ == '__main__' : n = 1_000_000 start = perf_counter () upto_for ( n ) duration = perf_counter () - start print ( 'upto_for' , duration ) start = perf_counter () upto_sum ( n ) duration = perf_counter () - start print ( 'upto_sum' , duration ) upto_for 0.0630132690002938 upto_sum 0.018293597999218036 Using Datetime Module Datetime module can also be used to find the time elapsed or spent in a code block provided you are not looking for high precision. the datetime.now() also works the same way as the timeit.default_timer() or time.perf_counter() but lacks the same precision as those. It returns the result in HH:MM:SS format. This function is generally used for getting the current time and not a preferred use case for calculating execution time. However, these can be programs that take quite some time to execute like training ML/AI models, web scraping large websites, etc. # importing the module from datetime import datetime # sample function that returns square # of the value passed def print_square ( x ): return ( x ** 2 ) # records the time at this instant of # the program in HH:MM:SS format start = datetime . now () # calls the function print_square ( 3 ) # records the time at this instant of the # program in HH:MM:SS format end = datetime . now () # printing the execution time by subtracting # the time before the function from # the time after the function print ( end - start ) 0:00:00.000034 %Timeit Magic inline import numpy as np import pandas as pd data = pd . Series ( np . random . randint ( 50 , 60 , 10_000 )) #Outliers data [ 7 ] = 3 data [ 1003 ] = 100 def find_outliers ( data ): \"\"\"Find outliers in data, return indices of outliers\"\"\" out = data [( data - data . mean ()) . abs () > 2 * data . std ()] return out . index find_outliers ( data ) Int64Index([7, 1003], dtype='int64') % timeit find_outliers ( data ) 1000 loops, best of 5: 629 \u00b5s per loop %prun -s cumulative find_outliers(data) Using cprofile Python includes a built in module called cProfile which is used to measure the execution time of a program.cProfiler module provides all information about how long the program is executing and how many times the function get called in a program. How does cProfile work? It records every function entry and exit. # importing cProfile import cProfile cProfile . run ( \"10 + 10\" ) 3 function calls in 0.000 seconds Ordered by: standard name ncalls tottime percall cumtime percall filename:lineno(function) 1 0.000 0.000 0.000 0.000 <string>:1(<module>) 1 0.000 0.000 0.000 0.000 {built-in method builtins.exec} 1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects} # importing cProfile import cProfile def f (): print ( \"hello\" ) cProfile . run ( 'f()' ) hello 39 function calls in 0.000 seconds Ordered by: standard name ncalls tottime percall cumtime percall filename:lineno(function) 1 0.000 0.000 0.000 0.000 <ipython-input-46-7b34c7345cd2>:4(f) 1 0.000 0.000 0.000 0.000 <string>:1(<module>) 3 0.000 0.000 0.000 0.000 iostream.py:195(schedule) 2 0.000 0.000 0.000 0.000 iostream.py:307(_is_master_process) 2 0.000 0.000 0.000 0.000 iostream.py:320(_schedule_flush) 2 0.000 0.000 0.000 0.000 iostream.py:382(write) 3 0.000 0.000 0.000 0.000 iostream.py:93(_event_pipe) 3 0.000 0.000 0.000 0.000 socket.py:480(send) 3 0.000 0.000 0.000 0.000 threading.py:1050(_wait_for_tstate_lock) 3 0.000 0.000 0.000 0.000 threading.py:1092(is_alive) 3 0.000 0.000 0.000 0.000 threading.py:507(is_set) 1 0.000 0.000 0.000 0.000 {built-in method builtins.exec} 2 0.000 0.000 0.000 0.000 {built-in method builtins.isinstance} 1 0.000 0.000 0.000 0.000 {built-in method builtins.print} 2 0.000 0.000 0.000 0.000 {built-in method posix.getpid} 3 0.000 0.000 0.000 0.000 {method 'acquire' of '_thread.lock' objects} 3 0.000 0.000 0.000 0.000 {method 'append' of 'collections.deque' objects} 1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects} Using line_profiler Python provides a built-in module to measure execution time and the module name is LineProfiler.It gives detailed report on time consumed by a program. ! pip install line_profiler # importing line_profiler module from line_profiler import LineProfiler def peek ( rk ): print ( rk ) rk = \"risk\" profile = LineProfiler ( peek ( rk )) profile . print_stats () risk Timer unit: 1e-06 s % load_ext line_profiler % lprun - f find_outliers","title":"Time Profiling"},{"location":"lib/pyTimeP/#using-timeit-module","text":"Python timeit module is often used to measure the execution time of small code snippets. We can also use the timeit() function which executes an anonymous function with a number of executions. It temporarily turns off garbage collection (the process of collecting unwanted variables whose use has been over and clears them by marking them as garbage values to free up the memory) during calculating the time of execution. How is the \"timeit\" command better than \"time\"? It averages results and ignores warm up. # importing the module import timeit # using the timeit method and lambda # expression to get the execution time of # the function. t = timeit . timeit ( lambda : \"print('Hello World!')\" ) # printing the execution time print ( t ) 0.08696221099944523 # importing the module import timeit # sample function that returns square # of the value passed def print_square ( x ): return ( x ** 2 ) # using the timeit method and lambda # expression to get the execution time of # the function, number defines how many # times we execute this function t = timeit . timeit ( lambda : print_square ( 3 ), number = 10 ) # printing the execution time print ( t ) 4.83900021208683e-06 Now, practically to estimate the execution time of a program we generally do not use the value obtained once as the ultimate correct value, because the execution of a program may depend upon os and availability of hardware at a particular instant of time. So generally we take multiple values of execution time and generally the average computed gives us the best possible answer. For this, we would use the timeit.repeat() method instead of timeit.timeit() which takes a repeat parameter and saves you the trouble of creating a loop and storing the values in the array. # importing the module import timeit # sample function that returns square # of the value passed def print_square ( x ): return ( x ** 2 ) # using the repeat method and lambda # expression to get the execution time of # the function, number defines how many # times we execute this function and the # repeat defines the number of times the # time calculation needs to be done. t = timeit . repeat ( lambda : print_square ( 3 ), number = 10 , repeat = 5 ) # printing the execution time print ( t ) [5.1380002332734875e-06, 3.6009996620123275e-06, 3.5189996197004803e-06, 3.484999979264103e-06, 3.50700065609999e-06] Also, we can use the timeit.default_timer() which basically records the time at the instant when the method is called. So we call the method just before and after the lines of the code for which we want to calculate the execution time and then basically the difference between the two times give us the result. So for finding the time we record the times using the timeit.defaultTimer() method, and then we print the difference between the two times in the last line. # importing the module import timeit # sample function that returns # square of the value passed def print_square ( x ): return ( x ** 2 ) # records the time at this instant # of the program start = timeit . default_timer () # calls the function print_square ( 3 ) # records the time at this instant # of the program end = timeit . default_timer () # printing the execution time by subtracting # the time before the function from # the time after the function print ( end - start ) 3.099999958067201e-05 \"\"\"Using \"timeit\"\"\" from timeit import timeit items = { 'a' : 1 , 'b' : 2 , } default = - 1 def use_catch ( key ): \"\"\"Use try/catch to get a key with default\"\"\" try : return items [ key ] except KeyError : return default def use_get ( key ): \"\"\"Use dict.get to get a key with default\"\"\" return items . get ( key , default ) if __name__ == '__main__' : # Key is in the dictionary print ( 'catch' , timeit ( 'use_catch(\"a\")' , 'from __main__ import use_catch' )) print ( 'get' , timeit ( 'use_get(\"a\")' , 'from __main__ import use_get' )) # Key is missing from the dictionary print ( 'catch' , timeit ( 'use_catch(\"x\")' , 'from __main__ import use_catch' )) print ( 'get' , timeit ( 'use_get(\"x\")' , 'from __main__ import use_get' )) catch 0.12401290399975551 get 0.16413491899947985 catch 0.32834199399985664 get 0.17250985499958915","title":"Using Timeit Module"},{"location":"lib/pyTimeP/#using-time-module","text":"We can use the time.perf_counter() method in the same way as the timeit.default_timer() method discussed above. It can use the highest possible resolution clock and gives you the most accurate result. In fact the timeit.default_timer() also uses time.perf_counter() as it\u2019s base. This also records the time before and after the required lines of code whose execution or elapsed time needs to be calculated. Then we subtract the recorded time before the start of the lines from the recorded time after the lines of the code. # importing the module import time # sample function that returns square # of the value passed def print_square ( x ): return ( x ** 2 ) # records the time at this instant of the # program start = time . perf_counter () # calls the function print_square ( 3 ) # records the time at this instant of the # program end = time . perf_counter () # printing the execution time by subtracting # the time before the function from # the time after the function print ( end - start ) 2.8733999897667672e-05 To measure the elapsed time or execution time of a block of code in nanoseconds, we can use the time.time_ns() function. This follows the same syntax as the time.perf_counter() function, like recording the time before and after the lines of the code and then subtracting the values and then printing them to the screen, but it records in nanoseconds instead of seconds. # importing the module import time # sample function that returns square # of the value passed def print_square ( x ): return ( x ** 2 ) # records the time in nanoseceonds at # this instant of the program start = time . time_ns () # calls the function print_square ( 3 ) # records the time in nanoseceonds at this # instant of the program end = time . time_ns () # printing the execution time by subtracting # the time before the function from # the time after the function print ( end - start ) 50676 \"\"\"Measuring time\"\"\" from time import perf_counter def upto_for ( n ): \"\"\"Sum 1...n with a for loop\"\"\" total = 0 for i in range ( n ): total += i return total def upto_sum ( n ): \"\"\"Sum 1...n with built-in sum and range\"\"\" return sum ( range ( n )) if __name__ == '__main__' : n = 1_000_000 start = perf_counter () upto_for ( n ) duration = perf_counter () - start print ( 'upto_for' , duration ) start = perf_counter () upto_sum ( n ) duration = perf_counter () - start print ( 'upto_sum' , duration ) upto_for 0.0630132690002938 upto_sum 0.018293597999218036","title":"Using Time Module"},{"location":"lib/pyTimeP/#using-datetime-module","text":"Datetime module can also be used to find the time elapsed or spent in a code block provided you are not looking for high precision. the datetime.now() also works the same way as the timeit.default_timer() or time.perf_counter() but lacks the same precision as those. It returns the result in HH:MM:SS format. This function is generally used for getting the current time and not a preferred use case for calculating execution time. However, these can be programs that take quite some time to execute like training ML/AI models, web scraping large websites, etc. # importing the module from datetime import datetime # sample function that returns square # of the value passed def print_square ( x ): return ( x ** 2 ) # records the time at this instant of # the program in HH:MM:SS format start = datetime . now () # calls the function print_square ( 3 ) # records the time at this instant of the # program in HH:MM:SS format end = datetime . now () # printing the execution time by subtracting # the time before the function from # the time after the function print ( end - start ) 0:00:00.000034","title":"Using Datetime Module"},{"location":"lib/pyTimeP/#timeit-magic-inline","text":"import numpy as np import pandas as pd data = pd . Series ( np . random . randint ( 50 , 60 , 10_000 )) #Outliers data [ 7 ] = 3 data [ 1003 ] = 100 def find_outliers ( data ): \"\"\"Find outliers in data, return indices of outliers\"\"\" out = data [( data - data . mean ()) . abs () > 2 * data . std ()] return out . index find_outliers ( data ) Int64Index([7, 1003], dtype='int64') % timeit find_outliers ( data ) 1000 loops, best of 5: 629 \u00b5s per loop %prun -s cumulative find_outliers(data)","title":"%Timeit Magic inline"},{"location":"lib/pyTimeP/#using-cprofile","text":"Python includes a built in module called cProfile which is used to measure the execution time of a program.cProfiler module provides all information about how long the program is executing and how many times the function get called in a program. How does cProfile work? It records every function entry and exit. # importing cProfile import cProfile cProfile . run ( \"10 + 10\" ) 3 function calls in 0.000 seconds Ordered by: standard name ncalls tottime percall cumtime percall filename:lineno(function) 1 0.000 0.000 0.000 0.000 <string>:1(<module>) 1 0.000 0.000 0.000 0.000 {built-in method builtins.exec} 1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects} # importing cProfile import cProfile def f (): print ( \"hello\" ) cProfile . run ( 'f()' ) hello 39 function calls in 0.000 seconds Ordered by: standard name ncalls tottime percall cumtime percall filename:lineno(function) 1 0.000 0.000 0.000 0.000 <ipython-input-46-7b34c7345cd2>:4(f) 1 0.000 0.000 0.000 0.000 <string>:1(<module>) 3 0.000 0.000 0.000 0.000 iostream.py:195(schedule) 2 0.000 0.000 0.000 0.000 iostream.py:307(_is_master_process) 2 0.000 0.000 0.000 0.000 iostream.py:320(_schedule_flush) 2 0.000 0.000 0.000 0.000 iostream.py:382(write) 3 0.000 0.000 0.000 0.000 iostream.py:93(_event_pipe) 3 0.000 0.000 0.000 0.000 socket.py:480(send) 3 0.000 0.000 0.000 0.000 threading.py:1050(_wait_for_tstate_lock) 3 0.000 0.000 0.000 0.000 threading.py:1092(is_alive) 3 0.000 0.000 0.000 0.000 threading.py:507(is_set) 1 0.000 0.000 0.000 0.000 {built-in method builtins.exec} 2 0.000 0.000 0.000 0.000 {built-in method builtins.isinstance} 1 0.000 0.000 0.000 0.000 {built-in method builtins.print} 2 0.000 0.000 0.000 0.000 {built-in method posix.getpid} 3 0.000 0.000 0.000 0.000 {method 'acquire' of '_thread.lock' objects} 3 0.000 0.000 0.000 0.000 {method 'append' of 'collections.deque' objects} 1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects}","title":"Using cprofile"},{"location":"lib/pyTimeP/#using-line_profiler","text":"Python provides a built-in module to measure execution time and the module name is LineProfiler.It gives detailed report on time consumed by a program. ! pip install line_profiler # importing line_profiler module from line_profiler import LineProfiler def peek ( rk ): print ( rk ) rk = \"risk\" profile = LineProfiler ( peek ( rk )) profile . print_stats () risk Timer unit: 1e-06 s % load_ext line_profiler % lprun - f find_outliers","title":"Using line_profiler"},{"location":"lib/pydatetime/","text":"Datetime from datetime import date from datetime import time from datetime import datetime ## DATE OBJECTS # Get today's date from the simple today() method from the date class today = date . today () print ( \"Today's date is \" , today ) # print out the date's individual components print ( \"Date Components: \" , today . day , today . month , today . year ) # retrieve today's weekday (0=Monday, 6=Sunday) print ( \"Today's Weekday #: \" , today . weekday ()) days = [ \"monday\" , \"tuesday\" , \"wednesday\" , \"thursday\" , \"friday\" , \"saturday\" , \"sunday\" ] print ( \"Which is a \" + days [ today . weekday ()]) ## DATETIME OBJECTS # Get today's date from the datetime class today = datetime . now () print ( \"The current date and time is \" , today ) # Get the current time t = datetime . time ( datetime . now ()) print ( \"The current time is \" , t ) Today's date is 2021-12-22 Date Components: 22 12 2021 Today's Weekday #: 2 Which is a wednesday The current date and time is 2021-12-22 20:01:50.628110 The current time is 20:01:50.631578 Pendulum The pendulum is one of the popular Python DateTime libraries to ease DateTime manipulation. It provides a cleaner and easier to use API. It simplifies the problem of complex date manipulations involving timezones which are not handled correctly in native datetime instances. It inherits from the standard datetime library but provides better functionality. So you can introduce Pendulums Datetime instances in projects which are already using built-in datetime class (except for the libraries that check the type of the objects by using the type function like sqlite3). from datetime import date from datetime import time from datetime import datetime ! pip install pendulum import pendulum dt1 = pendulum . datetime ( 2021 , 12 , 24 ) print ( dt1 ) #local() creates datetime instance with local timezone local = pendulum . local ( 2020 , 11 , 27 ) print ( local ) print ( local . timezone . name ) # Importing library import pendulum # Getting current UTC time utc_time = pendulum . now ( 'UTC' ) # Switching current timezone to # Kolkata timezone using in_timezone(). kolkata_time = utc_time . in_timezone ( 'Asia/Kolkata' ) print ( 'Current Date Time in Kolkata =' , kolkata_time ) # Generating Sydney timezone sydney_tz = pendulum . timezone ( 'Australia/Sydney' ) # Switching current timezone to # Sydney timezone using convert(). sydney_time = sydney_tz . convert ( utc_time ) print ( 'Current Date Time in Sydney =' , sydney_time ) 2021-12-24T00:00:00+00:00 2020-11-27T00:00:00+00:00 Etc/UTC Current Date Time in Kolkata = 2021-12-23T01:36:55.336688+05:30 Current Date Time in Sydney = 2021-12-23T07:06:55.336688+11:00 # Importing the library import pendulum # creating datetime instance dt = pendulum . datetime ( 2020 , 11 , 27 ) print ( dt ) # Manipulating datetime object using add() dt = dt . add ( years = 5 ) print ( dt ) # Manipulating datetime object using subtract() dt = dt . subtract ( months = 1 ) print ( dt ) # Similarly you can add or subtract # months,weeks,days,hours,minutes # individually or all at a time. dt = dt . add ( years = 3 , months = 2 , days = 6 , hours = 12 , minutes = 30 , seconds = 45 ) print ( dt ) 2020-11-27T00:00:00+00:00 2025-11-27T00:00:00+00:00 2025-10-27T00:00:00+00:00 2029-01-02T12:30:45+00:00 dt_here = pendulum . now () dt_there = dt_here . in_timezone ( \"Europe/London\" ) print ( dt_there ) 2021-12-22T20:27:11.672167+00:00 Formatting # Times and dates can be formatted using a set of predefined string # control codes now = datetime . now () # get the current date and time #### Date Formatting #### # %y/%Y - Year, %a/%A - weekday, %b/%B - month, %d - day of month print ( now . strftime ( \"The current year is: %Y\" )) # full year with century print ( now . strftime ( \" %a , %d %B, %y\" )) # abbreviated day, num, full month, abbreviated year # %c - locale's date and time, %x - locale's date, %X - locale's time print ( now . strftime ( \"Locale date and time: %c \" )) print ( now . strftime ( \"Locale date: %x \" )) print ( now . strftime ( \"Locale time: %X \" )) #### Time Formatting #### # %I/%H - 12/24 Hour, %M - minute, %S - second, %p - locale's AM/PM print ( now . strftime ( \"Current time: %I:%M:%S %p\" )) # 12-Hour:Minute:Second:AM print ( now . strftime ( \"24-hour time: %H:%M\" )) # 24-Hour:Minute The current year is: 2021 Mon, 19 July, 21 Locale date and time: Mon Jul 19 14:22:20 2021 Locale date: 07/19/21 Locale time: 14:22:20 Current time: 02:22:20 PM 24-hour time: 14:22 import pendulum # Creating new DateTime instance dt = pendulum . datetime ( 2021 , 12 , 27 , 12 , 30 , 15 ) print ( dt ) # Formatting date-time dt . to_day_datetime_string () formatted_str = dt . format ( 'dddd Do [of] MMMM YYYY HH:mm:ss A' , locale = 'fr' ) print ( formatted_str ) new_str = dt . strftime ( '%Y-%m- %d %H:%M:%S %Z%z' ) print ( new_str ) 2021-12-27T12:30:15+00:00 lundi 27e of d\u00e9cembre 2021 12:30:15 PM 2021-12-27 12:30:15 UTC+0000 import pendulum dt = pendulum . parse ( '1997-11-21T22:00:00' , tz = 'Asia/Calcutta' ) print ( dt ) # parsing of non standard string dt = pendulum . from_format ( '2020/11/21' , 'YYYY/MM/DD' ) print ( dt ) 1997-11-21T22:00:00+05:30 2020-11-21T00:00:00+00:00 Time Deltas from datetime import date from datetime import time from datetime import datetime from datetime import timedelta # construct a basic timedelta and print it print ( timedelta ( days = 365 , hours = 5 , minutes = 1 )) # print today's date now = datetime . now () print ( now ) print ( \"today is: \" , now ) # print today's date one year from now print ( \"one year from now it will be: \" , now + timedelta ( days = 365 )) # create a timedelta that uses more than one argument print ( \"in two weeks and 3 days it will be: \" , now + timedelta ( weeks = 2 , days = 3 )) # calculate the date 1 week ago, formatted as a string t = datetime . now () - timedelta ( weeks = 1 ) s = t . strftime ( \"%A %B %d , %Y\" ) print ( \"one week ago it was \" + s ) ### How many days until April Fools' Day? today = date . today () # get today's date afd = date ( today . year , 4 , 1 ) # get April Fool's for the same year # use date comparison to see if April Fool's has already gone for this year # if it has, use the replace() function to get the date for next year if afd < today : print ( \"April Fool's day already went by %d days ago\" % (( today - afd ) . days )) afd = afd . replace ( year = today . year + 1 ) # if so, get the date for next year # Now calculate the amount of time until April Fool's Day time_to_afd = afd - today print ( \"It's just\" , time_to_afd . days , \"days until next April Fools' Day!\" ) 365 days, 5:01:00 2021-07-19 14:28:46.250025 today is: 2021-07-19 14:28:46.250025 one year from now it will be: 2022-07-19 14:28:46.250025 in two weeks and 3 days it will be: 2021-08-05 14:28:46.250025 one week ago it was Monday July 12, 2021 April Fool's day already went by 109 days ago It's just 256 days until next April Fools' Day! import pendulum time_delta = pendulum . duration ( days = 2 , hours = 10 , years = 2 ) print ( time_delta ) # Date when i am writing this code is 2020-11-27. print ( 'future date =' , pendulum . now () + time_delta ) 2 years 2 days 10 hours future date = 2023-12-25T06:10:31.649124+00:00 dt9 = pendulum . datetime ( 2022 , 10 , 13 ) di = dt9 . diff_for_humans ( pendulum . today ()) print ( di ) 9 months after Calendar import calendar # create a plain text calendar c = calendar . TextCalendar ( calendar . SUNDAY ) str_te = c . formatmonth ( 2017 , 1 , 0 , 0 ) print ( str_te ) # loop over the days of a month # zeroes mean that the day of the week is in an overlapping month for i in c . itermonthdays ( 2017 , 8 ): print ( i ) # The Calendar module provides useful utilities for the given locale, # such as the names of days and months in both full and abbreviated forms for name in calendar . month_name : print ( name ) for day in calendar . day_name : print ( day ) # Calculate days based on a rule: For example, consider # a team meeting on the first Friday of every month. # To figure out what days that would be for each month, # we can use this script: print ( \"Team meetings will be on:\" ) for m in range ( 1 , 13 ): # returns an array of weeks that represent the month cal = calendar . monthcalendar ( 2017 , m ) # The first Friday has to be within the first two weeks weekone = cal [ 0 ] weektwo = cal [ 1 ] if weekone [ calendar . FRIDAY ] != 0 : meetday = weekone [ calendar . FRIDAY ] else : # if the first friday isn't in the first week, it must be in the second meetday = weektwo [ calendar . FRIDAY ] print ( \" %10s %2d \" % ( calendar . month_name [ m ], meetday )) January 2017 Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 0 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 0 0 January February March April May June July August September October November December Monday Tuesday Wednesday Thursday Friday Saturday Sunday Team meetings will be on: January 6 February 3 March 3 April 7 May 5 June 2 July 7 August 4 September 1 October 6 November 3 December 1","title":"Dates and Time"},{"location":"lib/pydatetime/#datetime","text":"from datetime import date from datetime import time from datetime import datetime ## DATE OBJECTS # Get today's date from the simple today() method from the date class today = date . today () print ( \"Today's date is \" , today ) # print out the date's individual components print ( \"Date Components: \" , today . day , today . month , today . year ) # retrieve today's weekday (0=Monday, 6=Sunday) print ( \"Today's Weekday #: \" , today . weekday ()) days = [ \"monday\" , \"tuesday\" , \"wednesday\" , \"thursday\" , \"friday\" , \"saturday\" , \"sunday\" ] print ( \"Which is a \" + days [ today . weekday ()]) ## DATETIME OBJECTS # Get today's date from the datetime class today = datetime . now () print ( \"The current date and time is \" , today ) # Get the current time t = datetime . time ( datetime . now ()) print ( \"The current time is \" , t ) Today's date is 2021-12-22 Date Components: 22 12 2021 Today's Weekday #: 2 Which is a wednesday The current date and time is 2021-12-22 20:01:50.628110 The current time is 20:01:50.631578","title":"Datetime"},{"location":"lib/pydatetime/#pendulum","text":"The pendulum is one of the popular Python DateTime libraries to ease DateTime manipulation. It provides a cleaner and easier to use API. It simplifies the problem of complex date manipulations involving timezones which are not handled correctly in native datetime instances. It inherits from the standard datetime library but provides better functionality. So you can introduce Pendulums Datetime instances in projects which are already using built-in datetime class (except for the libraries that check the type of the objects by using the type function like sqlite3). from datetime import date from datetime import time from datetime import datetime ! pip install pendulum import pendulum dt1 = pendulum . datetime ( 2021 , 12 , 24 ) print ( dt1 ) #local() creates datetime instance with local timezone local = pendulum . local ( 2020 , 11 , 27 ) print ( local ) print ( local . timezone . name ) # Importing library import pendulum # Getting current UTC time utc_time = pendulum . now ( 'UTC' ) # Switching current timezone to # Kolkata timezone using in_timezone(). kolkata_time = utc_time . in_timezone ( 'Asia/Kolkata' ) print ( 'Current Date Time in Kolkata =' , kolkata_time ) # Generating Sydney timezone sydney_tz = pendulum . timezone ( 'Australia/Sydney' ) # Switching current timezone to # Sydney timezone using convert(). sydney_time = sydney_tz . convert ( utc_time ) print ( 'Current Date Time in Sydney =' , sydney_time ) 2021-12-24T00:00:00+00:00 2020-11-27T00:00:00+00:00 Etc/UTC Current Date Time in Kolkata = 2021-12-23T01:36:55.336688+05:30 Current Date Time in Sydney = 2021-12-23T07:06:55.336688+11:00 # Importing the library import pendulum # creating datetime instance dt = pendulum . datetime ( 2020 , 11 , 27 ) print ( dt ) # Manipulating datetime object using add() dt = dt . add ( years = 5 ) print ( dt ) # Manipulating datetime object using subtract() dt = dt . subtract ( months = 1 ) print ( dt ) # Similarly you can add or subtract # months,weeks,days,hours,minutes # individually or all at a time. dt = dt . add ( years = 3 , months = 2 , days = 6 , hours = 12 , minutes = 30 , seconds = 45 ) print ( dt ) 2020-11-27T00:00:00+00:00 2025-11-27T00:00:00+00:00 2025-10-27T00:00:00+00:00 2029-01-02T12:30:45+00:00 dt_here = pendulum . now () dt_there = dt_here . in_timezone ( \"Europe/London\" ) print ( dt_there ) 2021-12-22T20:27:11.672167+00:00","title":"Pendulum"},{"location":"lib/pydatetime/#formatting","text":"# Times and dates can be formatted using a set of predefined string # control codes now = datetime . now () # get the current date and time #### Date Formatting #### # %y/%Y - Year, %a/%A - weekday, %b/%B - month, %d - day of month print ( now . strftime ( \"The current year is: %Y\" )) # full year with century print ( now . strftime ( \" %a , %d %B, %y\" )) # abbreviated day, num, full month, abbreviated year # %c - locale's date and time, %x - locale's date, %X - locale's time print ( now . strftime ( \"Locale date and time: %c \" )) print ( now . strftime ( \"Locale date: %x \" )) print ( now . strftime ( \"Locale time: %X \" )) #### Time Formatting #### # %I/%H - 12/24 Hour, %M - minute, %S - second, %p - locale's AM/PM print ( now . strftime ( \"Current time: %I:%M:%S %p\" )) # 12-Hour:Minute:Second:AM print ( now . strftime ( \"24-hour time: %H:%M\" )) # 24-Hour:Minute The current year is: 2021 Mon, 19 July, 21 Locale date and time: Mon Jul 19 14:22:20 2021 Locale date: 07/19/21 Locale time: 14:22:20 Current time: 02:22:20 PM 24-hour time: 14:22 import pendulum # Creating new DateTime instance dt = pendulum . datetime ( 2021 , 12 , 27 , 12 , 30 , 15 ) print ( dt ) # Formatting date-time dt . to_day_datetime_string () formatted_str = dt . format ( 'dddd Do [of] MMMM YYYY HH:mm:ss A' , locale = 'fr' ) print ( formatted_str ) new_str = dt . strftime ( '%Y-%m- %d %H:%M:%S %Z%z' ) print ( new_str ) 2021-12-27T12:30:15+00:00 lundi 27e of d\u00e9cembre 2021 12:30:15 PM 2021-12-27 12:30:15 UTC+0000 import pendulum dt = pendulum . parse ( '1997-11-21T22:00:00' , tz = 'Asia/Calcutta' ) print ( dt ) # parsing of non standard string dt = pendulum . from_format ( '2020/11/21' , 'YYYY/MM/DD' ) print ( dt ) 1997-11-21T22:00:00+05:30 2020-11-21T00:00:00+00:00","title":"Formatting"},{"location":"lib/pydatetime/#time-deltas","text":"from datetime import date from datetime import time from datetime import datetime from datetime import timedelta # construct a basic timedelta and print it print ( timedelta ( days = 365 , hours = 5 , minutes = 1 )) # print today's date now = datetime . now () print ( now ) print ( \"today is: \" , now ) # print today's date one year from now print ( \"one year from now it will be: \" , now + timedelta ( days = 365 )) # create a timedelta that uses more than one argument print ( \"in two weeks and 3 days it will be: \" , now + timedelta ( weeks = 2 , days = 3 )) # calculate the date 1 week ago, formatted as a string t = datetime . now () - timedelta ( weeks = 1 ) s = t . strftime ( \"%A %B %d , %Y\" ) print ( \"one week ago it was \" + s ) ### How many days until April Fools' Day? today = date . today () # get today's date afd = date ( today . year , 4 , 1 ) # get April Fool's for the same year # use date comparison to see if April Fool's has already gone for this year # if it has, use the replace() function to get the date for next year if afd < today : print ( \"April Fool's day already went by %d days ago\" % (( today - afd ) . days )) afd = afd . replace ( year = today . year + 1 ) # if so, get the date for next year # Now calculate the amount of time until April Fool's Day time_to_afd = afd - today print ( \"It's just\" , time_to_afd . days , \"days until next April Fools' Day!\" ) 365 days, 5:01:00 2021-07-19 14:28:46.250025 today is: 2021-07-19 14:28:46.250025 one year from now it will be: 2022-07-19 14:28:46.250025 in two weeks and 3 days it will be: 2021-08-05 14:28:46.250025 one week ago it was Monday July 12, 2021 April Fool's day already went by 109 days ago It's just 256 days until next April Fools' Day! import pendulum time_delta = pendulum . duration ( days = 2 , hours = 10 , years = 2 ) print ( time_delta ) # Date when i am writing this code is 2020-11-27. print ( 'future date =' , pendulum . now () + time_delta ) 2 years 2 days 10 hours future date = 2023-12-25T06:10:31.649124+00:00 dt9 = pendulum . datetime ( 2022 , 10 , 13 ) di = dt9 . diff_for_humans ( pendulum . today ()) print ( di ) 9 months after","title":"Time Deltas"},{"location":"lib/pydatetime/#calendar","text":"import calendar # create a plain text calendar c = calendar . TextCalendar ( calendar . SUNDAY ) str_te = c . formatmonth ( 2017 , 1 , 0 , 0 ) print ( str_te ) # loop over the days of a month # zeroes mean that the day of the week is in an overlapping month for i in c . itermonthdays ( 2017 , 8 ): print ( i ) # The Calendar module provides useful utilities for the given locale, # such as the names of days and months in both full and abbreviated forms for name in calendar . month_name : print ( name ) for day in calendar . day_name : print ( day ) # Calculate days based on a rule: For example, consider # a team meeting on the first Friday of every month. # To figure out what days that would be for each month, # we can use this script: print ( \"Team meetings will be on:\" ) for m in range ( 1 , 13 ): # returns an array of weeks that represent the month cal = calendar . monthcalendar ( 2017 , m ) # The first Friday has to be within the first two weeks weekone = cal [ 0 ] weektwo = cal [ 1 ] if weekone [ calendar . FRIDAY ] != 0 : meetday = weekone [ calendar . FRIDAY ] else : # if the first friday isn't in the first week, it must be in the second meetday = weektwo [ calendar . FRIDAY ] print ( \" %10s %2d \" % ( calendar . month_name [ m ], meetday )) January 2017 Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 0 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 0 0 January February March April May June July August September October November December Monday Tuesday Wednesday Thursday Friday Saturday Sunday Team meetings will be on: January 6 February 3 March 3 April 7 May 5 June 2 July 7 August 4 September 1 October 6 November 3 December 1","title":"Calendar"},{"location":"lib/pylog/","text":"Logging is a means of tracking events that happen when some software runs. Logging is important for software developing, debugging and running. If you don\u2019t have any logging record and your program crashes, there are very little chances that you detect the cause of the problem. And if you detect the cause, it will consume a lot of time. With logging, you can leave a trail of breadcrumbs so that if something goes wrong, we can determine the cause of the problem. Python has a built-in module logging which allows writing status messages to a file or any other output streams. The Basics Basics of using the logging module to record the events in a file are very simple. For that, simply import the module from the library. Create and configure the logger. It can have several parameters. But importantly, pass the name of the file in which you want to record the events. Here the format of the logger can also be set. By default, the file works in append mode but we can change that to write mode if required. Also, the level of the logger can be set which acts as the threshold for tracking based on the numeric values assigned to each level. There are several attributes which can be passed as parameters. The list of all those parameters is given in Python Library. The user can choose the required attribute according to the requirement. After that, create an object and use the various methods as shown in the example. #importing module import logging #Create and configure logger logging . basicConfig ( filename = \"newfile.log\" , format = ' %(asctime)s %(message)s ' , filemode = 'w' ) #Creating an object logger = logging . getLogger () #Setting the threshold of logger to DEBUG logger . setLevel ( logging . DEBUG ) #Test messages logger . debug ( \"Harmless debug Message\" ) logger . info ( \"Just an information\" ) logger . warning ( \"Its a Warning\" ) logger . error ( \"Did you try to divide by zero\" ) logger . critical ( \"Internet is down\" ) import logging extData = { 'user' : 'caal@example.com' } def anotherFunction (): logging . debug ( \"This is a debug-level log message\" , extra = extData ) def main (): # set the output file and debug level, and # use a custom formatting specification fmtStr = \" %(asctime)s : %(levelname)s : %(funcName)s Line: %(lineno)d User: %(user)s %(message)s \" dateStr = \"%m/ %d /%Y %I:%M:%S %p\" logging . basicConfig ( filename = \"output.log\" , level = logging . DEBUG , format = fmtStr , datefmt = dateStr ) logging . info ( \"This is an info-level log message\" , extra = extData ) logging . warning ( \"This is a warning-level message\" , extra = extData ) anotherFunction () if __name__ == \"__main__\" : main ()","title":"Logging"},{"location":"lib/pylog/#the-basics","text":"Basics of using the logging module to record the events in a file are very simple. For that, simply import the module from the library. Create and configure the logger. It can have several parameters. But importantly, pass the name of the file in which you want to record the events. Here the format of the logger can also be set. By default, the file works in append mode but we can change that to write mode if required. Also, the level of the logger can be set which acts as the threshold for tracking based on the numeric values assigned to each level. There are several attributes which can be passed as parameters. The list of all those parameters is given in Python Library. The user can choose the required attribute according to the requirement. After that, create an object and use the various methods as shown in the example. #importing module import logging #Create and configure logger logging . basicConfig ( filename = \"newfile.log\" , format = ' %(asctime)s %(message)s ' , filemode = 'w' ) #Creating an object logger = logging . getLogger () #Setting the threshold of logger to DEBUG logger . setLevel ( logging . DEBUG ) #Test messages logger . debug ( \"Harmless debug Message\" ) logger . info ( \"Just an information\" ) logger . warning ( \"Its a Warning\" ) logger . error ( \"Did you try to divide by zero\" ) logger . critical ( \"Internet is down\" ) import logging extData = { 'user' : 'caal@example.com' } def anotherFunction (): logging . debug ( \"This is a debug-level log message\" , extra = extData ) def main (): # set the output file and debug level, and # use a custom formatting specification fmtStr = \" %(asctime)s : %(levelname)s : %(funcName)s Line: %(lineno)d User: %(user)s %(message)s \" dateStr = \"%m/ %d /%Y %I:%M:%S %p\" logging . basicConfig ( filename = \"output.log\" , level = logging . DEBUG , format = fmtStr , datefmt = dateStr ) logging . info ( \"This is an info-level log message\" , extra = extData ) logging . warning ( \"This is a warning-level message\" , extra = extData ) anotherFunction () if __name__ == \"__main__\" : main ()","title":"The Basics"}]}